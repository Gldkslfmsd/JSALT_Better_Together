<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/b81d6c9138a51e7e109ec59d04f22f7186b3b7dc">35: TorchBeast: A PyTorch Platform for Distributed RL</a></td>
</tr>
<tr>
<td>0</td>
<td>0.753201</td>
<td>0.105783</td>
<td><a href="https://www.semanticscholar.org/paper/aecf56ffc72fc4e6635d6bef47fc1157c89b3a23">5: NL4Py: Agent-Based Modeling in Python with Parallelizable NetLogo Workspaces</a></td>
</tr>
<tr>
<td>0</td>
<td>0.721124</td>
<td>0.970876</td>
<td><a href="https://www.semanticscholar.org/paper/c164da9e4389e6218f9a78e378fa95bcb23f7d24">2: WarpDrive: Extremely Fast End-to-End Deep Multi-Agent Reinforcement Learning on a GPU</a></td>
</tr>
<tr>
<td>0</td>
<td>0.715995</td>
<td>0.311581</td>
<td><a href="https://www.semanticscholar.org/paper/af2b4b4a2fea517c73aa4ca0cd1d31d6adc32bc5">0: HeterPS: Distributed Deep Learning With Reinforcement Learning Based Scheduling in Heterogeneous Environments</a></td>
</tr>
<tr>
<td>0</td>
<td>0.688112</td>
<td>0.071432</td>
<td><a href="https://www.semanticscholar.org/paper/1ee9ba9c9a8b4606b0e944fa86df28c7cfb29ccb">5: EbitSim: An Enhanced BitTorrent Simulation Using OMNeT++ 4</a></td>
</tr>
<tr>
<td>0</td>
<td>0.674338</td>
<td>0.471497</td>
<td><a href="https://www.semanticscholar.org/paper/e62d521ee523552e19128eb035955452f53630cb">3: RL-Bélády: A Unified Learning Framework for Content Caching</a></td>
</tr>
<tr>
<td>0</td>
<td>0.656357</td>
<td>0.244159</td>
<td><a href="https://www.semanticscholar.org/paper/5f778ca7d13713595d1460764ed53bd0074b9a27">0: LoSAC: An Efficient Local Stochastic Average Control Method for Federated Optimization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.653246</td>
<td>0.958676</td>
<td><a href="https://www.semanticscholar.org/paper/642ecfdfc0a4c649ce702e6bbb1a3cf158577d72">0: Single episode transfer for differing environmental dynamics in reinforcement learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.650419</td>
<td>0.964031</td>
<td><a href="https://www.semanticscholar.org/paper/6cd495ced96ebe9838d533cfaf8b044fcb8a66c1">23: Single Episode Policy Transfer in Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.648868</td>
<td>0.976963</td>
<td><a href="https://www.semanticscholar.org/paper/6b5c4f1f4b4ec90383f43f8bb41ef9758762a53e">42: A Deep Bayesian Policy Reuse Approach Against Non-Stationary Agents</a></td>
</tr>
<tr>
<td>0</td>
<td>0.641243</td>
<td>0.996687</td>
<td><a href="https://www.semanticscholar.org/paper/8ede7ddf99986d69562455bc8d69222fc3e27350">258: Recurrent Experience Replay in Distributed Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.736140</td>
<td>0.995599</td>
<td><a href="https://www.semanticscholar.org/paper/89a6e3796aca72f370a1f1550f5759cea7eef382">65: SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference</a></td>
</tr>
<tr>
<td>0</td>
<td>0.637975</td>
<td>0.993086</td>
<td><a href="https://www.semanticscholar.org/paper/292bbd1287a0674cd9e3e79224e768ca557dcf81">70: Mastering Complex Control in MOBA Games with Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.640490</td>
<td>0.993015</td>
<td><a href="https://www.semanticscholar.org/paper/0772905d40b9afa3dc087a88184f09f3b3e1464f">907: Learning to Communicate with Deep Multi-Agent Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.598933</td>
<td>0.992198</td>
<td><a href="https://www.semanticscholar.org/paper/60c0d06bd24d1f66f84a44a8931d3276313a4029">11: AI-QMIX: Attention and Imagination for Dynamic Multi-Agent Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.562757</td>
<td>0.991853</td>
<td><a href="https://www.semanticscholar.org/paper/a9ae2bf20151247910fe8fc77cd4db8f35bfead0">2: Wield: Systematic Reinforcement Learning With Progressive Randomization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.621567</td>
<td>0.991588</td>
<td><a href="https://www.semanticscholar.org/paper/70dbcbcea2f804dd88f291235ae9781de511643a">60: An investigation of model-free planning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.678343</td>
<td>0.991542</td>
<td><a href="https://www.semanticscholar.org/paper/67ee02edb5d969c0657314ed6c1a6ed62121ac29">79: Accelerated Methods for Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.626039</td>
<td>0.991404</td>
<td><a href="https://www.semanticscholar.org/paper/6dd9466b1fe3482843f73f0957541b073fb9597c">39: R-MADDPG for Partially Observable Environments and Limited Communication</a></td>
</tr>
</table></html>
