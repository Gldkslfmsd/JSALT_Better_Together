<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/6999fd72868c4044e852c43a040a87a43d03ab3a">22: Prioritized Level Replay</a></td>
</tr>
<tr>
<td>0</td>
<td>0.808518</td>
<td>0.984969</td>
<td><a href="https://www.semanticscholar.org/paper/e0155830d8982da4631cb71546fca782b2e00c20">49: Composable Planning with Attributes</a></td>
</tr>
<tr>
<td>0</td>
<td>0.789737</td>
<td>0.973399</td>
<td><a href="https://www.semanticscholar.org/paper/ba7a309fcc8dd361bddd27662fdfd68294e58b80">33: Organizing Experience: a Deeper Look at Replay Mechanisms for Sample-Based Planning in Continuous State Domains</a></td>
</tr>
<tr>
<td>0</td>
<td>0.785166</td>
<td>0.921088</td>
<td><a href="https://www.semanticscholar.org/paper/27f1e4c7cad50cf0c27e7caaf1648f9d96d15976">1: Efficient Reinforcement Learning via Initial Pure Exploration</a></td>
</tr>
<tr>
<td>0</td>
<td>0.779038</td>
<td>0.915797</td>
<td><a href="https://www.semanticscholar.org/paper/e27d52806236f5f10d056068103396b2bd42c754">4: Abstraction in Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.778600</td>
<td>0.901881</td>
<td><a href="https://www.semanticscholar.org/paper/9b38a8b39932911d163986b3ace91f6d54b96e41">3: Effective Transfer via Demonstrations in Reinforcement Learning: A Preliminary Study</a></td>
</tr>
<tr>
<td>0</td>
<td>0.770852</td>
<td>0.799802</td>
<td><a href="https://www.semanticscholar.org/paper/8c29532c228c5daf8abd4688ad1550586282ed4d">0: Nested Policy Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.770191</td>
<td>0.907624</td>
<td><a href="https://www.semanticscholar.org/paper/817a19c7a753968d65d506039d9f1d4ae26f6b2a">0: Provably Efficient Causal Model-Based Reinforcement Learning for Systematic Generalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.767870</td>
<td>0.954938</td>
<td><a href="https://www.semanticscholar.org/paper/b974868199ff98013f9b907cdacd52315ca8dad4">24: Automated curricula through setter-solver interactions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763410</td>
<td>0.777969</td>
<td><a href="https://www.semanticscholar.org/paper/adc9ef39365b6c46a552f42406b585e29b358f64">15: Imitating Inscrutable Enemies: Learning from Stochastic Policy Observation, Retrieval and Reuse</a></td>
</tr>
<tr>
<td>0</td>
<td>0.739736</td>
<td>0.995451</td>
<td><a href="https://www.semanticscholar.org/paper/8d814620a1ca77e745bc8a33b96b86148f2804fe">195: Leveraging Procedural Generation to Benchmark Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.695586</td>
<td>0.995448</td>
<td><a href="https://www.semanticscholar.org/paper/ef2bc452812d6005ab0a66af6c3f97b6b0ba837e">346: Quantifying Generalization in Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.734010</td>
<td>0.994549</td>
<td><a href="https://www.semanticscholar.org/paper/ec05bd6725ac6a5217021881cac8553581b3e313">2: Measuring Progress in Deep Reinforcement Learning Sample Efficiency</a></td>
</tr>
<tr>
<td>0</td>
<td>0.745751</td>
<td>0.994490</td>
<td><a href="https://www.semanticscholar.org/paper/99278179243c3771440e6c3824f8aef2bf34ee07">30: A Survey of Generalisation in Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.724225</td>
<td>0.994399</td>
<td><a href="https://www.semanticscholar.org/paper/13e53be26a57673761137f4750cd205b14e720e2">0: Improving adaptability to new environments and removing catastrophic forgetting in Reinforcement Learning by using an eco-system of agents</a></td>
</tr>
<tr>
<td>0</td>
<td>0.686025</td>
<td>0.993503</td>
<td><a href="https://www.semanticscholar.org/paper/8272d2f9412e9151c023011205227859a5021177">20: Decoupling Value and Policy for Generalization in Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.676112</td>
<td>0.993419</td>
<td><a href="https://www.semanticscholar.org/paper/87ce35fc57efb4e3bcdd9a2d667d1b242fd16bbf">0: D OJO : A B ENCHMARK FOR L ARGE S CALE M ULTI T ASK R EINFORCEMENT L EARNING</a></td>
</tr>
<tr>
<td>0</td>
<td>0.631450</td>
<td>0.992733</td>
<td><a href="https://www.semanticscholar.org/paper/b19729b27a1b4c24b52f87308c907653300afa7f">644: Dota 2 with Large Scale Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.731150</td>
<td>0.992660</td>
<td><a href="https://www.semanticscholar.org/paper/51517d5d900a7c0cd33e210c48cb27ef3b96e5a9">1: JueWu-MC: Playing Minecraft with Sample-efficient Hierarchical Reinforcement Learning</a></td>
</tr>
</table></html>
