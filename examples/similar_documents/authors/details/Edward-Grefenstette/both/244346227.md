<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/99278179243c3771440e6c3824f8aef2bf34ee07">30: A Survey of Generalisation in Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.877486</td>
<td>0.988443</td>
<td><a href="https://www.semanticscholar.org/paper/d81dd2dc0ee02d996763f3ea1703eaff681485d7">33: Investigating Generalisation in Continuous Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826946</td>
<td>0.909203</td>
<td><a href="https://www.semanticscholar.org/paper/cf2abe0034d6e605434ad336a8e3a60a18fc473a">0: Domain Shifts in Reinforcement Learning: Identifying Disturbances in Environments</a></td>
</tr>
<tr>
<td>0</td>
<td>0.802635</td>
<td>0.939219</td>
<td><a href="https://www.semanticscholar.org/paper/76004f10694a7da0925125ff316b55d1ccdbe251">6: Contextual-MDPs for PAC-Reinforcement Learning with Rich Observations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.801837</td>
<td>0.987490</td>
<td><a href="https://www.semanticscholar.org/paper/de46f4e4613364792bbd13f185c381ab656a27ef">40: RL Unplugged: Benchmarks for Offline Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.791870</td>
<td>0.989029</td>
<td><a href="https://www.semanticscholar.org/paper/2c3a1a088ef51548264197ed8882f42e0ad73a9b">74: Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck</a></td>
</tr>
<tr>
<td>0</td>
<td>0.791359</td>
<td>0.979369</td>
<td><a href="https://www.semanticscholar.org/paper/f6e3df686d5218cc2a182c3ab61b1800183b0ea4">2: Towards Practical Credit Assignment for Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.790858</td>
<td>0.983839</td>
<td><a href="https://www.semanticscholar.org/paper/2470fcf0f89082de874ac9133ccb3a8667dd89a8">359: Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.787628</td>
<td>0.897112</td>
<td><a href="https://www.semanticscholar.org/paper/dc93e74bc7f236c498a4025dc731fd83996db3ae">7: Optimising Worlds to Evaluate and Influence Reinforcement Learning Agents</a></td>
</tr>
<tr>
<td>0</td>
<td>0.787005</td>
<td>0.989712</td>
<td><a href="https://www.semanticscholar.org/paper/484ee269ce0536ae15754602cb5143191b8e7853">3: Towards robust and domain agnostic reinforcement learning competitions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.703896</td>
<td>0.997707</td>
<td><a href="https://www.semanticscholar.org/paper/8d814620a1ca77e745bc8a33b96b86148f2804fe">195: Leveraging Procedural Generation to Benchmark Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.698490</td>
<td>0.996833</td>
<td><a href="https://www.semanticscholar.org/paper/3b3d7adb9047d01af6dfa2975ad8addd69715e96">13: Mastering Atari Games with Limited Data</a></td>
</tr>
<tr>
<td>0</td>
<td>0.704136</td>
<td>0.996673</td>
<td><a href="https://www.semanticscholar.org/paper/44164c068499fbe387a1765104d69a8cbc5f0327">4: Procedural Generalization by Planning with Self-Supervised World Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.773353</td>
<td>0.996656</td>
<td><a href="https://www.semanticscholar.org/paper/ef2bc452812d6005ab0a66af6c3f97b6b0ba837e">346: Quantifying Generalization in Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.783798</td>
<td>0.996467</td>
<td><a href="https://www.semanticscholar.org/paper/75f425319694047f763f02f2a07912cd5621cfa4">2: Evaluating the progress of Deep Reinforcement Learning in the real world: aligning domain-agnostic and domain-specific research</a></td>
</tr>
<tr>
<td>0</td>
<td>0.754109</td>
<td>0.995831</td>
<td><a href="https://www.semanticscholar.org/paper/21a7c5e95aeddc2b208f425cd3fa0c56e9e60692">8: Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Offline Environment</a></td>
</tr>
<tr>
<td>0</td>
<td>0.704008</td>
<td>0.995298</td>
<td><a href="https://www.semanticscholar.org/paper/3c91a4534d51d21ae67e4a9f9287bb2a14dc5e3b">2: Learning more skills through optimistic exploration</a></td>
</tr>
<tr>
<td>0</td>
<td>0.731317</td>
<td>0.995263</td>
<td><a href="https://www.semanticscholar.org/paper/d20c3b9eb381687d4aea549020fc48b0a8112bb2">5: Offline Meta-Reinforcement Learning with Online Self-Supervision</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763670</td>
<td>0.995158</td>
<td><a href="https://www.semanticscholar.org/paper/51965de80f86432d42749427db1e5bb0fa1e204c">3: B-Pref: Benchmarking Preference-Based Reinforcement Learning</a></td>
</tr>
</table></html>
