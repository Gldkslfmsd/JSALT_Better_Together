<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/4a4b71ff918ca8eeffa5dfe66be2db7fcc1291da">97: Learning to Understand Goal Specifications by Modelling Reward</a></td>
</tr>
<tr>
<td>1</td>
<td>0.885129</td>
<td>0.993037</td>
<td><a href="https://www.semanticscholar.org/paper/758311575a6385bb15d4f9af8c0e671cb98184b4">65: From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following</a></td>
</tr>
<tr>
<td>0</td>
<td>0.844114</td>
<td>0.921985</td>
<td><a href="https://www.semanticscholar.org/paper/9862caed8ee93321c78b0196e0b7eef516b545ba">263: Reverse Curriculum Generation for Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.842596</td>
<td>0.933136</td>
<td><a href="https://www.semanticscholar.org/paper/4a78cd72b184bec55e05db0e2df25d63765de068">2: Learning What To Do by Simulating the Past</a></td>
</tr>
<tr>
<td>0</td>
<td>0.839910</td>
<td>0.916054</td>
<td><a href="https://www.semanticscholar.org/paper/c28ec2a40a2c77e20d64cf1c85dc931106df8e83">386: Overcoming Exploration in Reinforcement Learning with Demonstrations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.837551</td>
<td>0.886450</td>
<td><a href="https://www.semanticscholar.org/paper/a001e436c35464aa9a4fda957d09b0189dc11ad7">0: Target-Driven Navigation with Imitation Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.836670</td>
<td>-0.026848</td>
<td>NA:233285048</td>
</tr>
<tr>
<td>0</td>
<td>0.835827</td>
<td>0.925182</td>
<td><a href="https://www.semanticscholar.org/paper/2565ce38975f8494b8949360aaf8138665c20d00">0: Implementation of Language-Action Reward Network in Reinforcement Learning by Using Natural Language</a></td>
</tr>
<tr>
<td>0</td>
<td>0.830887</td>
<td>0.854815</td>
<td><a href="https://www.semanticscholar.org/paper/d1b0a8ca7f6a8936dad8768e9c2b585b4b5597c5">3: Imitation Learning over Heterogeneous Agents with Restraining Bolts</a></td>
</tr>
<tr>
<td>0</td>
<td>0.829753</td>
<td>0.950148</td>
<td><a href="https://www.semanticscholar.org/paper/3693414d385401997c13a4faa39a8b6c6cd4a4dd">3: Communicating Hierarchical Neural Controllers for Learning Zero-shot Task Generalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.786379</td>
<td>0.991701</td>
<td><a href="https://www.semanticscholar.org/paper/30834ae1497c35d362eea14857d93c28d2d12b57">186: Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826433</td>
<td>0.990897</td>
<td><a href="https://www.semanticscholar.org/paper/9d90fa5b4f3b8a6ec0485417ef54fdf548d21714">16: ACTRCE: Augmenting Experience via Teacher's Advice For Multi-Goal Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793242</td>
<td>0.990874</td>
<td><a href="https://www.semanticscholar.org/paper/1abded7824da0d9a5ccabcd998d5d7d95acb0d54">8: HIGhER: Improving instruction following with Hindsight Generation for Experience Replay</a></td>
</tr>
<tr>
<td>0</td>
<td>0.836777</td>
<td>0.990580</td>
<td><a href="https://www.semanticscholar.org/paper/0d6a4e45acde6f47d704ed0752f17f7ab52223af">9: Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811525</td>
<td>0.990231</td>
<td><a href="https://www.semanticscholar.org/paper/3f0f6c19c6f5d4e4d5066984c5f3e922a2c2ff85">3: ELLA: Exploration through Learned Language Abstraction</a></td>
</tr>
<tr>
<td>0</td>
<td>0.898817</td>
<td>0.989857</td>
<td><a href="https://www.semanticscholar.org/paper/71b152f65fd9967ec39f1e1f359ad0d99be1bab2">29: Learning to Follow Language Instructions with Adversarial Reward Induction</a></td>
</tr>
<tr>
<td>0</td>
<td>0.832821</td>
<td>0.988878</td>
<td><a href="https://www.semanticscholar.org/paper/5dcad60d3ae2271e209badeb1755b06423415e27">4: Safe Reinforcement Learning with Natural Language Constraints</a></td>
</tr>
<tr>
<td>0</td>
<td>0.820528</td>
<td>0.988563</td>
<td><a href="https://www.semanticscholar.org/paper/c2c8482c713b94073f3d59895b373db4398ddfbb">86: Language as an Abstraction for Hierarchical Deep Reinforcement Learning</a></td>
</tr>
</table></html>
