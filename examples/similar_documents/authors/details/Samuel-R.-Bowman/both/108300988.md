<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/e2587eddd57bc4ba286d91b27c185083f16f40ee">484: What do you learn from context? Probing for sentence structure in contextualized word representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.975280</td>
<td>0.902620</td>
<td><a href="https://www.semanticscholar.org/paper/35badb4a18900e649f18f58dc810b85937dfed98">0: LEARN FROM CONTEXT ? P ROBING FOR SENTENCE STRUCTURE IN CONTEXTUALIZED WORD REPRESENTATIONS</a></td>
</tr>
<tr>
<td>0</td>
<td>0.829162</td>
<td>0.923517</td>
<td><a href="https://www.semanticscholar.org/paper/c84e20bc2b8f8e4bc1de218f9ca8a9511ea4ada1">6: Unsupervised Distillation of Syntactic Information from Contextualized Word Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.822007</td>
<td>0.910421</td>
<td><a href="https://www.semanticscholar.org/paper/baf29a39c8ad084eda4f8167383641bb9cd2ad87">17: Retrofitting Contextualized Word Embeddings with Paraphrases</a></td>
</tr>
<tr>
<td>0</td>
<td>0.821660</td>
<td>0.982447</td>
<td><a href="https://www.semanticscholar.org/paper/26f7305e4cf293b3daa672f0f75c1b0bac1e873a">102: Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809973</td>
<td>0.742464</td>
<td><a href="https://www.semanticscholar.org/paper/c673a02f629d482853f1f9523793e71118ac7273">1: Generationary or: “How We Went beyond Sense Inventories and Learnedto Gloss”</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809757</td>
<td>0.981433</td>
<td><a href="https://www.semanticscholar.org/paper/071b4b4abadfbebce542aa9de2d07342a49a15c0">0: On the Hierarchical Information in a Single Contextualised Word Representation (Student Abstract)</a></td>
</tr>
<tr>
<td>0</td>
<td>0.808744</td>
<td>0.845418</td>
<td><a href="https://www.semanticscholar.org/paper/7e870c766bbb7ca4ed7d2c86a696bec7d582a5c0">0: A Joint Deep Contextualized Word Representation for Deep Biaffine Dependency Parsing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.803992</td>
<td>0.991295</td>
<td><a href="https://www.semanticscholar.org/paper/850713961a5aa20812cf952f950f09d491fae281">27: Universal Text Representation from BERT: An Empirical Study</a></td>
</tr>
<tr>
<td>0</td>
<td>0.803753</td>
<td>0.990667</td>
<td><a href="https://www.semanticscholar.org/paper/b4980f5673152fb925c2422cc8118abc4718e78a">175: SciBERT: Pretrained Contextualized Embeddings for Scientific Text</a></td>
</tr>
<tr>
<td>0</td>
<td>0.802522</td>
<td>0.999122</td>
<td><a href="https://www.semanticscholar.org/paper/455a8838cde44f288d456d01c76ede95b56dc675">581: A Structural Probe for Finding Syntax in Word Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.830750</td>
<td>0.998791</td>
<td><a href="https://www.semanticscholar.org/paper/f6fbb6809374ca57205bd2cf1421d4f4fa04f975">429: Linguistic Knowledge and Transferability of Contextual Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.723065</td>
<td>0.998781</td>
<td><a href="https://www.semanticscholar.org/paper/0427110f0e79f41e69a8eb00a3ec8868bac26a4f">126: Do NLP Models Know Numbers? Probing Numeracy in Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.769945</td>
<td>0.998575</td>
<td><a href="https://www.semanticscholar.org/paper/5744f56d3253bd7c4341d36de40a93fceaa266b3">172: Semantics-aware BERT for Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.733765</td>
<td>0.998201</td>
<td><a href="https://www.semanticscholar.org/paper/199ff73d2f728e997f860b62a2322823d3e3d9e8">241: Designing and Interpreting Probes with Control Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.750786</td>
<td>0.998108</td>
<td><a href="https://www.semanticscholar.org/paper/97906df07855b029b7aae7c2a1c6c5e8df1d531c">653: BERT Rediscovers the Classical NLP Pipeline</a></td>
</tr>
<tr>
<td>0</td>
<td>0.738735</td>
<td>0.998015</td>
<td><a href="https://www.semanticscholar.org/paper/335613303ebc5eac98de757ed02a56377d99e03a">544: What Does BERT Learn about the Structure of Language?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.686485</td>
<td>0.997755</td>
<td><a href="https://www.semanticscholar.org/paper/031e4e43aaffd7a479738dcea69a2d5be7957aa3">381: ERNIE: Enhanced Representation through Knowledge Integration</a></td>
</tr>
<tr>
<td>0</td>
<td>0.598323</td>
<td>0.997700</td>
<td><a href="https://www.semanticscholar.org/paper/efeab0dcdb4c1cce5e537e57745d84774be99b9a">311: Assessing BERT's Syntactic Abilities</a></td>
</tr>
</table></html>
