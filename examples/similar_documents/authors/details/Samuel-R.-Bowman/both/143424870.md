<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/d9f6ada77448664b71128bb19df15765336974a6">797: SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.823453</td>
<td>0.947371</td>
<td><a href="https://www.semanticscholar.org/paper/5fe78eb0f142902237df11cb67c455787a759172">26: GLGE: A New General Language Generation Evaluation Benchmark</a></td>
</tr>
<tr>
<td>0</td>
<td>0.812533</td>
<td>-0.075845</td>
<td><a href="https://www.semanticscholar.org/paper/d994a6ae735eb9313f03fac1da1d01934e9ca201">0: The First Five Years of anAutomated Language</a></td>
</tr>
<tr>
<td>0</td>
<td>0.778131</td>
<td>0.657373</td>
<td><a href="https://www.semanticscholar.org/paper/3b618878525c8bb5ea64c84cd16b902c29ccfea2">3: Making a Point: Pointer-Generator Transformers for Disjoint Vocabularies</a></td>
</tr>
<tr>
<td>0</td>
<td>0.765362</td>
<td>0.992685</td>
<td><a href="https://www.semanticscholar.org/paper/88167f36dced91c279162d68af7225f2b4e2091c">3: Pre-Training a Language Model Without Human Language</a></td>
</tr>
<tr>
<td>0</td>
<td>0.756680</td>
<td>0.784557</td>
<td><a href="https://www.semanticscholar.org/paper/5e585d1fadffa2c25a297f988554c2e0e3373840">20: Smarnet: Teaching Machines to Read and Comprehend Like Human</a></td>
</tr>
<tr>
<td>0</td>
<td>0.754829</td>
<td>0.821256</td>
<td><a href="https://www.semanticscholar.org/paper/2e35a88aab7b5636ca8ec68967c0e91fee7e9583">13: Unsupervised Paraphrase Generation using Pre-trained Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.754542</td>
<td>0.990332</td>
<td><a href="https://www.semanticscholar.org/paper/b8568fe3218f0ab40447891c70b43657624d8a76">2: Transformers: "The End of History" for Natural Language Processing?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.748546</td>
<td>0.898311</td>
<td><a href="https://www.semanticscholar.org/paper/2c7cf0fe2daf9b59b3b127fc38d49915bd643f59">0: Effect and Analysis of Large-scale Language Model Rescoring on Competitive ASR Systems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.747773</td>
<td>0.955792</td>
<td><a href="https://www.semanticscholar.org/paper/4ecf8a6c196c3c484e09dfa65e764e8b6236aa62">0: On the Universality of Deep Contextual Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.591536</td>
<td>0.998933</td>
<td><a href="https://www.semanticscholar.org/paper/81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85">248: How Can We Know What Language Models Know?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.674758</td>
<td>0.997793</td>
<td><a href="https://www.semanticscholar.org/paper/c2a79e2a65b721d4de5f6d4806323174b9f8f393">17: Towards Zero-Label Language Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.795461</td>
<td>0.997679</td>
<td><a href="https://www.semanticscholar.org/paper/93b8da28d006415866bf48f9a6e06b5242129195">2544: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.706813</td>
<td>0.997627</td>
<td><a href="https://www.semanticscholar.org/paper/05f5f8b2065a520846d89771ebaea2bb1534e9c6">352: DeBERTa: Decoding-enhanced BERT with Disentangled Attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.704849</td>
<td>0.997559</td>
<td><a href="https://www.semanticscholar.org/paper/d0086b86103a620a86bc918746df0aa642e2a8a3">668: Language Models as Knowledge Bases?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.697578</td>
<td>0.997477</td>
<td><a href="https://www.semanticscholar.org/paper/8ae9a17c87a4518b513e860683a0ef7824be994d">281: Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference</a></td>
</tr>
<tr>
<td>0</td>
<td>0.700996</td>
<td>0.997407</td>
<td><a href="https://www.semanticscholar.org/paper/0e002114cd379efaca0ec5cda6d262b5fe0be104">83: MPNet: Masked and Permuted Pre-training for Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.727986</td>
<td>0.997288</td>
<td><a href="https://www.semanticscholar.org/paper/b47381e04739ea3f392ba6c8faaf64105493c196">258: Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.693712</td>
<td>0.997135</td>
<td><a href="https://www.semanticscholar.org/paper/e816f788767eec6a8ef0ea9eddd0e902435d4271">720: Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks</a></td>
</tr>
</table></html>
