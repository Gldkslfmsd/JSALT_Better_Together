<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/256623ff025f36d343588bcd0b966c1fd26afcf8">28: Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.884886</td>
<td>0.985285</td>
<td><a href="https://www.semanticscholar.org/paper/06a1bf4a7333bbc78dbd7470666b33bd9e26882b">70: Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.807543</td>
<td>0.882118</td>
<td><a href="https://www.semanticscholar.org/paper/03ca1715ad0637a1ab34343ff343947f17dd1082">0: On the Role of Pre-trained Language Models in Word Ordering: A Case Study with BART</a></td>
</tr>
<tr>
<td>0</td>
<td>0.804943</td>
<td>0.712906</td>
<td><a href="https://www.semanticscholar.org/paper/a9c9b81f05d8bc274182d6c6f3cc0b70d5c73cfa">11: Like a Baby: Visually Situated Neural Language Acquisition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.791184</td>
<td>0.952186</td>
<td><a href="https://www.semanticscholar.org/paper/d56c1fc337fb07ec004dc846f80582c327af717c">138: StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.790885</td>
<td>0.988425</td>
<td><a href="https://www.semanticscholar.org/paper/c6dc2f21e943c5a1dd35fb3d6ff18525c9c86ca0">17: Infusing Finetuning with Semantic Dependencies</a></td>
</tr>
<tr>
<td>0</td>
<td>0.786293</td>
<td>0.851403</td>
<td><a href="https://www.semanticscholar.org/paper/bfe973d679b431276bc1dcb3a019a0272f14ffe9">0: Neural Attentions for Natural Language Understanding and Modeling by Hongyin Luo</a></td>
</tr>
<tr>
<td>0</td>
<td>0.785604</td>
<td>0.779362</td>
<td><a href="https://www.semanticscholar.org/paper/08e87bf439354546d94ebe0449dc1bfb430efb27">0: Transforming Second Language Acquisition Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.783247</td>
<td>0.792640</td>
<td><a href="https://www.semanticscholar.org/paper/de7feb30c1f37c253d40ec5ea37e6039dd3a5c8b">8: Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study</a></td>
</tr>
<tr>
<td>0</td>
<td>0.775544</td>
<td>0.771439</td>
<td><a href="https://www.semanticscholar.org/paper/16dbd07e8ca7473f95acccffba1995cdb773863f">4: Sentence-Level BERT and Multi-Task Learning of Age and Gender in Social Media</a></td>
</tr>
<tr>
<td>0</td>
<td>0.737913</td>
<td>0.996196</td>
<td><a href="https://www.semanticscholar.org/paper/37f22855ec98ef7fc0958bb6898409bc5bc6a9f0">0: Intermediate Task Model Intermediate Task Output BERT Input Text Intermediate Task Model Intermediate Task Output ELMo BiLSTM Input Text Target Task Model Target Task Output Intermediate Task-Trained BERT Input Text Pretraining Task Model Pretraining Task Output BiLSTM</a></td>
</tr>
<tr>
<td>0</td>
<td>0.610161</td>
<td>0.994913</td>
<td><a href="https://www.semanticscholar.org/paper/b8022c096160ea0e04b67a9635a5069ab45b065c">31: Medical Exam Question Answering with Large-scale Reading Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.729295</td>
<td>0.994831</td>
<td><a href="https://www.semanticscholar.org/paper/aef4e4ff1f154d0f36ae597aebfe9eba358dc6e6">5: Transfer fine-tuning of BERT with phrasal paraphrases</a></td>
</tr>
<tr>
<td>0</td>
<td>0.698249</td>
<td>0.994624</td>
<td><a href="https://www.semanticscholar.org/paper/aba19f5ae0747dd1d02137ca596a74e70514a246">1: Low-Resource Named Entity Recognition via the Pre-Training Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.738062</td>
<td>0.994532</td>
<td><a href="https://www.semanticscholar.org/paper/b2a8c682683bbe0362d13f378d4f20002433a479">3: Do We Need Word Order Information for Cross-lingual Sequence Labeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.728065</td>
<td>0.994317</td>
<td><a href="https://www.semanticscholar.org/paper/1941f5b053ccc80fa44980d38ac074145591b4ec">12: A Bilingual Generative Transformer for Semantic Sentence Embedding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.698934</td>
<td>0.993941</td>
<td><a href="https://www.semanticscholar.org/paper/696ce5df90ab3fdae43b482c1cc673ff98e54605">7: Enhancing Natural Language Inference Using New and Expanded Training Data Sets and New Learning Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.593784</td>
<td>0.993940</td>
<td><a href="https://www.semanticscholar.org/paper/a9d112e4f6997a50d39001c9feb23c2f53cfc546">4: Jamo Pair Encoding: Subcharacter Representation-based Extreme Korean Vocabulary Compression for Efficient Subword Tokenization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.740482</td>
<td>0.993885</td>
<td><a href="https://www.semanticscholar.org/paper/e2a9e0cb1d5376d5f03d9e3f12cf962c49adb133">18: I Know What You Want: Semantic Learning for Text Comprehension</a></td>
</tr>
</table></html>
