<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/83b83ee4f27388445bdebb199cd75e5bf546dd85">66: The RepEval 2017 Shared Task: Multi-Genre Natural Language Inference with Sentence Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.821359</td>
<td>0.908192</td>
<td><a href="https://www.semanticscholar.org/paper/3f924938922c558d66fe3cab05b716c0b6e07b24">6: Sentence Pair Scoring: Towards Unified Framework for Text Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.821349</td>
<td>0.821427</td>
<td><a href="https://www.semanticscholar.org/paper/b186f3a65fdad24043051ce7cbc7a142ac431c7f">21: Complex Word Identification: Convolutional Neural Network vs. Feature Engineering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.820706</td>
<td>0.917518</td>
<td><a href="https://www.semanticscholar.org/paper/62708aafdd348a6fa1147be8f9b9afb91016e0d7">29: Sentence Pair Scoring: Towards Unified Framework for Text Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.817602</td>
<td>0.626373</td>
<td><a href="https://www.semanticscholar.org/paper/0e2933d2d2a10410e1a8cee07e47836c3245d38a">3: The ILSP/ARC submission to the WMT 2018 Parallel Corpus Filtering Shared Task</a></td>
</tr>
<tr>
<td>0</td>
<td>0.806135</td>
<td>0.789943</td>
<td><a href="https://www.semanticscholar.org/paper/350e78cfc83471276008a4a8023236accff6da50">41: Unbabelâ€™s Participation in the WMT16 Word-Level Translation Quality Estimation Shared Task</a></td>
</tr>
<tr>
<td>0</td>
<td>0.804908</td>
<td>0.854376</td>
<td><a href="https://www.semanticscholar.org/paper/dd2a14ca5a678695c2ab2ee23069321db86c9846">33: Split and Rephrase: Better Evaluation and Stronger Baselines</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793630</td>
<td>0.808271</td>
<td><a href="https://www.semanticscholar.org/paper/f79f7c9f6984a7acb41c6bf758d502b68d33ce81">4: Skipping Word: A Character-Sequential Representation based Framework for Question Answering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793413</td>
<td>0.752311</td>
<td><a href="https://www.semanticscholar.org/paper/5a908350fba3f464191eaa95df97f575a2f5ea2b">1: A label-oriented loss function for learning sentence representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792875</td>
<td>0.874156</td>
<td><a href="https://www.semanticscholar.org/paper/296aaba05c9a790f48b501d14f82c562d202c2e5">4: Pairwise Supervised Contrastive Learning of Sentence Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.820711</td>
<td>0.994633</td>
<td><a href="https://www.semanticscholar.org/paper/8775df853e1d6ab440685c00194deb2a2d1d2e4d">30: Sentence embeddings in NLI with iterative refinement encoders</a></td>
</tr>
<tr>
<td>0</td>
<td>0.604106</td>
<td>0.993628</td>
<td><a href="https://www.semanticscholar.org/paper/b8022c096160ea0e04b67a9635a5069ab45b065c">31: Medical Exam Question Answering with Large-scale Reading Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.727497</td>
<td>0.993612</td>
<td><a href="https://www.semanticscholar.org/paper/696ce5df90ab3fdae43b482c1cc673ff98e54605">7: Enhancing Natural Language Inference Using New and Expanded Training Data Sets and New Learning Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.807350</td>
<td>0.993545</td>
<td><a href="https://www.semanticscholar.org/paper/e44da7d8c71edcc6e575fa7faadd5e75785a7901">372: Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.813285</td>
<td>0.993218</td>
<td><a href="https://www.semanticscholar.org/paper/624527d8b92bc6d423784113ded0b9fd639add00">54: Pay Attention to the Ending:Strong Neural Baselines for the ROC Story Cloze Task</a></td>
</tr>
<tr>
<td>0</td>
<td>0.813483</td>
<td>0.992941</td>
<td><a href="https://www.semanticscholar.org/paper/256623ff025f36d343588bcd0b966c1fd26afcf8">28: Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.716853</td>
<td>0.992745</td>
<td><a href="https://www.semanticscholar.org/paper/83e7654d545fbbaaf2328df365a781fb67b841b4">782: Enhanced LSTM for Natural Language Inference</a></td>
</tr>
<tr>
<td>0</td>
<td>0.746530</td>
<td>0.992573</td>
<td><a href="https://www.semanticscholar.org/paper/e3633b1fa51fd6a199d7db719576cb5ed257580b">6: YNU-HPCC at Semeval-2018 Task 11: Using an Attention-based CNN-LSTM for Machine Comprehension using Commonsense Knowledge</a></td>
</tr>
<tr>
<td>0</td>
<td>0.740795</td>
<td>0.992472</td>
<td><a href="https://www.semanticscholar.org/paper/5bfb0b5494885d35bc15952c025fa2d8fbbd8c98">25: Explicit Contextual Semantics for Text Comprehension</a></td>
</tr>
</table></html>
