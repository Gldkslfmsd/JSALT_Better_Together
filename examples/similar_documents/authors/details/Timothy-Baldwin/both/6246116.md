<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/9937d5b404662c56b33fcbfa35453b72d250b319">118: Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.807976</td>
<td>0.970479</td>
<td><a href="https://www.semanticscholar.org/paper/0bb3a0d139e47608ec567fad150029c985c81868">71: Dict2vec : Learning Word Embeddings using Lexical Dictionaries</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800889</td>
<td>0.934686</td>
<td><a href="https://www.semanticscholar.org/paper/0e91275210db9fe154139930af2c0f42878771bb">57: A Strong Baseline for Learning Cross-Lingual Word Embeddings from Sentence Alignments</a></td>
</tr>
<tr>
<td>0</td>
<td>0.796083</td>
<td>0.920614</td>
<td><a href="https://www.semanticscholar.org/paper/5fc7b4dbc154bbbf26d8cee2f18f31ecbf286bcf">31: Generalizing Word Embeddings using Bag of Subwords</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793682</td>
<td>0.945588</td>
<td><a href="https://www.semanticscholar.org/paper/2c3210a10f207795153b28c7d4fb1242a6ce05c1">2: A Stronger Baseline for Multilingual Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792756</td>
<td>0.922583</td>
<td><a href="https://www.semanticscholar.org/paper/fffc562b101e849133ede7ca8c3d6dad0b704a37">1: Interpreting Word Embeddings Using a Distribution Agnostic Approach Employing Hellinger Distance</a></td>
</tr>
<tr>
<td>0</td>
<td>0.790840</td>
<td>0.682913</td>
<td><a href="https://www.semanticscholar.org/paper/ec55ba0139d5234144ccef99ec870c782909b741">0: Word Embedding Fine-Tuning using Graph Neural Networks on Local Word Co-Ocurrence Graphs</a></td>
</tr>
<tr>
<td>0</td>
<td>0.778010</td>
<td>0.698565</td>
<td><a href="https://www.semanticscholar.org/paper/cd5a9a0061de6a6841c63e60281133207b2d6763">1: Learning to Describe Phrases with Local and Global Contexts</a></td>
</tr>
<tr>
<td>0</td>
<td>0.776849</td>
<td>0.883402</td>
<td><a href="https://www.semanticscholar.org/paper/40c531fb0267437b0f76fd8f0080fb4de9ffe146">16: A Systematic Study of Leveraging Subword Information for Learning Word Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.776346</td>
<td>0.882649</td>
<td><a href="https://www.semanticscholar.org/paper/7ac4ddb109d84b25025a5a7fe3ed59bc6ab87685">20: Deep Learning Architecture for Complex Word Identification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.783030</td>
<td>0.997063</td>
<td><a href="https://www.semanticscholar.org/paper/0fc033f32f420ed3ff4330f60ccd0686db3deaea">79: Learning Term Embeddings for Hypernymy Identification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.743684</td>
<td>0.996941</td>
<td><a href="https://www.semanticscholar.org/paper/5f1697b60c10e76617463ecd998211d8d933022d">241: Learning Semantic Hierarchies via Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.719396</td>
<td>0.995954</td>
<td><a href="https://www.semanticscholar.org/paper/a6fe222d56ebcafab2322c5561bea1ec3e7413f9">51: Single or Multiple? Combining Word Representations Independently Learned from Text and WordNet</a></td>
</tr>
<tr>
<td>0</td>
<td>0.812861</td>
<td>0.995821</td>
<td><a href="https://www.semanticscholar.org/paper/d691814e162b29725871a75e8390430dd4c6fd7a">277: SensEmbed: Learning Sense Embeddings for Word and Relational Similarity</a></td>
</tr>
<tr>
<td>0</td>
<td>0.702493</td>
<td>0.995591</td>
<td><a href="https://www.semanticscholar.org/paper/bad2bbfc9b5cf6c93ece1731ac9389bf08407be0">189: Learning to Distinguish Hypernyms and Co-Hyponyms</a></td>
</tr>
<tr>
<td>0</td>
<td>0.768061</td>
<td>0.995563</td>
<td><a href="https://www.semanticscholar.org/paper/47291646a01c8786abd1b168cb78e6af575f9318">278: AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes</a></td>
</tr>
<tr>
<td>0</td>
<td>0.713274</td>
<td>0.995399</td>
<td><a href="https://www.semanticscholar.org/paper/12185cdbaae93de59f5dbb765fbecf3d9fde767c">133: Specializing Word Embeddings for Similarity or Relatedness</a></td>
</tr>
<tr>
<td>0</td>
<td>0.743792</td>
<td>0.994812</td>
<td><a href="https://www.semanticscholar.org/paper/c8f09a71fa6b2a4f5ae86296e5c22ae75414a916">114: Symmetric Pattern Based Word Embeddings for Improved Word Similarity Prediction</a></td>
</tr>
<tr>
<td>0</td>
<td>0.785554</td>
<td>0.994686</td>
<td><a href="https://www.semanticscholar.org/paper/d31dbecd516d216365a53a1da3580b014f16799b">1: Learning Relation Representations from Word Representations</a></td>
</tr>
</table></html>
