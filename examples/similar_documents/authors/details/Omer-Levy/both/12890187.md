<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/2012f32199adc88747d5a1b47c7b4ba1cb3cb995">1184: word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method</a></td>
</tr>
<tr>
<td>0</td>
<td>0.822043</td>
<td>0.827702</td>
<td><a href="https://www.semanticscholar.org/paper/e609bbdd47d5b6109ea7e2599eea84bd8e4b9936">0: Corrected CBOW Performs as well as Skip-gram</a></td>
</tr>
<tr>
<td>0</td>
<td>0.817564</td>
<td>0.961147</td>
<td><a href="https://www.semanticscholar.org/paper/677dfb804a7608624f988d16785a608cced790a6">2: Analogies Explained: Towards Understanding Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.817182</td>
<td>0.948861</td>
<td><a href="https://www.semanticscholar.org/paper/5eef4b688bf1062596f6e973e2fc4f1b675f09e9">68: Analogies Explained: Towards Understanding Word Embeddings</a></td>
</tr>
<tr>
<td>1</td>
<td>0.810884</td>
<td>0.987093</td>
<td><a href="https://www.semanticscholar.org/paper/66d0c1a5933caa3e5f1e21d7cb676ca2bfab32ab">473: An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809607</td>
<td>0.973730</td>
<td><a href="https://www.semanticscholar.org/paper/720ad49010638bcf93de3a58689de84a047d3040">17: What the Vec? Towards Probabilistically Grounded Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.805580</td>
<td>0.891300</td>
<td><a href="https://www.semanticscholar.org/paper/5ea131f148299adb97e36a2001b08c468b4bfff0">0: PolyLM: Learning about Polysemy through Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800263</td>
<td>0.320161</td>
<td><a href="https://www.semanticscholar.org/paper/77d792f7fac0e67660a47f39a838ed30c3da2662">41: Evaluating Word String Embeddings and Loss Functions for CNN-Based Word Spotting</a></td>
</tr>
<tr>
<td>0</td>
<td>0.798733</td>
<td>0.893574</td>
<td><a href="https://www.semanticscholar.org/paper/fffc562b101e849133ede7ca8c3d6dad0b704a37">1: Interpreting Word Embeddings Using a Distribution Agnostic Approach Employing Hellinger Distance</a></td>
</tr>
<tr>
<td>0</td>
<td>0.796915</td>
<td>0.621060</td>
<td><a href="https://www.semanticscholar.org/paper/6a91f9be910600088ba4cd257ca38e796c271b66">3: Low-Resource Unsupervised NMT: Diagnosing the Problem and Providing a Linguistically Motivated Solution</a></td>
</tr>
<tr>
<td>0</td>
<td>0.832523</td>
<td>0.997154</td>
<td><a href="https://www.semanticscholar.org/paper/940e8c477f3e7ddb1d3aa2f216a38c8f9486e544">567: word2vec Parameter Learning Explained</a></td>
</tr>
<tr>
<td>0</td>
<td>0.699645</td>
<td>0.992953</td>
<td><a href="https://www.semanticscholar.org/paper/f527bcfb09f32e6a4a8afc0b37504941c1ba2cee">6938: Distributed Representations of Sentences and Documents</a></td>
</tr>
<tr>
<td>0</td>
<td>0.726522</td>
<td>0.991085</td>
<td><a href="https://www.semanticscholar.org/paper/cc14a56eb0361261f9294646a727dc853813c532">106: Word Embedding Revisited: A New Representation Learning and Explicit Matrix Factorization Perspective</a></td>
</tr>
<tr>
<td>0</td>
<td>0.742037</td>
<td>0.990976</td>
<td><a href="https://www.semanticscholar.org/paper/04ebd82c48a580476fc5acad61b8ee036f92f1f5">300: Document Embedding with Paragraph Vectors</a></td>
</tr>
<tr>
<td>0</td>
<td>0.597817</td>
<td>0.989388</td>
<td><a href="https://www.semanticscholar.org/paper/75e143e08b69b56b3c02e80f95ed4de7ffb9ffe8">135: Medical Semantic Similarity with a Neural Language Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.750565</td>
<td>0.989018</td>
<td><a href="https://www.semanticscholar.org/paper/f4c018bcc8ea707b83247866bdc8ccb87cd9f5da">1561: Neural Word Embedding as Implicit Matrix Factorization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.719499</td>
<td>0.986132</td>
<td><a href="https://www.semanticscholar.org/paper/e398d9d7e090a8d6f906b5da59925da212f6bc51">164: Representation learning for very short texts using weighted word embedding aggregation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.660275</td>
<td>0.985578</td>
<td><a href="https://www.semanticscholar.org/paper/c5dba6ade9795f6ba42a011b16929bcc34d4ca58">79: Hierarchical Neural Language Models for Joint Representation of Streaming Documents and their Content</a></td>
</tr>
</table></html>
