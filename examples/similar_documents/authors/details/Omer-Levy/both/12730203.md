<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/500d570ce02abf42bc1bc535620741d4c5665e6a">625: Linguistic Regularities in Sparse and Explicit Word Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.858547</td>
<td>0.925127</td>
<td><a href="https://www.semanticscholar.org/paper/1f32b9520eb707ade82466184875865c075a9d0e">3: Geometric Relationship between Word and Context Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.846120</td>
<td>0.978762</td>
<td><a href="https://www.semanticscholar.org/paper/d483f9a0e0bc23477311341fb8f72462c0d97c33">50: Rehabilitation of Count-Based Models for Word Vector Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826619</td>
<td>0.000040</td>
<td><a href="https://www.semanticscholar.org/paper/f6ff54a82509f48847025621a4fa543902116f2b">0: Towards a Theoretical Understanding of Word and Relation Representation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.816550</td>
<td>0.973267</td>
<td><a href="https://www.semanticscholar.org/paper/7b3e09b3b0ebdc3f390f865a3da4233e6bfb49e8">76: Learning Sense-specific Word Embeddings By Exploiting Bilingual Resources</a></td>
</tr>
<tr>
<td>0</td>
<td>0.810350</td>
<td>0.723017</td>
<td><a href="https://www.semanticscholar.org/paper/de2899d6c850e9d57b80c471f8a06f669afa1812">6: Word Embeddings for Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.810011</td>
<td>0.941470</td>
<td><a href="https://www.semanticscholar.org/paper/dcbebb8fbd3ebef2816ebe0f7da12340a5725a8b">16: Wiktionary-based word embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809229</td>
<td>0.650080</td>
<td><a href="https://www.semanticscholar.org/paper/a9a5d671271fff45429084e184a788f611b6f194">88: FRAGE: Frequency-Agnostic Word Representation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.805581</td>
<td>0.921696</td>
<td><a href="https://www.semanticscholar.org/paper/a7ecf0e80f95867c582bba3ef34562b309503358">0: Estimator Vectors: OOV Word Embeddings based on Subword and Context Clue Estimates</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800120</td>
<td>0.986746</td>
<td><a href="https://www.semanticscholar.org/paper/fbff507f0e60496f15558ae1f7175cb9dcaa5f38">7: Angular-Based Word Meta-Embedding Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.665562</td>
<td>0.998809</td>
<td><a href="https://www.semanticscholar.org/paper/a41b880cdd9646578ab13e6e0b5356a0c4370811">462: Evaluation methods for unsupervised word embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.795057</td>
<td>0.998667</td>
<td><a href="https://www.semanticscholar.org/paper/6aa3d8bcca2ebdc52ef7cd786204c338f9d609f2">1171: Improving Distributional Similarity with Lessons Learned from Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.705281</td>
<td>0.996118</td>
<td><a href="https://www.semanticscholar.org/paper/113ebda58e23f0275d49497800da2907c9865bb4">35: Evaluation of Domain-specific Word Embeddings using Knowledge Resources</a></td>
</tr>
<tr>
<td>0</td>
<td>0.713239</td>
<td>0.996106</td>
<td><a href="https://www.semanticscholar.org/paper/0183b3e9d84c15c7048e6c2149ed86257ccdc6cb">824: Dependency-Based Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.789509</td>
<td>0.995686</td>
<td><a href="https://www.semanticscholar.org/paper/686b52953471a9d7a515215ba54ad0350c6b0472">101: Word Embeddings, Analogies, and Machine Learning: Beyond king - man + woman = queen</a></td>
</tr>
<tr>
<td>0</td>
<td>0.723606</td>
<td>0.995635</td>
<td><a href="https://www.semanticscholar.org/paper/808a225d7a943041612411b23c0b68f63b89a437">95: Word Embedding-based Antonym Detection using Thesauri and Distributional Information</a></td>
</tr>
<tr>
<td>0</td>
<td>0.773882</td>
<td>0.995467</td>
<td><a href="https://www.semanticscholar.org/paper/6a9b5bcafc5e88d731f644cf7ea59547df20495a">44: Online Learning of Interpretable Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.788508</td>
<td>0.995298</td>
<td><a href="https://www.semanticscholar.org/paper/34e1c23ccf3bbff0199c39c6c845cce866a91f78">17: Predicting Word Embeddings Variability</a></td>
</tr>
<tr>
<td>0</td>
<td>0.675060</td>
<td>0.994707</td>
<td><a href="https://www.semanticscholar.org/paper/3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118">1303: Donâ€™t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</a></td>
</tr>
</table></html>
