<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/395de0bd3837fdf4b4b5e5f04835bcc69c279481">2258: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.810538</td>
<td>0.852971</td>
<td><a href="https://www.semanticscholar.org/paper/41a78e2885b5dc8c719495a33985b5f4880f5b48">346: Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809270</td>
<td>0.927964</td>
<td><a href="https://www.semanticscholar.org/paper/582db0aa9a26acb9b380286052737cdc5348e696">0: Improving Neural Machine Translation by Denoising Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800818</td>
<td>0.959428</td>
<td><a href="https://www.semanticscholar.org/paper/12442420adf1c36887fafd108f4b7f4fc822ae60">129: Revisiting Self-Training for Neural Sequence Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.794783</td>
<td>0.954799</td>
<td><a href="https://www.semanticscholar.org/paper/20b676b433448818d09febbd5e32ab4668c99417">21: Pointer: Constrained Text Generation via Insertion-based Generative Pre-training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.790410</td>
<td>0.791126</td>
<td><a href="https://www.semanticscholar.org/paper/6a8702f18cc3a2c14d0a02dc201529f8aa6d07c6">9: Any-to-One Sequence-to-Sequence Voice Conversion Using Self-Supervised Discrete Speech Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.788558</td>
<td>0.852600</td>
<td><a href="https://www.semanticscholar.org/paper/0fc47b976051366cc1b1ec00e0f3d512deb3232a">47: Structural Neural Encoders for AMR-to-text Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.787878</td>
<td>0.867885</td>
<td><a href="https://www.semanticscholar.org/paper/ca7ce5099a575c14b7de69f2c498b794f5b1f67a">6: What BERT Sees: Cross-Modal Transfer for Visual Question Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.787622</td>
<td>0.815535</td>
<td><a href="https://www.semanticscholar.org/paper/c9331790d49f36d4a3f9c6d4ff01caf7ecfc92cc">0: Training and Inference Methods for High-Coverage Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.787373</td>
<td>0.916703</td>
<td><a href="https://www.semanticscholar.org/paper/790a5da9268e28794d82fa4df648691200445f34">0: Multiformer: A Head-Configurable Transformer-Based Model for Direct Speech Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.744239</td>
<td>0.997729</td>
<td><a href="https://www.semanticscholar.org/paper/d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5">16: Pretrained Language Models for Text Generation: A Survey</a></td>
</tr>
<tr>
<td>0</td>
<td>0.799870</td>
<td>0.997541</td>
<td><a href="https://www.semanticscholar.org/paper/1c71771c701aadfd72c5866170a9f5d71464bb88">695: Unified Language Model Pre-training for Natural Language Understanding and Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.777228</td>
<td>0.997510</td>
<td><a href="https://www.semanticscholar.org/paper/74276a37bfa50f90dfae37f767b2b67784bd402a">350: mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer</a></td>
</tr>
<tr>
<td>0</td>
<td>0.774007</td>
<td>0.997037</td>
<td><a href="https://www.semanticscholar.org/paper/092890a1f937655ae1b7ee0ed1d748798645c8b7">2: Bridging Cross-Lingual Gaps During Leveraging the Multilingual Sequence-to-Sequence Pretraining for Text Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.770459</td>
<td>0.996983</td>
<td><a href="https://www.semanticscholar.org/paper/b42e3a759348f27cca2f918a6bd0b139a5312e44">2: A Survey of Pretrained Language Models Based Text Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.626287</td>
<td>0.996397</td>
<td><a href="https://www.semanticscholar.org/paper/cdf13796e28639711a693e5410f06fa3b9b650f6">76: Can You Unpack That? Learning to Rewrite Questions-in-Context</a></td>
</tr>
<tr>
<td>0</td>
<td>0.625823</td>
<td>0.996013</td>
<td><a href="https://www.semanticscholar.org/paper/9a21740d87976bf76f4a9668a9da631035302fb2">59: Attention Is Not Only a Weight: Analyzing Transformers with Vector Norms</a></td>
</tr>
<tr>
<td>0</td>
<td>0.745021</td>
<td>0.996008</td>
<td><a href="https://www.semanticscholar.org/paper/ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc">1474: Cross-lingual Language Model Pretraining</a></td>
</tr>
<tr>
<td>0</td>
<td>0.713809</td>
<td>0.995945</td>
<td><a href="https://www.semanticscholar.org/paper/cc1847133e1ef0e5555eab726de5be91e38d9f23">5: Towards Zero-Shot Multilingual Synthetic Question and Answer Generation for Cross-Lingual Reading Comprehension</a></td>
</tr>
</table></html>
