<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/2cf3bd0cc1382f35384e259d99e4f9744eeaed28">83: Blockwise Self-Attention for Long Document Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.806106</td>
<td>0.827558</td>
<td><a href="https://www.semanticscholar.org/paper/5256b6d0ffe5b0dbcd979e2a8404326732b5ed51">138: Coarse-to-Fine Question Answering for Long Documents</a></td>
</tr>
<tr>
<td>0</td>
<td>0.805399</td>
<td>0.965593</td>
<td><a href="https://www.semanticscholar.org/paper/9dc624d7258d1a56117ca720aea953ce46b66b21">30: Efficient Attentions for Long Document Summarization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.788430</td>
<td>0.853806</td>
<td><a href="https://www.semanticscholar.org/paper/3bec5c111a2d33d0b6cb3f1a5c0e5e71f461eeee">0: RE-Net : A Character-Based Model Replicating R-Net</a></td>
</tr>
<tr>
<td>0</td>
<td>0.782854</td>
<td>0.927832</td>
<td><a href="https://www.semanticscholar.org/paper/431ad023149287abc496d61570ba167fb014cf54">2: Maximal Multiverse Learning for Promoting Cross-Task Generalization of Fine-Tuned Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.779085</td>
<td>0.972761</td>
<td><a href="https://www.semanticscholar.org/paper/c889d0b6c2ef833ce344607d88f254fa892e344c">2: Structural analysis of an all-purpose question answering model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.773711</td>
<td>0.971683</td>
<td><a href="https://www.semanticscholar.org/paper/309ddaec87c97fb2e1958a7c82bc5444040c6ffe">3: Undivided Attention: Are Intermediate Layers Necessary for BERT?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.770214</td>
<td>0.830588</td>
<td><a href="https://www.semanticscholar.org/paper/7a79bf9e43df1779f2ef6fdf6df477f0404b6e36">0: Skim-Attention: Learning to Focus via Document Layout</a></td>
</tr>
<tr>
<td>0</td>
<td>0.766025</td>
<td>0.955074</td>
<td><a href="https://www.semanticscholar.org/paper/e39a4b182c3bae017b08df20b37b9d1d97c9a4bf">15: Multi-Stage Pretraining for Low-Resource Domain Adaptation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.764008</td>
<td>0.984731</td>
<td><a href="https://www.semanticscholar.org/paper/ed0d9ef9891cf19c5c428e41effe5fedfdb5386e">0: Robust Question Answering: A study on Self-Attention Mechanism and Data Augmentation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.756849</td>
<td>0.996294</td>
<td><a href="https://www.semanticscholar.org/paper/0822f8d7e6a72a65e65f147d3a8d8fccd485da40">30: Shortformer: Better Language Modeling using Shorter Inputs</a></td>
</tr>
<tr>
<td>0</td>
<td>0.633638</td>
<td>0.996093</td>
<td><a href="https://www.semanticscholar.org/paper/7e9ff94476f41041c75e253e84f487db00e9c861">165: Long Range Arena: A Benchmark for Efficient Transformers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.653597</td>
<td>0.995814</td>
<td><a href="https://www.semanticscholar.org/paper/dbe7ab77cf1a56009acdc97dc018552aff10d72d">0: Supplementary Material: Implementation and Experiments for GAU-based Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.669210</td>
<td>0.995591</td>
<td><a href="https://www.semanticscholar.org/paper/b15ea460c77a4ee8aa159a30ab0331deedfcf392">40: BASE Layers: Simplifying Training of Large, Sparse Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.704359</td>
<td>0.995510</td>
<td><a href="https://www.semanticscholar.org/paper/4badd753be64c5c5b57dd2bb2e515fbe0c0720d8">13: SparseBERT: Rethinking the Importance Analysis in Self-attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.612205</td>
<td>0.995125</td>
<td><a href="https://www.semanticscholar.org/paper/1554887c6bd76c443a477b27dbcab35877787b27">10: LightSeq: A High Performance Inference Library for Transformers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.567483</td>
<td>0.994628</td>
<td><a href="https://www.semanticscholar.org/paper/e34a59a7391a28200ff9052c13bd4498a8eaa4af">0: S CALE E FFICIENTLY : I NSIGHTS FROM P RE - TRAINING AND F INE - TUNING T RANSFORMERS</a></td>
</tr>
<tr>
<td>0</td>
<td>0.614391</td>
<td>0.994619</td>
<td><a href="https://www.semanticscholar.org/paper/2d4f66046bb436864cd6bf589e3a931c405f9f44">22: Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.627430</td>
<td>0.994612</td>
<td><a href="https://www.semanticscholar.org/paper/6914a7997ff4be207fa7b3472a9c5879abaec646">18: RealFormer: Transformer Likes Residual Attention</a></td>
</tr>
</table></html>
