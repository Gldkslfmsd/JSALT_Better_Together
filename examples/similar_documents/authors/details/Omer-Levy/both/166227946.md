<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/b03c7ff961822183bab66b2e594415e585d3fd09">390: Are Sixteen Heads Really Better than One?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.847143</td>
<td>0.894665</td>
<td><a href="https://www.semanticscholar.org/paper/9c21e69cc477b9c076964cd112022727cd1e4235">6: Differentiable Subset Pruning of Transformer Heads</a></td>
</tr>
<tr>
<td>0</td>
<td>0.833797</td>
<td>0.990607</td>
<td><a href="https://www.semanticscholar.org/paper/07a64686ce8e43ac475a8d820a8a9f1d87989583">427: Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809689</td>
<td>0.421745</td>
<td><a href="https://www.semanticscholar.org/paper/b431938300a5e2baa9c88b83d8fec13c087cb85b">8: AttentionLite: Towards Efficient Self-Attention Models for Vision</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809338</td>
<td>0.891600</td>
<td><a href="https://www.semanticscholar.org/paper/903fb78627f336841e7689c84d7b2576e9331f0a">1: Recurrent Attention for Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.806716</td>
<td>-0.017248</td>
<td>NA:232312493</td>
</tr>
<tr>
<td>0</td>
<td>0.792597</td>
<td>0.916561</td>
<td><a href="https://www.semanticscholar.org/paper/168fc3525f7b97695a97b04e257ee9bd1e832acb">5: Memory Transformer</a></td>
</tr>
<tr>
<td>0</td>
<td>0.786257</td>
<td>0.878295</td>
<td><a href="https://www.semanticscholar.org/paper/efaff8f2f2cc948465f1972d4abdcf08013ee31b">0: Interpreting Self-Attention Weights</a></td>
</tr>
<tr>
<td>0</td>
<td>0.784228</td>
<td>0.954649</td>
<td><a href="https://www.semanticscholar.org/paper/0fe8b49369d70a2be473435a82b01544704b3c9f">5: Evolving Attention with Residual Convolutions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.775943</td>
<td>0.899240</td>
<td><a href="https://www.semanticscholar.org/paper/a1fbd97c8b36f312d526fc2904cc2869089240f4">72: Tree Transformer: Integrating Tree Structures into Self-Attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.751975</td>
<td>0.994699</td>
<td><a href="https://www.semanticscholar.org/paper/daab62304d0b1beeddad06846eaadce9c7610d9d">2: On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.821845</td>
<td>0.994233</td>
<td><a href="https://www.semanticscholar.org/paper/b56e05c989f564cf65e25c66afbce8ecf0920a04">1: Layer-wise Pruning of Transformer Attention Heads for Efficient Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.726756</td>
<td>0.994081</td>
<td><a href="https://www.semanticscholar.org/paper/738215a396f6eee1709c6b521a6199769f0ce674">58: Compressing Large-Scale Transformer-Based Models: A Case Study on BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.737498</td>
<td>0.993845</td>
<td><a href="https://www.semanticscholar.org/paper/60a4a3a886338d0c8e3579d392cb32f493430255">177: MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.778071</td>
<td>0.992974</td>
<td><a href="https://www.semanticscholar.org/paper/f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1">240: Reducing Transformer Depth on Demand with Structured Dropout</a></td>
</tr>
<tr>
<td>0</td>
<td>0.675171</td>
<td>0.992803</td>
<td><a href="https://www.semanticscholar.org/paper/b5613da1f643159c97cbf8555d6f5c4f05b36a9a">2: High Performance Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.726899</td>
<td>0.992801</td>
<td><a href="https://www.semanticscholar.org/paper/b832638a7c8f8e9b51b2762e36fbd29733af63b2">2: Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space</a></td>
</tr>
<tr>
<td>0</td>
<td>0.728580</td>
<td>0.992549</td>
<td><a href="https://www.semanticscholar.org/paper/5d34881ff68bd203ff790187e7e5c9e034389cfa">112: FastBERT: a Self-distilling BERT with Adaptive Inference Time</a></td>
</tr>
<tr>
<td>0</td>
<td>0.675594</td>
<td>0.992271</td>
<td><a href="https://www.semanticscholar.org/paper/e51417a105b8ec64c918ff89a91fce30c86a1f3b">0: HetuMoE: An Efficient Trillion-scale Mixture-of-Expert Distributed Training System</a></td>
</tr>
</table></html>
