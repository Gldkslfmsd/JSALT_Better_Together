<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/8659bf379ca8756755125a487c43cfe8611ce842">261: To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.805057</td>
<td>0.966280</td>
<td><a href="https://www.semanticscholar.org/paper/c28b7dfe341f1e13a5a98efbce7946ef795cf9b8">21: SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer</a></td>
</tr>
<tr>
<td>0</td>
<td>0.794457</td>
<td>0.964161</td>
<td><a href="https://www.semanticscholar.org/paper/ef9b7f9b8682fd57a5543a6217a6b54d955ed3cb">1: Y-Tuning: An Efficient Tuning Paradigm for Large-Scale Pre-Trained Models via Label Representation Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.782202</td>
<td>0.961031</td>
<td><a href="https://www.semanticscholar.org/paper/e812919d2cd818e7262f01b32dc5e630fc825af1">53: Improving and Simplifying Pattern Exploiting Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.782129</td>
<td>0.965778</td>
<td><a href="https://www.semanticscholar.org/paper/ec936b808e0fab9281c050ad4010cddec92c8cbe">0: P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.766316</td>
<td>0.976143</td>
<td><a href="https://www.semanticscholar.org/paper/3cfb319689f06bf04c2e28399361f414ca32c4b3">3501: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></td>
</tr>
<tr>
<td>0</td>
<td>0.764154</td>
<td>0.754011</td>
<td><a href="https://www.semanticscholar.org/paper/21d4d97fed755a9ecd5b67a42e1c69008989738c">49: Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline</a></td>
</tr>
<tr>
<td>0</td>
<td>0.754086</td>
<td>0.896599</td>
<td><a href="https://www.semanticscholar.org/paper/4d7ad046b74d8cce8fd7ec2af0702e913d85b6cf">0: Dataset Augmentation and Mixture-Of-Experts Working In Concert For Few-Shot Domain Adaptation Transfer Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.752444</td>
<td>0.878401</td>
<td><a href="https://www.semanticscholar.org/paper/26ed89bff49545f8c6ddfd05ba9a9c59c926d322">5: MAGMA - Multimodal Augmentation of Generative Models through Adapter-based Finetuning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.750764</td>
<td>0.147396</td>
<td><a href="https://www.semanticscholar.org/paper/56de9b050c982c93d8c5439bef5e7dffcfaccf95">0: Universal Representations : TowardsMulti-Task Learning & Beyond</a></td>
</tr>
<tr>
<td>0</td>
<td>0.716346</td>
<td>0.997556</td>
<td><a href="https://www.semanticscholar.org/paper/658721bc13b0fa97366d38c05a96bf0a9f4bb0ac">714: Multi-Task Deep Neural Networks for Natural Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.723741</td>
<td>0.995751</td>
<td><a href="https://www.semanticscholar.org/paper/3bcb17559ce96eb20fa79af8194f4af0380d194a">366: Pre-trained Models for Natural Language Processing: A Survey</a></td>
</tr>
<tr>
<td>0</td>
<td>0.749201</td>
<td>0.995707</td>
<td><a href="https://www.semanticscholar.org/paper/9f1c5777a193b2c3bb2b25e248a156348e5ba56d">151: Cloze-driven Pretraining of Self-attention Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.719202</td>
<td>0.994971</td>
<td><a href="https://www.semanticscholar.org/paper/f6fbb6809374ca57205bd2cf1421d4f4fa04f975">429: Linguistic Knowledge and Transferability of Contextual Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.642959</td>
<td>0.994783</td>
<td><a href="https://www.semanticscholar.org/paper/473921de1b52f98f34f37afd507e57366ff7d1ca">50: CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters</a></td>
</tr>
<tr>
<td>0</td>
<td>0.652435</td>
<td>0.994433</td>
<td><a href="https://www.semanticscholar.org/paper/335613303ebc5eac98de757ed02a56377d99e03a">544: What Does BERT Learn about the Structure of Language?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.601878</td>
<td>0.994313</td>
<td><a href="https://www.semanticscholar.org/paper/455a8838cde44f288d456d01c76ede95b56dc675">581: A Structural Probe for Finding Syntax in Word Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.653572</td>
<td>0.994213</td>
<td><a href="https://www.semanticscholar.org/paper/443fbfd614f35061a679b7eeba7d9302d0626a7d">10: DoubleTransfer at MEDIQA 2019: Multi-Source Transfer Learning for Natural Language Understanding in the Medical Domain</a></td>
</tr>
<tr>
<td>0</td>
<td>0.733776</td>
<td>0.994206</td>
<td><a href="https://www.semanticscholar.org/paper/e2587eddd57bc4ba286d91b27c185083f16f40ee">484: What do you learn from context? Probing for sentence structure in contextualized word representations</a></td>
</tr>
</table></html>
