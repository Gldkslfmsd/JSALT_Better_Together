<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/baf60d13c98916b77b09bc525ede1cd610ed1db5">211: Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping</a></td>
</tr>
<tr>
<td>0</td>
<td>0.864989</td>
<td>0.975962</td>
<td><a href="https://www.semanticscholar.org/paper/5b4be79081e1d6eac83e8a9e7a38bd7c338a4c78">23: How fine can fine-tuning be? Learning efficient language models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.840585</td>
<td>0.984412</td>
<td><a href="https://www.semanticscholar.org/paper/8c62277dada489904a63de4dd87336c27c68fb5e">3: Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.820623</td>
<td>0.930777</td>
<td><a href="https://www.semanticscholar.org/paper/5e8180e2ceddaab161e9be55bd81d8f911967302">0: Model Selection for Cross-lingual Transfer</a></td>
</tr>
<tr>
<td>0</td>
<td>0.817341</td>
<td>0.978531</td>
<td><a href="https://www.semanticscholar.org/paper/19803adec3b97fb2e3c8097f17bf33fabf311795">27: Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach</a></td>
</tr>
<tr>
<td>0</td>
<td>0.814926</td>
<td>0.968398</td>
<td><a href="https://www.semanticscholar.org/paper/69c71029b898de7bc1ff7e9dab77d7fd8d3bb759">0: On the Role of Corpus Ordering in Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811506</td>
<td>0.962044</td>
<td><a href="https://www.semanticscholar.org/paper/f2f3c83db919a2429c4fcad2d0a0ed4e5294354a">45: Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809723</td>
<td>0.984946</td>
<td><a href="https://www.semanticscholar.org/paper/84059eba69c02bd57b6b227710ba62168ade827e">68: Syntactic Data Augmentation Increases Robustness to Inference Heuristics</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809415</td>
<td>0.856889</td>
<td><a href="https://www.semanticscholar.org/paper/58e13e1fed9b28e50efcaff611772801d7980c80">3: Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.805035</td>
<td>0.875141</td>
<td><a href="https://www.semanticscholar.org/paper/1bed2b8a70715e95c419a627fc244a41f5501f7a">20: Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.845852</td>
<td>0.998318</td>
<td><a href="https://www.semanticscholar.org/paper/8b9d77d5e52a70af37451d3db3d32781b83ea054">117: On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines</a></td>
</tr>
<tr>
<td>0</td>
<td>0.729144</td>
<td>0.997320</td>
<td><a href="https://www.semanticscholar.org/paper/056935031bc5cf0aeeaa0946320de26e14a1817e">148: Revisiting Few-sample BERT Fine-tuning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793049</td>
<td>0.995933</td>
<td><a href="https://www.semanticscholar.org/paper/7402b604f14b8b91c53ed6eed04af92c59636c97">236: Well-Read Students Learn Better: On the Importance of Pre-training Compact Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.790548</td>
<td>0.995661</td>
<td><a href="https://www.semanticscholar.org/paper/a54b56af24bb4873ed0163b77df63b92bd018ddc">1992: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a></td>
</tr>
<tr>
<td>0</td>
<td>0.808210</td>
<td>0.995420</td>
<td><a href="https://www.semanticscholar.org/paper/d3cacb4806886eb2fe59c90d4b6f822c24ff1822">80: Visualizing and Understanding the Effectiveness of BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800241</td>
<td>0.995095</td>
<td><a href="https://www.semanticscholar.org/paper/93b8da28d006415866bf48f9a6e06b5242129195">2544: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.726426</td>
<td>0.993885</td>
<td><a href="https://www.semanticscholar.org/paper/98ef0db84e62aef969629264c9de1f4d0013f3b9">114: AdapterFusion: Non-Destructive Task Composition for Transfer Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.777769</td>
<td>0.993641</td>
<td><a href="https://www.semanticscholar.org/paper/7dc21b6c7c02708e4de6c6c260b4439b46fbd086">38: Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.851794</td>
<td>0.993618</td>
<td><a href="https://www.semanticscholar.org/paper/e54ffc76d805c48660bb0fd20019ca82ac94ba0d">35: Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning</a></td>
</tr>
</table></html>
