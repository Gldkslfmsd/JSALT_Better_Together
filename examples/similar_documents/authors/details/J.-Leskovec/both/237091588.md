<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/4f68e07c6c3173480053fd52391851d6f80d651b">233: On the Opportunities and Risks of Foundation Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.713427</td>
<td>0.172182</td>
<td><a href="https://www.semanticscholar.org/paper/b4104140c2da65393c44b51b36ceb9f141996171">3: On Interactive Machine Learning and the Potential of Cognitive Feedback</a></td>
</tr>
<tr>
<td>0</td>
<td>0.701627</td>
<td>-0.003144</td>
<td><a href="https://www.semanticscholar.org/paper/ca86070b22fe39577d37b175da1233690d4432c3">41: Strategy selection: An introduction to the modeling challenge.</a></td>
</tr>
<tr>
<td>0</td>
<td>0.694693</td>
<td>0.104278</td>
<td><a href="https://www.semanticscholar.org/paper/562070d5ba785ade71dfbd3497cb1de0cdbe85d0">0: Detect, Understand, Act: A Neuro-symbolic Hierarchical Reinforcement Learning Framework</a></td>
</tr>
<tr>
<td>0</td>
<td>0.681511</td>
<td>0.148168</td>
<td><a href="https://www.semanticscholar.org/paper/cbbc79f797647fa2bfd6c838b0c2e51365185b9a">1: Evaluating the Safety of Deep Reinforcement Learning Models using Semi-Formal Verification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.674016</td>
<td>0.035239</td>
<td><a href="https://www.semanticscholar.org/paper/a2c145dbb30c942f136b2525a0132bf72d51b0c2">1: Model-Based Stabilisation of Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.667201</td>
<td>0.134371</td>
<td><a href="https://www.semanticscholar.org/paper/d7d6398e9c14df991f4d26a430a0f6ff53dd1e2c">2: Beyond black boxes: tackling artificial intelligence as a design material</a></td>
</tr>
<tr>
<td>0</td>
<td>0.667033</td>
<td>-0.030032</td>
<td><a href="https://www.semanticscholar.org/paper/a56aff6c56eb76395941c546825003798a21a764">19: AI and Robotics Implications for the Poor</a></td>
</tr>
<tr>
<td>0</td>
<td>0.666946</td>
<td>-0.010677</td>
<td><a href="https://www.semanticscholar.org/paper/8c130a806ea0d5717dec050688cdc764501bbbc0">2: Handling the Efficiencyâ€“Personalization Trade-Off in Service Robotics: A Machine-Learning Approach</a></td>
</tr>
<tr>
<td>0</td>
<td>0.661307</td>
<td>-0.005839</td>
<td><a href="https://www.semanticscholar.org/paper/67a8e7df499ed8e88a980653a8e8fba8153743b4">18: A Backtesting Protocol in the Era of Machine Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.572666</td>
<td>0.986425</td>
<td><a href="https://www.semanticscholar.org/paper/6b85b63579a916f705a8e10a49bd8d849d91b1fc">4191: Language Models are Few-Shot Learners</a></td>
</tr>
<tr>
<td>0</td>
<td>0.530347</td>
<td>0.980950</td>
<td><a href="https://www.semanticscholar.org/paper/daa2ddc13a6c4d06f4933b259d346187eab2b24e">0: CILDA: Contrastive Data Augmentation using Intermediate Layer Knowledge Distillation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.572594</td>
<td>0.980549</td>
<td><a href="https://www.semanticscholar.org/paper/4383a975c09b72ba2f1a77cd779bb6965dbfb2fb">45: Scaling Laws for Transfer</a></td>
</tr>
<tr>
<td>0</td>
<td>0.555582</td>
<td>0.980272</td>
<td><a href="https://www.semanticscholar.org/paper/6bd91a3183ddb844641acb9f3fe9faec6a9ff617">8: Meta-learning via Language Model In-context Tuning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.530860</td>
<td>0.980263</td>
<td><a href="https://www.semanticscholar.org/paper/0abb08c4ec5feab4cdd82c471866dd4395c573ce">26: Contrastive Distillation on Intermediate Representations for Language Model Compression</a></td>
</tr>
<tr>
<td>0</td>
<td>0.501535</td>
<td>0.979467</td>
<td><a href="https://www.semanticscholar.org/paper/17d5884215b5afa53545cd7cb6135de5478da4ec">112: CERT: Contrastive Self-supervised Learning for Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.520205</td>
<td>0.979399</td>
<td><a href="https://www.semanticscholar.org/paper/943559a635a4ee8d8b4d65ed2776d4a9cb6fc967">0: Continual Prompt Tuning for Dialog State Tracking</a></td>
</tr>
<tr>
<td>0</td>
<td>0.514892</td>
<td>0.979045</td>
<td><a href="https://www.semanticscholar.org/paper/1c49e8e25f52449ae6a986cad9d6a1f8fc7d2e71">0: Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene</a></td>
</tr>
<tr>
<td>0</td>
<td>0.521464</td>
<td>0.978670</td>
<td><a href="https://www.semanticscholar.org/paper/2558c42f6d24d3a85fe44ed4faef8a482785c174">8: ZeroPrompt: Scaling Prompt-Based Pretraining to 1, 000 Tasks Improves Zero-Shot Generalization</a></td>
</tr>
</table></html>
