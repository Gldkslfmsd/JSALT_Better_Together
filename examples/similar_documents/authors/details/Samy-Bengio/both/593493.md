<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/25fb5a6abcd88ee52bdb3165b844c941e90eb9bf">592: Revisiting Distributed Synchronous SGD</a></td>
</tr>
<tr>
<td>0</td>
<td>0.779135</td>
<td>0.017613</td>
<td>NA:237302908</td>
</tr>
<tr>
<td>0</td>
<td>0.777147</td>
<td>0.935698</td>
<td><a href="https://www.semanticscholar.org/paper/b61c0c3a6102bc06b982c5eade65cb3829415e18">1: ALADDIN: Asymmetric Centralized Training for Distributed Deep Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.761859</td>
<td>0.855868</td>
<td><a href="https://www.semanticscholar.org/paper/2229ac756f89c3db017293918548555734d2f891">87: TicTac: Accelerating Distributed Deep Learning with Communication Scheduling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.746712</td>
<td>0.902663</td>
<td><a href="https://www.semanticscholar.org/paper/46fa969cf8a7b4fc91538372848cd5ad23a67ffe">11: Adaptive Quantization of Model Updates for Communication-Efficient Federated Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.744280</td>
<td>0.017613</td>
<td><a href="https://www.semanticscholar.org/paper/7f569ba6c261c5c1f8b9ab9db3c2b95c864e2446">0: Scalable Deep Learning Inference: Algorithmic Approach</a></td>
</tr>
<tr>
<td>0</td>
<td>0.743895</td>
<td>0.759961</td>
<td><a href="https://www.semanticscholar.org/paper/51c91c03b97014c2e0bff2a1ba2f3b29f2bda0d8">0: Versatile Communication Optimization for Deep Learning by Modularized Parameter Server</a></td>
</tr>
<tr>
<td>0</td>
<td>0.742173</td>
<td>0.847138</td>
<td><a href="https://www.semanticscholar.org/paper/cbeed2879ee453919460438f19c1588b80057391">0: ALLEVIATED FORGETTING IN LOCAL TRAINING</a></td>
</tr>
<tr>
<td>0</td>
<td>0.742060</td>
<td>0.870058</td>
<td><a href="https://www.semanticscholar.org/paper/72055aff17a462bcccb3250becaf62c9911cdd8b">66: MATCHA: Speeding Up Decentralized SGD via Matching Decomposition Sampling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.738896</td>
<td>0.846639</td>
<td><a href="https://www.semanticscholar.org/paper/902cf9be7a4cba3dc53f38518eb8f6997a9ef102">0: An Efficient Method for Training Deep Learning Networks Distributed</a></td>
</tr>
<tr>
<td>0</td>
<td>0.750892</td>
<td>0.992950</td>
<td><a href="https://www.semanticscholar.org/paper/dc588d4c89026edf2beebf3937a940ea9ff34414">147: Asynchronous Stochastic Gradient Descent with Delay Compensation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.723303</td>
<td>0.990038</td>
<td><a href="https://www.semanticscholar.org/paper/d1e4365de165463e51134f10bf3939f2b00a6667">481: Deep learning with Elastic Averaging SGD</a></td>
</tr>
<tr>
<td>0</td>
<td>0.657898</td>
<td>0.987147</td>
<td><a href="https://www.semanticscholar.org/paper/3439a127e45fb763881f03ef3ec735a1db0e0ccc">668: 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs</a></td>
</tr>
<tr>
<td>0</td>
<td>0.736363</td>
<td>0.986221</td>
<td><a href="https://www.semanticscholar.org/paper/9ee76c41dd161df75cb50ac06d2868afec63b0db">363: Scalable distributed DNN training using commodity GPU cloud computing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.743906</td>
<td>0.981524</td>
<td><a href="https://www.semanticscholar.org/paper/ffa641a39315830fcb73ca78b09df69fbc180ce9">118: Asynchrony begets momentum, with an application to deep learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.770888</td>
<td>0.981379</td>
<td><a href="https://www.semanticscholar.org/paper/4dc2ab5d60dcc2e5a2e0655e5ddcc6b124f03f11">197: Staleness-Aware Async-SGD for Distributed Deep Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.705632</td>
<td>0.980854</td>
<td><a href="https://www.semanticscholar.org/paper/c169b81aea9391582df66731e42a62057a66aa75">0: LOSP: Overlap Synchronization Parallel With Local Compensation for Fast Distributed Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.758554</td>
<td>0.979248</td>
<td><a href="https://www.semanticscholar.org/paper/667f953d8b35b8a9ea5edae36eda17e93f4065e3">104: How to scale distributed deep learning?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.641627</td>
<td>0.977007</td>
<td><a href="https://www.semanticscholar.org/paper/a47bd5fdb0abbe5fc2fe093f2cab0d0fa3a0aef3">6: Intermittent Pulling with Local Compensation for Communication-Efficient Federated Learning</a></td>
</tr>
</table></html>
