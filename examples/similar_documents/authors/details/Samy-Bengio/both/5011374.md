<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/3fee7b836b71125a5f6a3696b9c383dae18c21e8">232: A Study on Overfitting in Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.839276</td>
<td>0.981849</td>
<td><a href="https://www.semanticscholar.org/paper/05c82617cdaa16c9dc17c32f3cb5ed4a7182b13e">52: Automatic Data Augmentation for Generalization in Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.838644</td>
<td>0.993539</td>
<td><a href="https://www.semanticscholar.org/paper/caea502325b6a82b1b437c62585992609b5aa542">125: Assessing Generalization in Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.837059</td>
<td>0.979459</td>
<td><a href="https://www.semanticscholar.org/paper/cbb1991b08a223f714543d19478eb3f1f26754fb">5: How to Make Deep RL Work in Practice</a></td>
</tr>
<tr>
<td>0</td>
<td>0.820483</td>
<td>0.908662</td>
<td><a href="https://www.semanticscholar.org/paper/b4c9a4aaa456e079e9427e254f05e441adfc3757">0: New algorithmic approaches for reinforcement learning , with application to integer programming</a></td>
</tr>
<tr>
<td>0</td>
<td>0.815335</td>
<td>0.980821</td>
<td><a href="https://www.semanticscholar.org/paper/325dcf5f434ced6a716133262bad919b434f847b">5: Improving Deep Reinforcement Learning with Knowledge Transfer</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811114</td>
<td>0.957976</td>
<td><a href="https://www.semanticscholar.org/paper/1fe4d11d8ce3d033dbead6f4a567f86fe21bf8ee">22: Neural Logic Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.810473</td>
<td>0.485843</td>
<td><a href="https://www.semanticscholar.org/paper/1ddaf6b338bb088eb2ea6529ce3ca95a5e6b14f7">6: Mathematical Models of Overparameterized Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809481</td>
<td>0.976048</td>
<td><a href="https://www.semanticscholar.org/paper/2405c131d0807c7190b253b8e0524df20b8c633d">0: Shared Learning : Enhancing Reinforcement in $Q$-Ensembles</a></td>
</tr>
<tr>
<td>1</td>
<td>0.806676</td>
<td>0.993299</td>
<td><a href="https://www.semanticscholar.org/paper/33690ff21ef1efb576410e656f2e60c89d0307d6">1102: Deep Reinforcement Learning that Matters</a></td>
</tr>
<tr>
<td>0</td>
<td>0.706836</td>
<td>0.995658</td>
<td>NA:248940896</td>
</tr>
<tr>
<td>0</td>
<td>0.814307</td>
<td>0.995216</td>
<td><a href="https://www.semanticscholar.org/paper/3caf26a81f8b8be27b8ebc09bdb93416b4c40fe6">23: Improving Generalization in Reinforcement Learning with Mixture Regularization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.846745</td>
<td>0.994743</td>
<td><a href="https://www.semanticscholar.org/paper/567eb0ab96fb7f2869e9c4c204da9f2cf422d946">105: Generalization and Regularization in DQN</a></td>
</tr>
<tr>
<td>0</td>
<td>0.810909</td>
<td>0.994564</td>
<td><a href="https://www.semanticscholar.org/paper/ef2bc452812d6005ab0a66af6c3f97b6b0ba837e">346: Quantifying Generalization in Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.802177</td>
<td>0.994353</td>
<td><a href="https://www.semanticscholar.org/paper/a622be547caf0b1223626de5e69377c20ae11265">109: A Dissection of Overfitting and Generalization in Continuous Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.642467</td>
<td>0.993599</td>
<td><a href="https://www.semanticscholar.org/paper/4b63e34276aa98d5345efa7fe09bb06d8a9d8f52">818: Deep Exploration via Bootstrapped DQN</a></td>
</tr>
<tr>
<td>0</td>
<td>0.639231</td>
<td>0.992132</td>
<td><a href="https://www.semanticscholar.org/paper/8d814620a1ca77e745bc8a33b96b86148f2804fe">195: Leveraging Procedural Generation to Benchmark Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.718365</td>
<td>0.991990</td>
<td><a href="https://www.semanticscholar.org/paper/ff84c46d4653782218549bd99631130df2d2859e">57: Distilling Policy Distillation</a></td>
</tr>
</table></html>
