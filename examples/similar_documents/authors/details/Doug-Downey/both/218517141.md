<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/17293893e0ef799fc038b5da01fc6baf3b08abdd">10: Stolen Probability: A Structural Weakness of Neural Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.753857</td>
<td>0.926416</td>
<td><a href="https://www.semanticscholar.org/paper/0ca2a7465fe88f1f4912b8dd7b4b0db69a268b0b">21: Neural Lattice Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.747082</td>
<td>0.947403</td>
<td><a href="https://www.semanticscholar.org/paper/8a5eaf21751c6062b31cafb859ab1af26fbc8ba6">3: All Word Embeddings from One Embedding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.731927</td>
<td>0.518370</td>
<td><a href="https://www.semanticscholar.org/paper/18f467dc9aceab49f8e198876ee1e83abff8cbd4">14: Interpretable probabilistic embeddings: bridging the gap between topic models and neural networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.725534</td>
<td>0.830242</td>
<td><a href="https://www.semanticscholar.org/paper/bf386f23f2c826461e61c583d6cb63326e81c17b">18: Quantum-Inspired Complex Word Embedding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.724072</td>
<td>0.939453</td>
<td><a href="https://www.semanticscholar.org/paper/970f474fc5408f079f627adbc9a300dfca7e317e">65: Learning to Compute Word Embeddings On the Fly</a></td>
</tr>
<tr>
<td>0</td>
<td>0.723679</td>
<td>0.912812</td>
<td><a href="https://www.semanticscholar.org/paper/3d5a009d57bdb7dee5a92ca09e2ee49664ca0878">0: The Impact of Word Embeddings on Neural Dependency Parsing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.717677</td>
<td>0.181248</td>
<td><a href="https://www.semanticscholar.org/paper/299909c7b607a87cdcae4b7d81deb38e4bb6711f">0: A tensor-competition based architecture: to capture the influence of word sense</a></td>
</tr>
<tr>
<td>0</td>
<td>0.716672</td>
<td>0.528067</td>
<td><a href="https://www.semanticscholar.org/paper/842ac0768f387260161a3d6d0b6220eb82117396">1: Ensemble Methods for Word Embedding Model Based on Judicial Text</a></td>
</tr>
<tr>
<td>0</td>
<td>0.716613</td>
<td>0.805831</td>
<td><a href="https://www.semanticscholar.org/paper/181fc1a4a5db14412bbfdb906a027e7106e911db">4: Parsing with Context Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.593684</td>
<td>0.988273</td>
<td><a href="https://www.semanticscholar.org/paper/635741220ce35c211f6df19280abb247a98fbb86">9: Cross-lingual Transfer Learning for Spoken Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.615725</td>
<td>0.988178</td>
<td><a href="https://www.semanticscholar.org/paper/9604a6202cc3bd0bb38177a25fc8acfadc404d46">7: Mimic and Conquer: Heterogeneous Tree Structure Distillation for Syntactic NLP</a></td>
</tr>
<tr>
<td>0</td>
<td>0.637051</td>
<td>0.986490</td>
<td><a href="https://www.semanticscholar.org/paper/1405f8dd4aff025badb205532b59916412065e82">0: Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via Adaptive Gradient Gating for Rare Token Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>-1.000000</td>
<td>0.986441</td>
<td><a href="https://www.semanticscholar.org/paper/3a29aa4eff48624752c07059a44d3288a678c8ab">400: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></td>
</tr>
<tr>
<td>0</td>
<td>0.652063</td>
<td>0.985943</td>
<td><a href="https://www.semanticscholar.org/paper/9d902474f238e7a0b8c6009c5eb2172fae40dd7d">6: Universal Language Model Fine-Tuning with Subword Tokenization for Polish</a></td>
</tr>
<tr>
<td>0</td>
<td>0.622351</td>
<td>0.985352</td>
<td><a href="https://www.semanticscholar.org/paper/dec69765fc6c188897b09c8282d32db788e2c261">14: Improving Pre-Trained Multilingual Model with Vocabulary Expansion</a></td>
</tr>
<tr>
<td>0</td>
<td>0.649869</td>
<td>0.984567</td>
<td><a href="https://www.semanticscholar.org/paper/61f46eca3a385d1bd9d94b559e855171762b8a41">1: Subword ELMo</a></td>
</tr>
<tr>
<td>0</td>
<td>0.607371</td>
<td>0.984043</td>
<td><a href="https://www.semanticscholar.org/paper/061c712d9bb32439c70d9d3c01882f4097fc3df3">1: Recursive Tree-Structured Self-Attention for Answer Sentence Selection</a></td>
</tr>
<tr>
<td>0</td>
<td>0.646439</td>
<td>0.983527</td>
<td><a href="https://www.semanticscholar.org/paper/dce5ca746224a97e532a034c371305f8bddcb5fc">151: Learning Joint Multilingual Sentence Representations with Neural Machine Translation</a></td>
</tr>
</table></html>
