<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/66661a68dbf1d98d794fd025113b103683510303">693: Audio augmentation for speech recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.835840</td>
<td>0.901109</td>
<td><a href="https://www.semanticscholar.org/paper/1d51d0e9252df3217c05df2accfa40e02acdfabe">5: A Survey of the Effects of Data Augmentation for Automatic Speech Recognition Systems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.828102</td>
<td>0.581473</td>
<td><a href="https://www.semanticscholar.org/paper/15c0eb93f8aed97dab2a997963ec2dbe9408670b">1: Towards Speech Robustness for Acoustic Scene Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.825064</td>
<td>0.813631</td>
<td><a href="https://www.semanticscholar.org/paper/a91820efd1d73f642214bdde8e3920aa62973635">5: On Practical Aspects of Multi-condition Training Based on Augmentation for Reverberation-/Noise-Robust Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.815928</td>
<td>0.740678</td>
<td><a href="https://www.semanticscholar.org/paper/1b97c38d0156dc8bf300b41c2ba5c0463c3a2c00">4: Training Data Augmentation and Data Selection</a></td>
</tr>
<tr>
<td>0</td>
<td>0.799494</td>
<td>0.409195</td>
<td><a href="https://www.semanticscholar.org/paper/3b48ee3e939c75de073ea20c6b4bfe19c8e65532">72: Audio Super Resolution using Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.797316</td>
<td>0.736820</td>
<td><a href="https://www.semanticscholar.org/paper/917b1b8470687cbe02224dfc8b87b9ad999afaa8">0: Robust Recognition of Speech with Background Music in Acoustically Under-Resourced Scenarios</a></td>
</tr>
<tr>
<td>0</td>
<td>0.794139</td>
<td>0.736106</td>
<td><a href="https://www.semanticscholar.org/paper/a0f7e1f456f7195c9ad5b180246e663ae16facd4">1: Bandwidth Embeddings for Mixed-bandwidth Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792049</td>
<td>0.777512</td>
<td><a href="https://www.semanticscholar.org/paper/54b1994f130cd89a9305e51f037a523535cabb29">0: Bandwidth Extension for Deep Speaker Embedding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.785603</td>
<td>0.832857</td>
<td><a href="https://www.semanticscholar.org/paper/a84b5f22a15b795aa150438365f5b252fc3ea1a8">0: Speech Synthesis as Augmentation for Low-Resource ASR</a></td>
</tr>
<tr>
<td>0</td>
<td>0.699075</td>
<td>0.986623</td>
<td><a href="https://www.semanticscholar.org/paper/81fdda553e8d06e94f98dbb795bd8ff7d2dfc4ad">213: Wav2Letter: an End-to-End ConvNet-based Speech Recognition System</a></td>
</tr>
<tr>
<td>0</td>
<td>0.733068</td>
<td>0.985275</td>
<td><a href="https://www.semanticscholar.org/paper/da8c898dfe4804ed60833ee0d019969a8c588a46">66: Letter-Based Speech Recognition with Gated ConvNets</a></td>
</tr>
<tr>
<td>0</td>
<td>0.651238</td>
<td>0.984955</td>
<td><a href="https://www.semanticscholar.org/paper/6ce6a9a30cd69bd2842a4b581cf48c6815bdfdd8">702: Purely Sequence-Trained Neural Networks for ASR Based on Lattice-Free MMI</a></td>
</tr>
<tr>
<td>0</td>
<td>0.686668</td>
<td>0.982722</td>
<td><a href="https://www.semanticscholar.org/paper/44765630c55bce4a438c65ed2b739011ca4c1f23">6: Speech Transformer with Speaker Aware Persistent Memory</a></td>
</tr>
<tr>
<td>0</td>
<td>0.678709</td>
<td>0.982582</td>
<td><a href="https://www.semanticscholar.org/paper/c9bd15c7838c1d3cdd5f5113a2efd9440f86b3da">317: English Conversational Telephone Speech Recognition by Humans and Machines</a></td>
</tr>
<tr>
<td>0</td>
<td>0.669461</td>
<td>0.980882</td>
<td><a href="https://www.semanticscholar.org/paper/e6504f8e3b4b3cf429b8d3ce4091ae9d1afa5df3">78: Multi-Dialect Speech Recognition with a Single Sequence-to-Sequence Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.737036</td>
<td>0.980379</td>
<td><a href="https://www.semanticscholar.org/paper/eedef63cc160fe9138b5be900243d5907537d55c">57: Toward Domain-Invariant Speech Recognition via Large Scale Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.780248</td>
<td>0.980025</td>
<td><a href="https://www.semanticscholar.org/paper/187be9194de4974e428409a8693d1bdba7dd6e7d">7: MixSpeech: Data Augmentation for Low-Resource Automatic Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.677776</td>
<td>0.979520</td>
<td><a href="https://www.semanticscholar.org/paper/2683061823326301221f3604b5b071957b54be2c">14: Unsupervised Speaker Adaptation Using Attention-Based Speaker Memory for End-to-End ASR</a></td>
</tr>
</table></html>
