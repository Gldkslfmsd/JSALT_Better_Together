<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/d58244ed9b86e9ad7f90cb302d32e5f96a72d040">83: Structure and performance of a dependency language model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.824622</td>
<td>0.945804</td>
<td><a href="https://www.semanticscholar.org/paper/9d3a863b71f093e1cbc9304a3287c1ddc48c6f31">184: Grammatical Trigrams: A Probabilistic Model of Link Grammar</a></td>
</tr>
<tr>
<td>0</td>
<td>0.814823</td>
<td>0.929827</td>
<td><a href="https://www.semanticscholar.org/paper/05839b28361e350fbc2fa80ba1f5c012c0c9aee9">37: Combining semantic and syntactic structure for language modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.814246</td>
<td>0.861891</td>
<td><a href="https://www.semanticscholar.org/paper/d39d39a6246c13928bbd66d122f3e61e67b584ef">153: A Generative Model for Parsing Natural Language to Meaning Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.807253</td>
<td>0.043761</td>
<td><a href="https://www.semanticscholar.org/paper/6ac97c3115807165b71a6e0df5319ed9e011c157">0: Statistical language paring model based on dependency</a></td>
</tr>
<tr>
<td>0</td>
<td>0.804207</td>
<td>0.843761</td>
<td><a href="https://www.semanticscholar.org/paper/3278401ff7e1ae095cab87e94507caf141ec5d80">7: Dependency Structure Language Model for Information Retrieval</a></td>
</tr>
<tr>
<td>0</td>
<td>0.802712</td>
<td>0.551738</td>
<td><a href="https://www.semanticscholar.org/paper/0ca2a7465fe88f1f4912b8dd7b4b0db69a268b0b">21: Neural Lattice Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.787836</td>
<td>0.921164</td>
<td><a href="https://www.semanticscholar.org/paper/fe39324a4ca8c3a9001086e1b0114efd6ab653f3">35: Statistical language modeling combining N-gram and context-free grammars</a></td>
</tr>
<tr>
<td>0</td>
<td>0.785161</td>
<td>0.963065</td>
<td><a href="https://www.semanticscholar.org/paper/4d5efad58d63c03a3a8497bc2ac18ac4c198a720">14: Introducing linguistic constraints into statistical language modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.784549</td>
<td>0.556752</td>
<td><a href="https://www.semanticscholar.org/paper/47570e7f63e296f224a0e7f9a0d08b0de3cbaf40">842: Grammar as a Foreign Language</a></td>
</tr>
<tr>
<td>0</td>
<td>0.796850</td>
<td>0.994052</td>
<td><a href="https://www.semanticscholar.org/paper/89c5fc5fefc9906df89a2042d7bb255daabb3fb4">9: Long Distance Dependency in Language Modeling: An Empirical Study</a></td>
</tr>
<tr>
<td>0</td>
<td>0.614265</td>
<td>0.993945</td>
<td><a href="https://www.semanticscholar.org/paper/651963e00201b9a489fcf8188d1903c8e975f09b">2: Research of Pinyin-To-Character conversion based on Maximum Entropy model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.790344</td>
<td>0.992216</td>
<td><a href="https://www.semanticscholar.org/paper/bb8e5322dca1657e0cd2925fe1209a16a0c3aefb">74: The SuperARV Language Model: Investigating the Effectiveness of Tightly Integrating Multiple Knowledge Sources</a></td>
</tr>
<tr>
<td>0</td>
<td>0.760119</td>
<td>0.992012</td>
<td><a href="https://www.semanticscholar.org/paper/6541812a987881d1c960cd3ac8f18064767e4eef">5: Semantic tokenization of verbalized numbers in language modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.782555</td>
<td>0.991409</td>
<td><a href="https://www.semanticscholar.org/paper/a1c3748820d6b5ab4e7334524815df9bb6d20aed">318: Structured language modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.711665</td>
<td>0.990712</td>
<td><a href="https://www.semanticscholar.org/paper/bc5a3d71a8c2e80299f6b4b0bff80e44c50a76de">111: Variable n-grams and extensions for conversational speech language modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.730676</td>
<td>0.989565</td>
<td><a href="https://www.semanticscholar.org/paper/a434c9cb11768b61e9ac608d31a81b62c1ad474e">26: Putting it all together: language model combination</a></td>
</tr>
<tr>
<td>0</td>
<td>0.691541</td>
<td>0.989443</td>
<td><a href="https://www.semanticscholar.org/paper/7c895368ecc26399f2e234462ea0fbb2754aee94">55: A class-based language model for large-vocabulary speech recognition extracted from part-of-speech statistics</a></td>
</tr>
<tr>
<td>0</td>
<td>0.683576</td>
<td>0.989357</td>
<td><a href="https://www.semanticscholar.org/paper/3245eda84cb756678fd6dc7c474d54a75501497f">45: The Use of Clustering Techniques for Language Modeling-Application to Asian Language</a></td>
</tr>
</table></html>
