<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/e92eaea488d016b773e3d58f8b0e795dac808365">4: OPT: Open Pre-trained Transformer Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.823273</td>
<td>0.920895</td>
<td><a href="https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992">32318: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.816630</td>
<td>0.957286</td>
<td><a href="https://www.semanticscholar.org/paper/5b4be79081e1d6eac83e8a9e7a38bd7c338a4c78">23: How fine can fine-tuning be? Learning efficient language models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.813595</td>
<td>0.909545</td>
<td><a href="https://www.semanticscholar.org/paper/ae8eff35dcdd47e80416fa5a4d8f51f8809cdc7b">14: Deep Transformers with Latent Depth</a></td>
</tr>
<tr>
<td>0</td>
<td>0.804541</td>
<td>-0.012654</td>
<td>NA:236528222</td>
</tr>
<tr>
<td>0</td>
<td>0.794066</td>
<td>0.659293</td>
<td><a href="https://www.semanticscholar.org/paper/fe82735fe8ae2163a37aa2787eee0db8efc745b6">22: transformers . zip : Compressing Transformers with Pruning and Quantization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793462</td>
<td>0.979142</td>
<td><a href="https://www.semanticscholar.org/paper/02806b916d2c341d5a5ac7dc3c19e3f2363d402f">10: Know What You Don't Need: Single-Shot Meta-Pruning for Attention Heads</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792436</td>
<td>0.966240</td>
<td><a href="https://www.semanticscholar.org/paper/1187c70c4011f935642084e84186284ac0add3d0">23: Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.791315</td>
<td>0.963630</td>
<td><a href="https://www.semanticscholar.org/paper/0d5a3fd61911590e887927c39e3cedd36c9c3c8c">14: Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.790220</td>
<td>0.898048</td>
<td><a href="https://www.semanticscholar.org/paper/071b1c7bb2f45011967bc08de32ee21d409e02de">3: Sentence Bottleneck Autoencoders from Transformer Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.820080</td>
<td>0.997769</td>
<td><a href="https://www.semanticscholar.org/paper/15190e8b459bd85d546286f7d7da61b4f4f3f58a">5: What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.762039</td>
<td>0.997692</td>
<td><a href="https://www.semanticscholar.org/paper/ff0b2681d7b05e16c46dfb71d980cc2f605907cd">119: Finetuned Language Models Are Zero-Shot Learners</a></td>
</tr>
<tr>
<td>0</td>
<td>0.586618</td>
<td>0.997082</td>
<td><a href="https://www.semanticscholar.org/paper/77d956cdab4508d569ae5741549b78e715fd0749">24: TruthfulQA: Measuring How Models Mimic Human Falsehoods</a></td>
</tr>
<tr>
<td>0</td>
<td>0.711228</td>
<td>0.997055</td>
<td><a href="https://www.semanticscholar.org/paper/31e396eab8edb44f79e3158eeefc3280afb404f4">1: How Many Data Samples is an Additional Instruction Worth?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.636978</td>
<td>0.997051</td>
<td><a href="https://www.semanticscholar.org/paper/4bbb874ac94f789ffaf9b38058571f785828031a">0: Entity-Conditioned Question Generation for Robust Attention Distribution in Neural Information Retrieval</a></td>
</tr>
<tr>
<td>0</td>
<td>0.767750</td>
<td>0.996909</td>
<td><a href="https://www.semanticscholar.org/paper/5f05f55634c05fe13436957748dd63c3381978e0">0: FPM: A Collection of Large-scale Foundation Pre-trained Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.765133</td>
<td>0.996872</td>
<td><a href="https://www.semanticscholar.org/paper/1403e6b9adf7712c35ae56327d52fe54603b87e1">12: Few-shot Learning with Multilingual Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.756888</td>
<td>0.996861</td>
<td><a href="https://www.semanticscholar.org/paper/c28b7dfe341f1e13a5a98efbce7946ef795cf9b8">21: SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer</a></td>
</tr>
<tr>
<td>0</td>
<td>0.707316</td>
<td>0.996627</td>
<td><a href="https://www.semanticscholar.org/paper/7f631586a368f1762866b01ff9f43c265851d52e">16: DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing</a></td>
</tr>
</table></html>
