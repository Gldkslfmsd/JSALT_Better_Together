<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/bb6317bbd2c4a81e94cf3d7eb1b73da246a022db">169: Generalization through Memorization: Nearest Neighbor Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.789009</td>
<td>0.949220</td>
<td><a href="https://www.semanticscholar.org/paper/f21be3f230cb2721904671c7747165edad8bd033">0: Language Modelling via Learning to Rank</a></td>
</tr>
<tr>
<td>0</td>
<td>0.772609</td>
<td>0.862240</td>
<td><a href="https://www.semanticscholar.org/paper/ccab10f5c3c33a76a9931aecb43a25cdc8b67451">2: Being Generous with Sub-Words towards Small NMT Children</a></td>
</tr>
<tr>
<td>0</td>
<td>0.769462</td>
<td>-0.017669</td>
<td><a href="https://www.semanticscholar.org/paper/818cc598688ca2d2a49514017a87bc89fd59ef1f">0: Improving Entity Recall in Automatic Speech Recognition with Neural Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.767624</td>
<td>-0.017669</td>
<td>NA:233406210</td>
</tr>
<tr>
<td>0</td>
<td>0.763473</td>
<td>0.962047</td>
<td><a href="https://www.semanticscholar.org/paper/e54ffc76d805c48660bb0fd20019ca82ac94ba0d">35: Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.759774</td>
<td>0.978652</td>
<td><a href="https://www.semanticscholar.org/paper/75a35576efee34622254f265e4cbeb5e01eea7a1">31: On the Language Neutrality of Pre-trained Multilingual Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.759734</td>
<td>0.979063</td>
<td><a href="https://www.semanticscholar.org/paper/6afe0fb12ceacadbbfed7202d430770a3f344731">138: Revisiting Pre-Trained Models for Chinese Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.758247</td>
<td>0.966642</td>
<td><a href="https://www.semanticscholar.org/paper/030fdb6a604df56d4e3ba7b35689d6e251ac78b0">4: Is Your Language Model Ready for Dense Representation Fine-tuning?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.757082</td>
<td>0.990125</td>
<td><a href="https://www.semanticscholar.org/paper/002c256d30d6be4b23d365a8de8ae0e67e4c9641">37: Improving language models by retrieving from trillions of tokens</a></td>
</tr>
<tr>
<td>0</td>
<td>0.678479</td>
<td>0.995119</td>
<td><a href="https://www.semanticscholar.org/paper/112fd54ee193237b24f2ce7fce79e399609a29c5">89: The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives</a></td>
</tr>
<tr>
<td>0</td>
<td>0.638912</td>
<td>0.993181</td>
<td><a href="https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8">467: CTRL: A Conditional Transformer Language Model for Controllable Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.666966</td>
<td>0.992866</td>
<td><a href="https://www.semanticscholar.org/paper/5b015296730273921889e54a0a31e3b173017026">120: TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue</a></td>
</tr>
<tr>
<td>0</td>
<td>0.730885</td>
<td>0.992587</td>
<td><a href="https://www.semanticscholar.org/paper/311909621177c397c6b7099beff32332124f7d46">32: On Learning Universal Representations Across Languages</a></td>
</tr>
<tr>
<td>0</td>
<td>0.709928</td>
<td>0.992275</td>
<td><a href="https://www.semanticscholar.org/paper/84476fdf6ead3553f4493dff8e02308439d6222b">24: Improve Transformer Models with Better Relative Position Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.573536</td>
<td>0.991207</td>
<td><a href="https://www.semanticscholar.org/paper/c9982f631ffba3c5201dd920bb523b47f2565351">3: HisBERT for Conversational Reading Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.637650</td>
<td>0.990709</td>
<td><a href="https://www.semanticscholar.org/paper/9d7fbdb2e9817a6396992a1c92f75206689852d9">109: On Identifiability in Transformers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.703719</td>
<td>0.990652</td>
<td><a href="https://www.semanticscholar.org/paper/395de0bd3837fdf4b4b5e5f04835bcc69c279481">2258: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.149461</td>
<td>0.990545</td>
<td><a href="https://www.semanticscholar.org/paper/dc35daba3fb34b2e6a5b12530badb7b799262bbf">33: On Position Embeddings in BERT</a></td>
</tr>
</table></html>
