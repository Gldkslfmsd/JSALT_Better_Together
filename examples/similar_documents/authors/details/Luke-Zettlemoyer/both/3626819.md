<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/3febb2bed8865945e7fddc99efd791887bb7e14f">7841: Deep Contextualized Word Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.877169</td>
<td>0.982173</td>
<td><a href="https://www.semanticscholar.org/paper/bc8fa64625d9189f5801837e7b133e7fe3c581f7">705: Learned in Translation: Contextualized Word Vectors</a></td>
</tr>
<tr>
<td>0</td>
<td>0.863580</td>
<td>0.986819</td>
<td><a href="https://www.semanticscholar.org/paper/ac11062f1f368d97f4c826c317bf50dcc13fdb59">257: Dissecting Contextual Word Embeddings: Architecture and Representation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.856987</td>
<td>0.980448</td>
<td><a href="https://www.semanticscholar.org/paper/460ab2a10990e67b38b6b37b4208d6c552864419">51: Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.850019</td>
<td>0.730145</td>
<td><a href="https://www.semanticscholar.org/paper/1ba89d99a0e8075a4a69ae03e05835fb5a9ca925">12: Different Contexts Lead to Different Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.847795</td>
<td>0.952326</td>
<td><a href="https://www.semanticscholar.org/paper/42b24ccc6f529ef9244f551a124943a900ed7471">11: Quantifying the Contextualization of Word Representations with Semantic Class Probing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.840100</td>
<td>0.691921</td>
<td><a href="https://www.semanticscholar.org/paper/f9f91e7bac46b13444eddeb2438b01089e73b786">295: Tailoring Continuous Word Representations for Dependency Parsing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.839063</td>
<td>0.972956</td>
<td><a href="https://www.semanticscholar.org/paper/9b1933038680b13c06b60dfe810e96a3a0ef9d37">17: Contextual and Non-Contextual Word Embeddings: an in-depth Linguistic Investigation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.833614</td>
<td>0.837298</td>
<td><a href="https://www.semanticscholar.org/paper/3bedaea865f4fb922f6f7ee257ab73e2399de7b5">3: Multiple Word Embeddings for Increased Diversity of Representation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.828305</td>
<td>0.871790</td>
<td><a href="https://www.semanticscholar.org/paper/4823b8705db9f6808019169f05f97f0962dd9cc7">4: Attention Word Embedding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.676596</td>
<td>0.992924</td>
<td><a href="https://www.semanticscholar.org/paper/1a9954d86466a7e4de6f98ddee452ceb50e15d86">160: DocBERT: BERT for Document Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.748773</td>
<td>0.992903</td>
<td><a href="https://www.semanticscholar.org/paper/b82d38a1d7970ca68e0a88a881d260f663c76dbd">90: Portuguese Named Entity Recognition using BERT-CRF</a></td>
</tr>
<tr>
<td>0</td>
<td>0.638292</td>
<td>0.992723</td>
<td><a href="https://www.semanticscholar.org/paper/a4bc4b98a917174ac2ab14bd5e66d64306079ab5">286: BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis</a></td>
</tr>
<tr>
<td>0</td>
<td>0.735173</td>
<td>0.992317</td>
<td><a href="https://www.semanticscholar.org/paper/1e077413b25c4d34945cc2707e17e46ed4fe784a">2192: Universal Language Model Fine-tuning for Text Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.786616</td>
<td>0.991940</td>
<td><a href="https://www.semanticscholar.org/paper/5df0b8b80aecda1efdebac5d1ab7bcf94a88c68f">71: Probing Biomedical Embeddings from Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.644148</td>
<td>0.991765</td>
<td><a href="https://www.semanticscholar.org/paper/01edf4b0884903b310c1b0a1cb9cc3cb4bf2d8b2">14: NCUEE at MEDIQA 2019: Medical Text Inference Using Ensemble BERT-BiLSTM-Attention Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.745427</td>
<td>0.991454</td>
<td><a href="https://www.semanticscholar.org/paper/3a7bbc46795929f0eace82b64c44c92a48682fb5">336: FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP</a></td>
</tr>
<tr>
<td>0</td>
<td>0.825216</td>
<td>0.991293</td>
<td><a href="https://www.semanticscholar.org/paper/edfe9dd16316618e694cd087d0d418dac91eb48c">198: Pooled Contextualized Embeddings for Named Entity Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.706557</td>
<td>0.990749</td>
<td><a href="https://www.semanticscholar.org/paper/a022bda79947d1f656a1164003c1b3ae9a843df9">601: How to Fine-Tune BERT for Text Classification?</a></td>
</tr>
</table></html>
