<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/34733eaf66007516347a40ad5d9bbe1cc9dacb6b">4202: A Simple Framework for Contrastive Learning of Visual Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.840260</td>
<td>0.847541</td>
<td><a href="https://www.semanticscholar.org/paper/79ef0efa7217bf85712c652b3b3640176b9e2feb">61: Unsupervised Learning of Dense Visual Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.838462</td>
<td>0.953041</td>
<td><a href="https://www.semanticscholar.org/paper/175b27969cf0c24595151fb07c84437fd0344a5a">13: i-Mix: A Strategy for Regularizing Contrastive Representation Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.815107</td>
<td>0.742287</td>
<td><a href="https://www.semanticscholar.org/paper/2a9934c1279d7abb303721681360949b96e6f574">0: Self-Supervised Video Representation Learning with Motion-Contrastive Perception</a></td>
</tr>
<tr>
<td>0</td>
<td>0.814563</td>
<td>0.844609</td>
<td><a href="https://www.semanticscholar.org/paper/6f92dcefc5f6b4346f619ae7546a8bd2d6decade">130: Dense Contrastive Learning for Self-Supervised Visual Pre-Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.808435</td>
<td>0.235224</td>
<td><a href="https://www.semanticscholar.org/paper/27f071e680905e5a2344ced8f09b9c26067f9658">0: Learning Rich Representations For Structured Visual Prediction Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.808253</td>
<td>0.949062</td>
<td><a href="https://www.semanticscholar.org/paper/35d11f5e8cc0673424b352f78e162ba7c6f587a4">1: Revisiting Low-Resolution Images Retrieval with Attention Mechanism and Contrastive Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.807939</td>
<td>0.295132</td>
<td><a href="https://www.semanticscholar.org/paper/ec65a4119efe6cf80dc3515d9966c9ded990a2f0">5: Deep Learning for Visual Understanding [From the Guest Editors]</a></td>
</tr>
<tr>
<td>0</td>
<td>0.806334</td>
<td>0.893723</td>
<td><a href="https://www.semanticscholar.org/paper/870fa9669fe9e453c011ed12f45ff37bfbb04b6b">3: Unsupervised Representation Transfer for Small Networks: I Believe I Can Distill On-the-Fly</a></td>
</tr>
<tr>
<td>0</td>
<td>0.804085</td>
<td>0.260423</td>
<td><a href="https://www.semanticscholar.org/paper/6498137be3d49b9a713d675b4aaea79a8e839ca0">18: Visual Concept Recognition and Localization via Iterative Introspection</a></td>
</tr>
<tr>
<td>0</td>
<td>0.835562</td>
<td>0.996461</td>
<td><a href="https://www.semanticscholar.org/paper/add2f205338d70e10ce5e686df4a690e2851bdfc">3096: Momentum Contrast for Unsupervised Visual Representation Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.768763</td>
<td>0.992759</td>
<td><a href="https://www.semanticscholar.org/paper/38643c2926b10f6f74f122a7037e2cd20d77c0f1">755: Supervised Contrastive Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.758217</td>
<td>0.991645</td>
<td><a href="https://www.semanticscholar.org/paper/9b09d296059909490096e34e9df2d95314787ad5">669: Learning Representations by Maximizing Mutual Information Across Views</a></td>
</tr>
<tr>
<td>0</td>
<td>0.725799</td>
<td>0.989592</td>
<td><a href="https://www.semanticscholar.org/paper/38f93092ece8eee9771e61c1edaf11b1293cae1b">1379: Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.751993</td>
<td>0.989588</td>
<td><a href="https://www.semanticscholar.org/paper/3e7f5f4382ac6f9c4fef6197dd21abf74456acd1">715: Big Self-Supervised Models are Strong Semi-Supervised Learners</a></td>
</tr>
<tr>
<td>0</td>
<td>0.781900</td>
<td>0.987954</td>
<td><a href="https://www.semanticscholar.org/paper/0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d">687: Exploring Simple Siamese Representation Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.794257</td>
<td>0.987836</td>
<td><a href="https://www.semanticscholar.org/paper/1cae417456711c4da184f5efcd1b7464a7a0661a">738: Data-Efficient Image Recognition with Contrastive Predictive Coding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.781222</td>
<td>0.987717</td>
<td><a href="https://www.semanticscholar.org/paper/88f11ad3fe04aab7f4bcf80a079140e717357f02">353: Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere</a></td>
</tr>
<tr>
<td>0</td>
<td>0.704540</td>
<td>0.986056</td>
<td><a href="https://www.semanticscholar.org/paper/97f4d09175705be4677d675fa27e55defac44800">951: Contrastive Multiview Coding</a></td>
</tr>
</table></html>
<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/34733eaf66007516347a40ad5d9bbe1cc9dacb6b">4202: A Simple Framework for Contrastive Learning of Visual Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.840260</td>
<td>0.847541</td>
<td><a href="https://www.semanticscholar.org/paper/79ef0efa7217bf85712c652b3b3640176b9e2feb">61: Unsupervised Learning of Dense Visual Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.838462</td>
<td>0.953041</td>
<td><a href="https://www.semanticscholar.org/paper/175b27969cf0c24595151fb07c84437fd0344a5a">13: i-Mix: A Strategy for Regularizing Contrastive Representation Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.815107</td>
<td>0.742287</td>
<td><a href="https://www.semanticscholar.org/paper/2a9934c1279d7abb303721681360949b96e6f574">0: Self-Supervised Video Representation Learning with Motion-Contrastive Perception</a></td>
</tr>
<tr>
<td>0</td>
<td>0.814563</td>
<td>0.844609</td>
<td><a href="https://www.semanticscholar.org/paper/6f92dcefc5f6b4346f619ae7546a8bd2d6decade">130: Dense Contrastive Learning for Self-Supervised Visual Pre-Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.808435</td>
<td>0.235224</td>
<td><a href="https://www.semanticscholar.org/paper/27f071e680905e5a2344ced8f09b9c26067f9658">0: Learning Rich Representations For Structured Visual Prediction Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.808253</td>
<td>0.949062</td>
<td><a href="https://www.semanticscholar.org/paper/35d11f5e8cc0673424b352f78e162ba7c6f587a4">1: Revisiting Low-Resolution Images Retrieval with Attention Mechanism and Contrastive Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.807939</td>
<td>0.295132</td>
<td><a href="https://www.semanticscholar.org/paper/ec65a4119efe6cf80dc3515d9966c9ded990a2f0">5: Deep Learning for Visual Understanding [From the Guest Editors]</a></td>
</tr>
<tr>
<td>0</td>
<td>0.806334</td>
<td>0.893723</td>
<td><a href="https://www.semanticscholar.org/paper/870fa9669fe9e453c011ed12f45ff37bfbb04b6b">3: Unsupervised Representation Transfer for Small Networks: I Believe I Can Distill On-the-Fly</a></td>
</tr>
<tr>
<td>0</td>
<td>0.804085</td>
<td>0.260423</td>
<td><a href="https://www.semanticscholar.org/paper/6498137be3d49b9a713d675b4aaea79a8e839ca0">18: Visual Concept Recognition and Localization via Iterative Introspection</a></td>
</tr>
<tr>
<td>0</td>
<td>0.835562</td>
<td>0.996461</td>
<td><a href="https://www.semanticscholar.org/paper/add2f205338d70e10ce5e686df4a690e2851bdfc">3096: Momentum Contrast for Unsupervised Visual Representation Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.768763</td>
<td>0.992759</td>
<td><a href="https://www.semanticscholar.org/paper/38643c2926b10f6f74f122a7037e2cd20d77c0f1">755: Supervised Contrastive Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.758217</td>
<td>0.991645</td>
<td><a href="https://www.semanticscholar.org/paper/9b09d296059909490096e34e9df2d95314787ad5">669: Learning Representations by Maximizing Mutual Information Across Views</a></td>
</tr>
<tr>
<td>0</td>
<td>0.725799</td>
<td>0.989592</td>
<td><a href="https://www.semanticscholar.org/paper/38f93092ece8eee9771e61c1edaf11b1293cae1b">1379: Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.751993</td>
<td>0.989588</td>
<td><a href="https://www.semanticscholar.org/paper/3e7f5f4382ac6f9c4fef6197dd21abf74456acd1">715: Big Self-Supervised Models are Strong Semi-Supervised Learners</a></td>
</tr>
<tr>
<td>0</td>
<td>0.781900</td>
<td>0.987954</td>
<td><a href="https://www.semanticscholar.org/paper/0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d">687: Exploring Simple Siamese Representation Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.794257</td>
<td>0.987836</td>
<td><a href="https://www.semanticscholar.org/paper/1cae417456711c4da184f5efcd1b7464a7a0661a">738: Data-Efficient Image Recognition with Contrastive Predictive Coding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.781222</td>
<td>0.987717</td>
<td><a href="https://www.semanticscholar.org/paper/88f11ad3fe04aab7f4bcf80a079140e717357f02">353: Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere</a></td>
</tr>
<tr>
<td>0</td>
<td>0.704540</td>
<td>0.986056</td>
<td><a href="https://www.semanticscholar.org/paper/97f4d09175705be4677d675fa27e55defac44800">951: Contrastive Multiview Coding</a></td>
</tr>
</table></html>
