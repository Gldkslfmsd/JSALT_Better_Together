<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/981ce6b655cc06416ff6bf7fac8c6c2076fd7fac">1061: Identifying and attacking the saddle point problem in high-dimensional non-convex optimization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.971991</td>
<td>0.967653</td>
<td><a href="https://www.semanticscholar.org/paper/8710635f1b6acceaeec1bd3422b9120a54df9dab">83: On the saddle point problem for non-convex optimization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.791204</td>
<td>0.448644</td>
<td><a href="https://www.semanticscholar.org/paper/681402a12379014f81979ebb7af64c3100f9a7dd">3: Asymptotic Properties of Stationary Solutions of Coupled Nonconvex Nonsmooth Empirical Risk Minimization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.776477</td>
<td>0.411251</td>
<td><a href="https://www.semanticscholar.org/paper/69408b840c1e5993e1b0d62d5bc49d3c26fb2d3f">0: On Solving Local Minimax Optimization: A Follow-the-Ridge Approach</a></td>
</tr>
<tr>
<td>0</td>
<td>0.772368</td>
<td>0.497130</td>
<td><a href="https://www.semanticscholar.org/paper/bd654ddebf16a6d280a128e616f6e4ec85e7251a">0: Quasi-Newton Methods for Saddle Point Problems and Beyond</a></td>
</tr>
<tr>
<td>0</td>
<td>0.770830</td>
<td>0.058031</td>
<td><a href="https://www.semanticscholar.org/paper/8a4311f9f99126bec6be494589722bcce8133f38">0: EM-type algorithms for non-convex and high-dimensional problems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.769890</td>
<td>0.805718</td>
<td><a href="https://www.semanticscholar.org/paper/33ce66750e9168cf067e7546649f6527d70b9502">6: Adaptive First-and Zeroth-order Methods for Weakly Convex Stochastic Optimization Problems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.769434</td>
<td>0.228929</td>
<td><a href="https://www.semanticscholar.org/paper/4c0c43eced3c0e14fe182e79fc7f435786bfb881">9: A note on the existence of saddle points of p-th power Lagrangian for constrained nonconvex optimization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.765893</td>
<td>0.210213</td>
<td><a href="https://www.semanticscholar.org/paper/c31e4f8322eedd0f78b818a5b1bd787614e1d310">5: Decomposable norm minimization with proximal-gradient homotopy algorithm</a></td>
</tr>
<tr>
<td>0</td>
<td>0.764319</td>
<td>0.252537</td>
<td><a href="https://www.semanticscholar.org/paper/e9524e38573fc1e4ecd679ac7a1653f6c8c781fd">1: Two generalized non-monotone explicit strongly convergent extragradient methods for solving pseudomonotone equilibrium problems and applications</a></td>
</tr>
<tr>
<td>0</td>
<td>0.735851</td>
<td>0.984454</td>
<td><a href="https://www.semanticscholar.org/paper/ad8a12a19e74d9788f8fe92f5c0dfea7b6a52aba">946: The Loss Surfaces of Multilayer Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.613096</td>
<td>0.981426</td>
<td><a href="https://www.semanticscholar.org/paper/ae99a03a48eb22e3de5ddbb16be5f945adbdafb6">11: Beyond Backprop: Alternating Minimization with co-Activation Memory</a></td>
</tr>
<tr>
<td>0</td>
<td>0.715089</td>
<td>0.977846</td>
<td><a href="https://www.semanticscholar.org/paper/82c1130394e99ec56bb4d6f0250389a8b747fe93">107: Open Problem: The landscape of the loss surfaces of multilayer networks</a></td>
</tr>
<tr>
<td>0</td>
<td>-1.000000</td>
<td>0.977829</td>
<td><a href="https://www.semanticscholar.org/paper/6d9cb3d3c0330a6c2f42d159a3a706b6b49744b2">134: The Loss Surface of Multilayer Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.597829</td>
<td>0.977466</td>
<td><a href="https://www.semanticscholar.org/paper/95daf6a30a328852bd35d8fce826e88417a99d6c">1: A Zeroth-Order Adaptive Learning Rate Method to Reduce Cost of Hyperparameter Tuning for Deep Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.601491</td>
<td>0.974465</td>
<td><a href="https://www.semanticscholar.org/paper/57df3014c4ff1bea951499b2eff9e1d6b4295263">20: Symmetry-invariant optimization in deep networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.633410</td>
<td>0.973162</td>
<td><a href="https://www.semanticscholar.org/paper/995c0cd37dfe857f54cf554bbc71e96948c5f25f">44: Convergent Block Coordinate Descent for Training Tikhonov Regularized Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.539525</td>
<td>0.972760</td>
<td><a href="https://www.semanticscholar.org/paper/761714ad5f60c24457282e05183c88362d8b13af">11: Diagonal Rescaling For Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.566295</td>
<td>0.972074</td>
<td><a href="https://www.semanticscholar.org/paper/6edccc78976b2142bf3bebc48cf7a34c04957586">0: L G ] 2 4 M ay 2 01 7 Depth Creates No Bad Local Minima</a></td>
</tr>
</table></html>
