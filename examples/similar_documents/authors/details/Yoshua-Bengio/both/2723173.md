<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/cd85a549add0c7c7def36aca29837efd24b24080">1948: FitNets: Hints for Thin Deep Nets</a></td>
</tr>
<tr>
<td>0</td>
<td>0.838153</td>
<td>0.953996</td>
<td><a href="https://www.semanticscholar.org/paper/bc6dfc6bda2d929fec91042dce1831fd07999b39">230: Improved Knowledge Distillation via Teacher Assistant</a></td>
</tr>
<tr>
<td>0</td>
<td>0.819131</td>
<td>0.766203</td>
<td><a href="https://www.semanticscholar.org/paper/f04a7069d4b6b3072084d9b82eadc91f58b0165b">47: Are ResNets Provably Better than Linear Predictors?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.814236</td>
<td>0.659641</td>
<td><a href="https://www.semanticscholar.org/paper/91a9b1a9110da18a840db9f8ccf497bb18dcd46a">0: Towards interpreting deep neural networks via layer behavior understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.803876</td>
<td>0.967572</td>
<td><a href="https://www.semanticscholar.org/paper/51db1f3c8dfc7d4077da39c96bb90a6358128111">1378: Deep Networks with Stochastic Depth</a></td>
</tr>
<tr>
<td>0</td>
<td>0.799577</td>
<td>0.751344</td>
<td><a href="https://www.semanticscholar.org/paper/3a54fb23e3d3871ff2616489607579f34ae31203">2: Shallow Learning For Deep Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.799231</td>
<td>0.823795</td>
<td><a href="https://www.semanticscholar.org/paper/5d06f68124c258c5c70c18a029a8bbdb1efec897">15: Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.796060</td>
<td>0.103885</td>
<td><a href="https://www.semanticscholar.org/paper/1b692fce2b26d9fad8e8b06f0058e4668dd88db8">1: Training Matters: Unlocking Potentials of Deeper Graph Convolutional Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792991</td>
<td>-0.001082</td>
<td><a href="https://www.semanticscholar.org/paper/ab59f928facaf3993a625e1448f68ed3ddc612fc">0: Efficient knowledge distillation of teacher model to multiple student models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.790404</td>
<td>-0.001082</td>
<td>NA:236932613</td>
</tr>
<tr>
<td>0</td>
<td>0.787633</td>
<td>0.989837</td>
<td><a href="https://www.semanticscholar.org/paper/0410659b6a311b281d10e0e44abce9b1c06be462">747: A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.731440</td>
<td>0.988015</td>
<td><a href="https://www.semanticscholar.org/paper/ea0e55bb82feceef4b8b828dc5ccdb50beff761a">58: Self-supervised Knowledge Distillation Using Singular Value Decomposition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.801516</td>
<td>0.985527</td>
<td><a href="https://www.semanticscholar.org/paper/37df8ac712c44c2e0e3ab4c14c395e34f3cba733">46: Few Sample Knowledge Distillation for Efficient Network Compression</a></td>
</tr>
<tr>
<td>0</td>
<td>0.839779</td>
<td>0.985190</td>
<td><a href="https://www.semanticscholar.org/paper/3c4b6f59a4dd9b6fb589abb826d063f7872a5808">139: Learning from Multiple Teacher Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.819718</td>
<td>0.983558</td>
<td><a href="https://www.semanticscholar.org/paper/a167d8a4ee261540c2b709dde2d94572c6ea3fc8">78: Snapshot Distillation: Teacher-Student Optimization in One Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.666328</td>
<td>0.983403</td>
<td><a href="https://www.semanticscholar.org/paper/b50995cb56ffb0f8a72aea996a93e149baa74836">31: Feature Fusion for Online Mutual Knowledge Distillation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.697014</td>
<td>0.982729</td>
<td><a href="https://www.semanticscholar.org/paper/0e0d4d4f6bc9ef53f7167b596ac779b36604f7ff">4: Tree-like Decision Distillation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.815835</td>
<td>0.982602</td>
<td><a href="https://www.semanticscholar.org/paper/108ded53967d0bfac215e590fd914ff50cd4bc62">1: Teacher-Class Network: A Neural Network Compression Mechanism</a></td>
</tr>
<tr>
<td>0</td>
<td>0.634820</td>
<td>0.982376</td>
<td><a href="https://www.semanticscholar.org/paper/ab25d65142b736dbae006bf2f268b39659dd44c7">126: Label Refinery: Improving ImageNet Classification through Label Progression</a></td>
</tr>
</table></html>
