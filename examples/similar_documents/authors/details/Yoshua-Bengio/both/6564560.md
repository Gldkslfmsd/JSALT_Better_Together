<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/123ae35aa7d6838c817072032ce5615bb891652d">553: BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811813</td>
<td>0.940002</td>
<td><a href="https://www.semanticscholar.org/paper/cb34321f97082d47c4ac6c34bcd5cf65d72edc8e">67: BNN+: Improved Binary Network Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.783017</td>
<td>0.593083</td>
<td><a href="https://www.semanticscholar.org/paper/1e54f15e2f9f5f6024379658cce3bd0f11631974">0: Two-dimensional perceptrons</a></td>
</tr>
<tr>
<td>0</td>
<td>0.781808</td>
<td>0.921635</td>
<td><a href="https://www.semanticscholar.org/paper/481aa9aa287a979a22ecf7f362c713a0225f4a4d">6: DSConv: Efficient Convolution Operator</a></td>
</tr>
<tr>
<td>0</td>
<td>0.771678</td>
<td>0.789142</td>
<td><a href="https://www.semanticscholar.org/paper/309037d7fa4bf15768b23ba77c959ceb8cd18c87">6: Rapid training of deep neural networks without skip connections or normalization layers using Deep Kernel Shaping</a></td>
</tr>
<tr>
<td>0</td>
<td>0.769613</td>
<td>0.964335</td>
<td><a href="https://www.semanticscholar.org/paper/c94965b8d36ae6c71a6d29a100530e05cc79f19b">11: ADaPTION: Toolbox and Benchmark for Training Convolutional Neural Networks with Reduced Numerical Precision Weights and Activation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.764887</td>
<td>0.859342</td>
<td><a href="https://www.semanticscholar.org/paper/50d6dda7794a225e0cfc81334e3a3135b459188c">3: A comprehensive review of Binary Neural Network</a></td>
</tr>
<tr>
<td>0</td>
<td>0.762587</td>
<td>0.587124</td>
<td><a href="https://www.semanticscholar.org/paper/4d376d6978dad0374edfa6709c9556b42d3594d3">28670: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></td>
</tr>
<tr>
<td>0</td>
<td>0.761695</td>
<td>0.914682</td>
<td><a href="https://www.semanticscholar.org/paper/976e07e7f4f766583f88e15a89b588be1bf35028">0: Bit-wise Training of Neural Network Weights</a></td>
</tr>
<tr>
<td>0</td>
<td>0.759942</td>
<td>0.292400</td>
<td><a href="https://www.semanticscholar.org/paper/e6816c2a0e3f468a0a7640a19a39e462b19e8020">1: Applying the Maxout Model to Increase the Performance of the Multilayer Perceptron in Shallow Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.780650</td>
<td>0.997329</td>
<td><a href="https://www.semanticscholar.org/paper/1b82d54e9a3b06c603d7987ba3ecf437425f6330">455: Training deep neural networks with low precision multiplications</a></td>
</tr>
<tr>
<td>0</td>
<td>0.817969</td>
<td>0.995307</td>
<td><a href="https://www.semanticscholar.org/paper/67c191bcce6821f736798cb9b31472bcdd1e52a6">274: Neural Networks with Few Multiplications</a></td>
</tr>
<tr>
<td>0</td>
<td>-1.000000</td>
<td>0.994711</td>
<td><a href="https://www.semanticscholar.org/paper/db55998b130cc5020e3f83e45192bfdfbe92d6e3">629: XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.731203</td>
<td>0.994153</td>
<td><a href="https://www.semanticscholar.org/paper/397de65a9a815ec39b3704a79341d687205bc80a">53: A Deep Neural Network Compression Pipeline: Pruning, Quantization, Huffman Encoding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.728314</td>
<td>0.994133</td>
<td><a href="https://www.semanticscholar.org/paper/b7cf49e30355633af2db19f35189410c8515e91f">1478: Deep Learning with Limited Numerical Precision</a></td>
</tr>
<tr>
<td>0</td>
<td>0.748257</td>
<td>0.993035</td>
<td><a href="https://www.semanticscholar.org/paper/f73e69f2376e793590aa751effd05aef57c639ff">138: Ternary neural networks for resource-efficient AI applications</a></td>
</tr>
<tr>
<td>0</td>
<td>0.624002</td>
<td>0.992958</td>
<td><a href="https://www.semanticscholar.org/paper/0a58e39761d7c38e08a6a4d5ddcf02326bf8a1bf">5: Optimality Assessment of Memory-Bounded ConvNets Deployed on Resource-Constrained RISC Cores</a></td>
</tr>
<tr>
<td>0</td>
<td>0.754177</td>
<td>0.992747</td>
<td><a href="https://www.semanticscholar.org/paper/1d29498544257bba3761f25d939bc10e1d46dd39">114: Resiliency of Deep Neural Networks under Quantization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.740323</td>
<td>0.992620</td>
<td><a href="https://www.semanticscholar.org/paper/6a5a8b424e1a498bbf323934291e79ae6dfecc84">72: Deep Convolutional Neural Network Inference with Floating-point Weights and Fixed-point Activations</a></td>
</tr>
</table></html>
