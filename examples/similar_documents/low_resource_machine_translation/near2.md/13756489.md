<html><table><tr>
<th>embedding</th>
<th>cos</th>
<th>id1</th>
<th>id2</th>
<th>paper1</th>
<th>paper2</th>
</tr>
<tr>
<td>proposed</td>
<td>1.000000</td>
<td>13756489</td>
<td>13756489</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
</tr>
<tr>
<td>proposed</td>
<td>0.986160</td>
<td>13756489</td>
<td>3725815</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/c8efcc854d97dfc2a42b83316a2109f9d166e43f">1390: Self-Attention with Relative Position Representations</a></td>
</tr>
<tr>
<td>proposed</td>
<td>0.980694</td>
<td>13756489</td>
<td>173990158</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/949fef650da4c41afe6049a183b504b3cc91f4bd">513: Multimodal Transformer for Unaligned Multimodal Language Sequences</a></td>
</tr>
<tr>
<td>proposed</td>
<td>0.975534</td>
<td>13756489</td>
<td>57759363</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/c4744a7c2bb298e4a52289a1e085c71cc3d37bc6">2464: Transformer-XL: Attentive Language Models beyond a Fixed-Length Context</a></td>
</tr>
<tr>
<td>proposed</td>
<td>0.975361</td>
<td>13756489</td>
<td>49667762</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/ac4dafdef1d2b685b7f28a11837414573d39ff4e">514: Universal Transformers</a></td>
</tr>
<tr>
<td>proposed</td>
<td>0.971838</td>
<td>13756489</td>
<td>234762885</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/08ffdec40291a2ccb5f8a6cc048b01247fb34b96">22: Relative Positional Encoding for Transformers with Linear Complexity</a></td>
</tr>
<tr>
<td>proposed</td>
<td>0.971374</td>
<td>13756489</td>
<td>201698358</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/8cef9900c04d7f661c08f4b5b1ed4337ace042a3">111: Transformer Dissection: An Unified Understanding for Transformerâ€™s Attention via the Lens of Kernel</a></td>
</tr>
<tr>
<td>proposed</td>
<td>0.969607</td>
<td>13756489</td>
<td>235368340</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/d8d2e574965fe733eb1416e03df2b5c2914fc530">243: A Survey of Transformers</a></td>
</tr>
<tr>
<td>proposed</td>
<td>0.969351</td>
<td>13756489</td>
<td>159041867</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/f4238bd2385a52413ccbacfd9e409a650235bd13">208: Adaptive Attention Span in Transformers</a></td>
</tr>
<tr>
<td>proposed</td>
<td>0.968862</td>
<td>13756489</td>
<td>174799399</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/a39398f68ae7e042f2ef5009e31b4e6a20fd5736">395: Learning Deep Transformer Models for Machine Translation</a></td>
</tr>
<tr>
<td>specter2</td>
<td>1.000000</td>
<td>13756489</td>
<td>13756489</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
</tr>
<tr>
<td>specter2</td>
<td>0.949891</td>
<td>13756489</td>
<td>184482605</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/9911b0e21b5229d3e87ab4c48371cdd5474d7120">5: UC Davis at SemEval-2019 Task 1: DAG Semantic Parsing with Attention-based Decoder</a></td>
</tr>
<tr>
<td>specter2</td>
<td>0.943217</td>
<td>13756489</td>
<td>7831483</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/e837b79de602c69395498c1fbbe39bbb4e6f75ad">276: Learning to Transduce with Unbounded Memory</a></td>
</tr>
<tr>
<td>specter2</td>
<td>0.939371</td>
<td>13756489</td>
<td>202572959</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/0ce184bd55a4736ec64e5d82a85421298e0373ea">496: A Comparative Study on Transformer vs RNN in Speech Applications</a></td>
</tr>
<tr>
<td>specter2</td>
<td>0.937087</td>
<td>13756489</td>
<td>65578684</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/949f014a215c4ba4db5da2eb1f0e4098efd39d7d">65: Making sense of neural machine translation</a></td>
</tr>
<tr>
<td>specter2</td>
<td>0.934651</td>
<td>13756489</td>
<td>221971055</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/0f787519c22c5e2009c41c3343fedef7905b6f54">25: What does it mean to be language-agnostic? Probing multilingual sentence encoders for typological properties</a></td>
</tr>
<tr>
<td>specter2</td>
<td>0.933798</td>
<td>13756489</td>
<td>31481386</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/b2fd9d0402642fab489e8ca8a18adae049145ab2">69: Guiding Neural Machine Translation Decoding with External Knowledge</a></td>
</tr>
<tr>
<td>specter2</td>
<td>0.933096</td>
<td>13756489</td>
<td>1939094</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/84481a9a79e2d283210ca38308e297fe0a082a67">7: Transformation and Decomposition for Efficiently Implementing and Improving Dependency-to-String Model In Moses</a></td>
</tr>
<tr>
<td>specter2</td>
<td>0.931335</td>
<td>13756489</td>
<td>246015516</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/e475d7c3b10d548e59590902474ff99206a732f3">2: The Dark Side of the Language: Pre-trained Transformers in the DarkNet</a></td>
</tr>
<tr>
<td>specter2</td>
<td>0.930967</td>
<td>13756489</td>
<td>238242056</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/d30a77c0ac45fd89bf188d9615e6ae329ae16ade">5: Simple But Effective GRU Variants</a></td>
</tr>
<tr>
<td>specter</td>
<td>1.000000</td>
<td>13756489</td>
<td>13756489</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
</tr>
<tr>
<td>specter</td>
<td>0.884208</td>
<td>13756489</td>
<td>29769422</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/3861ae2a6bdd2a759c2d901a6583e63a216bc2fc">114: Weighted Transformer Network for Machine Translation</a></td>
</tr>
<tr>
<td>specter</td>
<td>0.838452</td>
<td>13756489</td>
<td>4842909</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/8c1b00128e74f1cd92aede3959690615695d5101">942: QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension</a></td>
</tr>
<tr>
<td>specter</td>
<td>0.832856</td>
<td>13756489</td>
<td>237278113</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/015b2fb3a12bd0134b91af272b29b62371ef9022">0: Recurrent multiple shared layers in Depth for Neural Machine Translation</a></td>
</tr>
<tr>
<td>specter</td>
<td>0.830833</td>
<td>13756489</td>
<td>232105052</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/9ed25f101f19ea735ca300848948ed64064b97ca">183: Random Feature Attention</a></td>
</tr>
<tr>
<td>specter</td>
<td>0.826631</td>
<td>13756489</td>
<td>41563021</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/4287831564f99d3e9af102d451a3087c20171490">14: CNN Is All You Need</a></td>
</tr>
<tr>
<td>specter</td>
<td>0.826304</td>
<td>13756489</td>
<td>248023020</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/b4f19c43e05f4d5c06c703c1fe2b399e019dfe5c">0: Reproduce Simple QANet on SQuAD 2.0</a></td>
</tr>
<tr>
<td>specter</td>
<td>0.825661</td>
<td>13756489</td>
<td>67265754</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td>Paper with id CorpusId:67265754 not found</td>
</tr>
<tr>
<td>specter</td>
<td>0.825534</td>
<td>13756489</td>
<td>222209122</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/53af14897ccca4f8d6c465ee133e859f99ddd7e7">39: Shallow-to-Deep Training for Neural Machine Translation</a></td>
</tr>
<tr>
<td>specter</td>
<td>0.823175</td>
<td>13756489</td>
<td>6461746</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/6a31ff302d15f5159dc566603f15ac1d774c290b">9: An Efficient Character-Level Neural Machine Translation</a></td>
</tr>
<tr>
<td>specter.K280</td>
<td>1.000000</td>
<td>13756489</td>
<td>13756489</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
</tr>
<tr>
<td>specter.K280</td>
<td>0.884840</td>
<td>13756489</td>
<td>29769422</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/3861ae2a6bdd2a759c2d901a6583e63a216bc2fc">114: Weighted Transformer Network for Machine Translation</a></td>
</tr>
<tr>
<td>specter.K280</td>
<td>0.849938</td>
<td>13756489</td>
<td>237278113</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/015b2fb3a12bd0134b91af272b29b62371ef9022">0: Recurrent multiple shared layers in Depth for Neural Machine Translation</a></td>
</tr>
<tr>
<td>specter.K280</td>
<td>0.848056</td>
<td>13756489</td>
<td>4842909</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/8c1b00128e74f1cd92aede3959690615695d5101">942: QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension</a></td>
</tr>
<tr>
<td>specter.K280</td>
<td>0.843385</td>
<td>13756489</td>
<td>6461746</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/6a31ff302d15f5159dc566603f15ac1d774c290b">9: An Efficient Character-Level Neural Machine Translation</a></td>
</tr>
<tr>
<td>specter.K280</td>
<td>0.838706</td>
<td>13756489</td>
<td>67265754</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td>Paper with id CorpusId:67265754 not found</td>
</tr>
<tr>
<td>specter.K280</td>
<td>0.835126</td>
<td>13756489</td>
<td>232105052</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/9ed25f101f19ea735ca300848948ed64064b97ca">183: Random Feature Attention</a></td>
</tr>
<tr>
<td>specter.K280</td>
<td>0.834353</td>
<td>13756489</td>
<td>234336004</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/1f133158a8973fb33fea188f20517cd7e69bfe7f">202: FNet: Mixing Tokens with Fourier Transforms</a></td>
</tr>
<tr>
<td>specter.K280</td>
<td>0.830981</td>
<td>13756489</td>
<td>188797700</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/a26a98f40149cf89d9f73f6c59cd443d5982d9d1">0: The big question</a></td>
</tr>
<tr>
<td>specter.K280</td>
<td>0.829584</td>
<td>13756489</td>
<td>49864419</td>
<td><a href="https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776">58361: Attention is All you Need</a></td>
<td><a href="https://www.semanticscholar.org/paper/10bb4ef7a6719ea132e00f0ab5680919a4131d99">524: BAM: Bottleneck Attention Module</a></td>
</tr>
</table></html>
