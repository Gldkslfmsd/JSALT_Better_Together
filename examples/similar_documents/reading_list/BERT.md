<html><table><tr>
<th>Method</th>
<th>cosS</th>
<th>cosP</th>
<th>paper</th>
</tr>
<tr>
<td>Specter</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992">32318: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.874</td>
<td>0.810</td>
<td><a href="https://www.semanticscholar.org/paper/c79a8fd667f59e6f1ca9d54afc34f792e9079c7e">8: TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.872</td>
<td>0.930</td>
<td><a href="https://www.semanticscholar.org/paper/6c8503803760c5c7790f72437d0f8b874334e6f0">25: Span Selection Pre-training for Question Answering</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.865</td>
<td>0.862</td>
<td><a href="https://www.semanticscholar.org/paper/8617b501fedf65efaf82c3f911fe490407ba3650">7: BERT for Question Answering on SQuAD 2 . 0</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.861</td>
<td>0.933</td>
<td><a href="https://www.semanticscholar.org/paper/34591cdb1e96e4ebe4bec38da9e88718f5c3d2df">171: Incorporating BERT into Neural Machine Translation</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.852</td>
<td>0.989</td>
<td><a href="https://www.semanticscholar.org/paper/46b3ba0f3cb8340bc94f26e0fdf6dc4e38f68948">67: Hierarchical Transformers for Long Document Classification</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.835</td>
<td>0.968</td>
<td><a href="https://www.semanticscholar.org/paper/9ba6ad0de7dbe1a3b10c44106049adb96f87d483">5: KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.833</td>
<td>0.862</td>
<td><a href="https://www.semanticscholar.org/paper/80e949e0e58e06c6f4f75ac4a2d7216b0fa1cfb8">25: Recycling a Pre-trained BERT Encoder for Neural Machine Translation</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.833</td>
<td>0.927</td>
<td><a href="https://www.semanticscholar.org/paper/7d767f64e88fdec81a24190c629dcfe23c940793">21: Natural Language Generation for Effective Knowledge Distillation</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.832</td>
<td>0.921</td>
<td><a href="https://www.semanticscholar.org/paper/8f8504aa422cada0f95a44255f804f1d79067b73">2: Latent Universal Task-Specific BERT</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992">32318: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.804</td>
<td>0.990</td>
<td><a href="https://www.semanticscholar.org/paper/e0c6abdbdecf04ffac65c440da77fb9d66bb474c">4103: XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.852</td>
<td>0.989</td>
<td><a href="https://www.semanticscholar.org/paper/46b3ba0f3cb8340bc94f26e0fdf6dc4e38f68948">67: Hierarchical Transformers for Long Document Classification</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.727</td>
<td>0.988</td>
<td><a href="https://www.semanticscholar.org/paper/afd110eace912c2b273e64851c6b4df2658622eb">215: Visualizing and Measuring the Geometry of BERT</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.522</td>
<td>0.987</td>
<td><a href="https://www.semanticscholar.org/paper/4b73e1b62791b25225bacd92d0163f71409e6022">10: Example-Based Named Entity Recognition</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.782</td>
<td>0.987</td>
<td><a href="https://www.semanticscholar.org/paper/a022bda79947d1f656a1164003c1b3ae9a843df9">601: How to Fine-Tune BERT for Text Classification?</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.791</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/1fa9ed2bea208511ae698a967875e943049f16b6">2630: HuggingFace's Transformers: State-of-the-art Natural Language Processing</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.805</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035">3381: Improving Language Understanding by Generative Pre-Training</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.655</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/802e8a4b2d530883f1b2d3642ac14a1b37d90afc">3: PGSG at SemEval-2020 Task 12: BERT-LSTM with Tweetsâ€™ Pretrained Model and Noisy Student Training Method</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.724</td>
<td>0.985</td>
<td><a href="https://www.semanticscholar.org/paper/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de">6919: RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></td>
</tr>
</table></html>
