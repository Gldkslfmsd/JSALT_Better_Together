<html><table><tr>
<th>Method</th>
<th>cosS</th>
<th>cosP</th>
<th>paper</th>
</tr>
<tr>
<td>Specter</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/853d4d94651c6d9f8ed4d114e1eb21f15f786daa">284: A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.894</td>
<td>0.750</td>
<td><a href="https://www.semanticscholar.org/paper/4cdd370ab288d6a8cde0f7b25eb6abe18018cdeb">2: Abstractive summarization by neural attention model with document content memory</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.865</td>
<td>0.969</td>
<td><a href="https://www.semanticscholar.org/paper/99ea0837b8be4583366156d3a83f83c0b41d9eb0">15: On the Abstractiveness of Neural Document Summarization</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.862</td>
<td>0.883</td>
<td><a href="https://www.semanticscholar.org/paper/e0e6d4270c14aceb8491c57f07ff9d894feca07c">2: An Effective Joint Framework for Document Summarization</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.850</td>
<td>0.943</td>
<td><a href="https://www.semanticscholar.org/paper/29a294eaec7b485245aa21d994f7300f6b5da8fc">591: Neural Summarization by Extracting Sentences and Words</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.848</td>
<td>0.924</td>
<td><a href="https://www.semanticscholar.org/paper/afc3b39d8aa0100d7aeb6123668a9bec9bd6c63e">20: Adapting Neural Single-Document Summarization Model for Abstractive Multi-Document Summarization: A Pilot Study</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.847</td>
<td>0.988</td>
<td><a href="https://www.semanticscholar.org/paper/0a0c6f5223b807c107f27ccebc6d16e9ff84a97d">0: A Long Texts Summarization Approach to Scientific Articles</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.844</td>
<td>0.909</td>
<td><a href="https://www.semanticscholar.org/paper/ffc43c1264029c7fc655e190fcb34d079643f34c">4: Abstractive Summarization for Amazon Reviews</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.843</td>
<td>0.996</td>
<td><a href="https://www.semanticscholar.org/paper/7cc730da554003dda77796d2cb4f06da5dfd5592">169: Hierarchical Transformers for Multi-Document Summarization</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.841</td>
<td>0.953</td>
<td><a href="https://www.semanticscholar.org/paper/9b3f7cf0550833208ee4dffaf45549b19dcfa867">9: Topic Augmented Generator for Abstractive Summarization</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.838</td>
<td>0.778</td>
<td><a href="https://www.semanticscholar.org/paper/16d0afaeb8419ec1c37c3473ab581df916148d72">0: Explorer Neural Latent Extractive Document Summarization</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.838</td>
<td>0.980</td>
<td><a href="https://www.semanticscholar.org/paper/193b92b2c703dff93fbb0d58070fd2b7651ab3f3">69: Extractive Summarization of Long Documents by Combining Global and Local Context</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.838</td>
<td>0.892</td>
<td><a href="https://www.semanticscholar.org/paper/309a8aef55ca8f89ef56973bb2c3b38d84a29113">56: Neural Extractive Summarization with Side Information</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.838</td>
<td>0.937</td>
<td><a href="https://www.semanticscholar.org/paper/01f8314c8e03af0afb8391576d1ffb9ed821f199">21: Improving Neural Abstractive Document Summarization with Structural Regularization</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.836</td>
<td>0.821</td>
<td><a href="https://www.semanticscholar.org/paper/60606b1939c630f94a2f9152e36849d79307c0c0">0: MeanSum: A Model for Unsupervised Neural Multi-document Abstractive Summarization.</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.832</td>
<td>0.916</td>
<td><a href="https://www.semanticscholar.org/paper/aa7bf38298f90539e6461e30c964c51adb91cf57">62: A Supervised Approach to Extractive Summarisation of Scientific Papers</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.831</td>
<td>0.766</td>
<td><a href="https://www.semanticscholar.org/paper/bd6e5d4eacedc1c54c053deb4648bec0fabec8bf">24: Extractive Text Summarization using Neural Networks</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.828</td>
<td>0.984</td>
<td><a href="https://www.semanticscholar.org/paper/c6a712f98be7f0a9957c373fafa4b2fcfe4d661b">5: Enriching and Controlling Global Semantics for Text Summarization</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.824</td>
<td>0.830</td>
<td><a href="https://www.semanticscholar.org/paper/1ef87a2ca7867492285a1fb3fc83bd325d4dcac6">2: Abstractive Summarization with the Aid of Extractive Summarization</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.822</td>
<td>0.902</td>
<td><a href="https://www.semanticscholar.org/paper/fb3a2ba6cedaeb56a8cac9c61be8fee58efe6de3">155: Deep Recurrent Generative Decoder for Abstractive Text Summarization</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/853d4d94651c6d9f8ed4d114e1eb21f15f786daa">284: A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.715</td>
<td>0.998</td>
<td><a href="https://www.semanticscholar.org/paper/917f8fd2802b04ea0e8e51210457f0f904de97ae">103: Pretraining-Based Natural Language Generation for Text Summarization</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.798</td>
<td>0.998</td>
<td><a href="https://www.semanticscholar.org/paper/7af89df3691d8c33aaf1858f7cc51da1bc9549a9">445: Bottom-Up Abstractive Summarization</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.807</td>
<td>0.998</td>
<td><a href="https://www.semanticscholar.org/paper/cc27ec53160d88c25fc5096c0df65536eb780de4">186: Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.770</td>
<td>0.997</td>
<td><a href="https://www.semanticscholar.org/paper/367c41f623f86e75d3154f6cab5b749cb7eb06b5">73: Searching for Effective Neural Extractive Summarization: What Works and Whatâ€™s Next</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.765</td>
<td>0.997</td>
<td><a href="https://www.semanticscholar.org/paper/4e346eb1628df6a12c1a121f862fb3a16c6fec60">266: Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.752</td>
<td>0.997</td>
<td><a href="https://www.semanticscholar.org/paper/929b4775b6896634e11a8feb0ca4ca64ef7b3e24">136: Extractive Summarization as Text Matching</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.811</td>
<td>0.997</td>
<td><a href="https://www.semanticscholar.org/paper/cfa90e184cab9701a68e9b2fdd9222a1f508a354">21: Efficient Adaptation of Pretrained Transformers for Abstractive Summarization</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.808</td>
<td>0.997</td>
<td><a href="https://www.semanticscholar.org/paper/9cc0004792c55b89612a8a99668375b415bef2a4">6: Discourse Understanding and Factual Consistency in Abstractive Summarization</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.668</td>
<td>0.996</td>
<td><a href="https://www.semanticscholar.org/paper/15bb07d0996ece844de8cae24d3dc15972e6841a">4: How well do you know your summarization datasets?</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.826</td>
<td>0.996</td>
<td><a href="https://www.semanticscholar.org/paper/636519d62d0257d9cab06962dfeb5e6020a5bc69">28: Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.672</td>
<td>0.996</td>
<td><a href="https://www.semanticscholar.org/paper/93dc7870d37ea8aad5dc282d255dacf4fef33821">86: Optimizing the Factual Correctness of a Summary: A Study of Summarizing Radiology Reports</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.843</td>
<td>0.996</td>
<td><a href="https://www.semanticscholar.org/paper/7cc730da554003dda77796d2cb4f06da5dfd5592">169: Hierarchical Transformers for Multi-Document Summarization</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.803</td>
<td>0.996</td>
<td><a href="https://www.semanticscholar.org/paper/5665805becad6c87b194b260f2270d86d560bd3f">84: On Extractive and Abstractive Neural Document Summarization with Transformer Language Models</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.698</td>
<td>0.995</td>
<td><a href="https://www.semanticscholar.org/paper/eb011ccdf9ea739ea86be85b268a4d958266b624">39: Sample Efficient Text Summarization Using a Single Pre-Trained Transformer</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.810</td>
<td>0.995</td>
<td><a href="https://www.semanticscholar.org/paper/af93e1accba69994cdc36254ef93584af307fd8a">166: Neural Text Summarization: A Critical Evaluation</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.803</td>
<td>0.995</td>
<td><a href="https://www.semanticscholar.org/paper/86cb79083bfa5dc6329ab1b8c7099af76fefde36">45: A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.769</td>
<td>0.995</td>
<td><a href="https://www.semanticscholar.org/paper/dae58784124ddef4e8d2cab3692e02ec8ce4ec66">1: Exploring Multitask Learning for Low-Resource AbstractiveSummarization</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.780</td>
<td>0.994</td>
<td><a href="https://www.semanticscholar.org/paper/2b590573c920ddf166cc0b9f1c2d100858c9ac84">10: FFCI: A Framework for Interpretable Automatic Evaluation of Summarization</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.638</td>
<td>0.994</td>
<td><a href="https://www.semanticscholar.org/paper/2dd84aa297426ff5d78760d1358c9167e84e944b">22: Cross-Lingual Training for Automatic Question Generation</a></td>
</tr>
</table></html>
