<html><table><tr>
<th>Method</th>
<th>cosS</th>
<th>cosP</th>
<th>paper</th>
</tr>
<tr>
<td>Specter</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/f4c018bcc8ea707b83247866bdc8ccb87cd9f5da">1561: Neural Word Embedding as Implicit Matrix Factorization</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.890</td>
<td>0.990</td>
<td><a href="https://www.semanticscholar.org/paper/cc14a56eb0361261f9294646a727dc853813c532">106: Word Embedding Revisited: A New Representation Learning and Explicit Matrix Factorization Perspective</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.839</td>
<td>0.946</td>
<td><a href="https://www.semanticscholar.org/paper/9b235727491a6057ced4248eb1f9ec0750232c44">2: Continuous Word Embedding Fusion via Spectral Decomposition</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.835</td>
<td>0.865</td>
<td><a href="https://www.semanticscholar.org/paper/04a22a9eb76b399a78467728fb1c9cf4507a7031">3: Word Embedding With Zipfâ€™s Context</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.831</td>
<td>0.923</td>
<td><a href="https://www.semanticscholar.org/paper/1f50db5786913b43f9668f997fc4c97d9cd18730">9: Spectral Word Embedding with Negative Sampling</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.818</td>
<td>0.896</td>
<td><a href="https://www.semanticscholar.org/paper/f59cb86bf5c9221301841130f02ff473b561bfaa">0: Unsupervised Learning of Multi-Sense Embedding with Matrix Factorization and Sparse Soft Clustering</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.812</td>
<td>0.956</td>
<td><a href="https://www.semanticscholar.org/paper/c8f09a71fa6b2a4f5ae86296e5c22ae75414a916">114: Symmetric Pattern Based Word Embeddings for Improved Word Similarity Prediction</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.812</td>
<td>0.878</td>
<td><a href="https://www.semanticscholar.org/paper/3f7b6fe8a850c0632c6b97b0190d26f5650e6097">1: Word2rate: training and evaluating multiple word embeddings as statistical transitions</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.812</td>
<td>0.977</td>
<td><a href="https://www.semanticscholar.org/paper/3290c8aa90285e95e302419474ba7ef96944feac">11: PSDVec: a Toolbox for Incremental and Scalable Word Embedding</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.804</td>
<td>0.004</td>
<td><a href="https://www.semanticscholar.org/paper/a43295286e4cba02465dd198107963d8eada0124">0: Complex-Valued Vectors for Word Representation</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/f4c018bcc8ea707b83247866bdc8ccb87cd9f5da">1561: Neural Word Embedding as Implicit Matrix Factorization</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.675</td>
<td>0.991</td>
<td><a href="https://www.semanticscholar.org/paper/c485fa1e053fe65621bb76bf0ab1789472e21427">25: Incremental Skip-gram Model with Negative Sampling</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.640</td>
<td>0.990</td>
<td><a href="https://www.semanticscholar.org/paper/2ff5b6adcb22a02606afc6a77a5cc6937478750e">7: Efficient and accurate Word2Vec implementations in GPU and shared-memory multicore architectures</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.890</td>
<td>0.990</td>
<td><a href="https://www.semanticscholar.org/paper/cc14a56eb0361261f9294646a727dc853813c532">106: Word Embedding Revisited: A New Representation Learning and Explicit Matrix Factorization Perspective</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.775</td>
<td>0.989</td>
<td><a href="https://www.semanticscholar.org/paper/720ad49010638bcf93de3a58689de84a047d3040">17: What the Vec? Towards Probabilistically Grounded Embeddings</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.726</td>
<td>0.989</td>
<td><a href="https://www.semanticscholar.org/paper/c5dba6ade9795f6ba42a011b16929bcc34d4ca58">79: Hierarchical Neural Language Models for Joint Representation of Streaming Documents and their Content</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.751</td>
<td>0.989</td>
<td><a href="https://www.semanticscholar.org/paper/2012f32199adc88747d5a1b47c7b4ba1cb3cb995">1184: word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.510</td>
<td>0.988</td>
<td><a href="https://www.semanticscholar.org/paper/193e0149fd1133b75c8ca4d58dcebb03046f1345">1: Generating Distributed Representation of User Movement for Extracting Detour Spots</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.779</td>
<td>0.988</td>
<td><a href="https://www.semanticscholar.org/paper/80daf3097356bfbae455a9b86a5e0f0fb0479ae3">53: Random Walks on Context Spaces: Towards an Explanation of the Mysteries of Semantic Word Embeddings</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.695</td>
<td>0.987</td>
<td><a href="https://www.semanticscholar.org/paper/3b5093c165a6f7118c294fcf611b3a206ac5107d">16: word2vec Skip-Gram with Negative Sampling is a Weighted Logistic PCA</a></td>
</tr>
</table></html>
