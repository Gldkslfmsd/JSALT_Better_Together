<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/455a8838cde44f288d456d01c76ede95b56dc675">581: A Structural Probe for Finding Syntax in Word Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809918</td>
<td>0.907482</td>
<td><a href="https://www.semanticscholar.org/paper/4c636fa1843d15d0b0b7d3b8e3dca376b606abc7">8: Shallow Syntax in Deep Water</a></td>
</tr>
<tr>
<td>0</td>
<td>0.805367</td>
<td>0.867087</td>
<td><a href="https://www.semanticscholar.org/paper/ca03a22db1983e2d7a3b11e7ff78c5a1549ad593">8: Probing sentence embeddings for structure-dependent tense</a></td>
</tr>
<tr>
<td>0</td>
<td>0.805161</td>
<td>0.465672</td>
<td><a href="https://www.semanticscholar.org/paper/acf7dd2137dc79e99733311b1f3cb704f876e15e">1: Syntactic realization with data-driven neural tree grammars</a></td>
</tr>
<tr>
<td>0</td>
<td>0.794932</td>
<td>0.971894</td>
<td><a href="https://www.semanticscholar.org/paper/f0ed5550bafbe8d2ec85f8ba519d55b0775fa359">6: Picking BERT’s Brain: Probing for Linguistic Dependencies in Contextualized Embeddings Using Representational Similarity Analysis</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792626</td>
<td>0.773138</td>
<td><a href="https://www.semanticscholar.org/paper/2efd695b61d7f54615de8cf447f36a45926e3df2">2: Gating Mechanisms for Combining Character and Word-level Word Representations: an Empirical Study</a></td>
</tr>
<tr>
<td>0</td>
<td>0.789741</td>
<td>-0.005785</td>
<td>NA:200115164</td>
</tr>
<tr>
<td>0</td>
<td>0.787071</td>
<td>0.531378</td>
<td><a href="https://www.semanticscholar.org/paper/70c50befabf67b01edd06e885bd11df74c6b89ea">1: A systematic evaluation of semantic representations in natural language processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.780457</td>
<td>0.945649</td>
<td><a href="https://www.semanticscholar.org/paper/191e5153ccc9722c5a44d79c7ceda52c947a9dcb">11: Probing Multilingual Sentence Representations With X-Probe</a></td>
</tr>
<tr>
<td>0</td>
<td>0.775243</td>
<td>0.833217</td>
<td><a href="https://www.semanticscholar.org/paper/a9423f8152e72b964ceadda9f7674a9aaebfea41">2: Are Neural Networks Extracting Linguistic Properties or Memorizing Training Data? An Observation with a Multilingual Probe for Predicting Tense</a></td>
</tr>
<tr>
<td>0</td>
<td>0.802522</td>
<td>0.999122</td>
<td><a href="https://www.semanticscholar.org/paper/e2587eddd57bc4ba286d91b27c185083f16f40ee">484: What do you learn from context? Probing for sentence structure in contextualized word representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.660357</td>
<td>0.998765</td>
<td><a href="https://www.semanticscholar.org/paper/efeab0dcdb4c1cce5e537e57745d84774be99b9a">311: Assessing BERT's Syntactic Abilities</a></td>
</tr>
<tr>
<td>0</td>
<td>0.701998</td>
<td>0.998670</td>
<td><a href="https://www.semanticscholar.org/paper/97906df07855b029b7aae7c2a1c6c5e8df1d531c">653: BERT Rediscovers the Classical NLP Pipeline</a></td>
</tr>
<tr>
<td>0</td>
<td>0.694765</td>
<td>0.998610</td>
<td><a href="https://www.semanticscholar.org/paper/0427110f0e79f41e69a8eb00a3ec8868bac26a4f">126: Do NLP Models Know Numbers? Probing Numeracy in Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.753337</td>
<td>0.998530</td>
<td><a href="https://www.semanticscholar.org/paper/335613303ebc5eac98de757ed02a56377d99e03a">544: What Does BERT Learn about the Structure of Language?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.745719</td>
<td>0.997948</td>
<td><a href="https://www.semanticscholar.org/paper/f6fbb6809374ca57205bd2cf1421d4f4fa04f975">429: Linguistic Knowledge and Transferability of Contextual Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.636901</td>
<td>0.997604</td>
<td><a href="https://www.semanticscholar.org/paper/2ff41a463a374b138bb5a012e5a32bc4beefec20">304: Pre-Training With Whole Word Masking for Chinese BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.737161</td>
<td>0.997547</td>
<td><a href="https://www.semanticscholar.org/paper/165d51a547cd920e6ac55660ad5c404dcb9562ed">157: Open Sesame: Getting inside BERT’s Linguistic Knowledge</a></td>
</tr>
<tr>
<td>0</td>
<td>0.672990</td>
<td>0.997238</td>
<td><a href="https://www.semanticscholar.org/paper/809cc93921e4698bde891475254ad6dfba33d03b">623: How Multilingual is Multilingual BERT?</a></td>
</tr>
</table></html>
