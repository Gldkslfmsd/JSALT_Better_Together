<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/f37e1b62a767a307c046404ca96bc140b3e68cb5">22193: GloVe: Global Vectors for Word Representation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.853153</td>
<td>0.883146</td>
<td><a href="https://www.semanticscholar.org/paper/bca7cf5353f453a3e0993af847accc219f14797d">0: Measuring Enrichment Of Word Embeddings With Subword And Dictionary Information</a></td>
</tr>
<tr>
<td>0</td>
<td>0.850273</td>
<td>0.845094</td>
<td><a href="https://www.semanticscholar.org/paper/1a57b15f2e0b122028925b580c3b616980f5fd30">9: Modeling Context Words as Regions: An Ordinal Regression Approach to Word Embedding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.846087</td>
<td>0.791250</td>
<td><a href="https://www.semanticscholar.org/paper/d7c5d39bb1c51c4c10c98429bc18b400c05e8c21">1: Distributed Representation of Words in Vector Space for Kannada Language</a></td>
</tr>
<tr>
<td>0</td>
<td>0.845217</td>
<td>0.713854</td>
<td><a href="https://www.semanticscholar.org/paper/f696e771844dc3d17293436490363a69892f2ea8">0: Learning Word Vectors with Linear Constraints: A Matrix Factorization Approach</a></td>
</tr>
<tr>
<td>0</td>
<td>0.842963</td>
<td>0.935736</td>
<td><a href="https://www.semanticscholar.org/paper/2303181e72fcc10faa91cfb0bf38fced4682a6b7">16: Comparative analysis of different word embedding models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.836480</td>
<td>0.873487</td>
<td><a href="https://www.semanticscholar.org/paper/46ccf2c7583ec0e50b1d8570e7ddd6c779221982">3: Enhanced word embedding with multiple prototypes</a></td>
</tr>
<tr>
<td>0</td>
<td>0.832331</td>
<td>0.792972</td>
<td><a href="https://www.semanticscholar.org/paper/41757db952dfb50fe972b6343e1d1e83471d7b4d">0: Effects of Positivization on the Paragraph Vector Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826957</td>
<td>0.940020</td>
<td><a href="https://www.semanticscholar.org/paper/cd31379581419f6138fe644bdec5fc88df4a1efd">61: Semantic Structure and Interpretability of Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.814889</td>
<td>0.902934</td>
<td><a href="https://www.semanticscholar.org/paper/ca49121c731fa7a56ba1dadb37f95a36b66064ab">18: Redefining Context Windows for Word Embedding Models: An Experimental Study</a></td>
</tr>
<tr>
<td>0</td>
<td>0.862419</td>
<td>0.992130</td>
<td><a href="https://www.semanticscholar.org/paper/e2dba792360873aef125572812f3673b1a85d850">6497: Enriching Word Vectors with Subword Information</a></td>
</tr>
<tr>
<td>0</td>
<td>0.646447</td>
<td>0.989302</td>
<td><a href="https://www.semanticscholar.org/paper/892e53fe5cd39f037cb2a961499f42f3002595dd">3000: Bag of Tricks for Efficient Text Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.774270</td>
<td>0.986695</td>
<td><a href="https://www.semanticscholar.org/paper/fe841ef5aa8f768c04196dbdc0ee10f3ea703574">10: Predicting and interpreting embeddings for out of vocabulary words in downstream tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.610537</td>
<td>0.985599</td>
<td><a href="https://www.semanticscholar.org/paper/8411d6041933f164eda5460cb664906c2e1e1404">0: Semantic Enhancement and Multi-level Label Embedding for Chinese News Headline Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.769013</td>
<td>0.983185</td>
<td><a href="https://www.semanticscholar.org/paper/3f1802d3f4f5f6d66875dac09112f978f12e1e1e">972: A Simple but Tough-to-Beat Baseline for Sentence Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>-1.000000</td>
<td>0.982973</td>
<td><a href="https://www.semanticscholar.org/paper/fdbb252f29ee0b72fc5467c0ae11f7cb30149f46">404: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></td>
</tr>
<tr>
<td>0</td>
<td>0.760321</td>
<td>0.982556</td>
<td><a href="https://www.semanticscholar.org/paper/59761abc736397539bdd01ad7f9d91c8607c0457">391: context2vec: Learning Generic Context Embedding with Bidirectional LSTM</a></td>
</tr>
<tr>
<td>0</td>
<td>0.635605</td>
<td>0.982245</td>
<td><a href="https://www.semanticscholar.org/paper/1f8c70ce22fc5b34ee725d79d4a061b3062f6fc5">220: Analogical Reasoning on Chinese Morphological and Semantic Relations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.735728</td>
<td>0.981180</td>
<td><a href="https://www.semanticscholar.org/paper/95b3d6b9d50bfd4b0a77a096269762b28e218346">809: Advances in Pre-Training Distributed Word Representations</a></td>
</tr>
</table></html>
