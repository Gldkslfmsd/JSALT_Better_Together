<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/93499a7c7f699b6630a86fad964536f9423bb6d0">5804: Effective Approaches to Attention-based Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.953700</td>
<td>-0.042502</td>
<td><a href="https://www.semanticscholar.org/paper/8a7f874060fe1da3b5e605d541f4b2039ecf39ab">36: Bayesian Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.876531</td>
<td>0.864443</td>
<td><a href="https://www.semanticscholar.org/paper/1d95aa40663a49ba4c58c6b1cc60ed676dc437fd">0: Neural Machine Translation with Bilingual History Involved Attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.862938</td>
<td>0.823390</td>
<td><a href="https://www.semanticscholar.org/paper/3e858cc107ef38a25837fdafcef06763dbb68c5a">0: An Effective Coverage Approach for Attention-based Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.854274</td>
<td>0.919898</td>
<td><a href="https://www.semanticscholar.org/paper/7876f448942e2658c3911c42b32ced10f85a4800">51: Multimodal Attention for Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.846084</td>
<td>0.961394</td>
<td><a href="https://www.semanticscholar.org/paper/c6c171d2a9be192d60af7b434e4ba2fcbbad7f48">42: Memory-augmented Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.844674</td>
<td>0.935688</td>
<td><a href="https://www.semanticscholar.org/paper/2f0b5f77e0851b888fde387414113d07b7e83706">8: Addressing the Under-Translation Problem from the Entropy Perspective</a></td>
</tr>
<tr>
<td>0</td>
<td>0.839012</td>
<td>0.918592</td>
<td><a href="https://www.semanticscholar.org/paper/21d3349a0fe4e6747f5241b2b87d8dccffb0bda9">1: Machine Translation Using Improved Attention-based Transformer with Hybrid Input</a></td>
</tr>
<tr>
<td>0</td>
<td>0.830967</td>
<td>0.902313</td>
<td><a href="https://www.semanticscholar.org/paper/fe090ea947cf728cdef6cb5ef00e1e4fbf94e181">25: Towards Understanding Neural Machine Translation with Word Importance</a></td>
</tr>
<tr>
<td>0</td>
<td>0.829118</td>
<td>0.892978</td>
<td><a href="https://www.semanticscholar.org/paper/12a0a79ff9b073e108ecf8741ab360631eecfa06">0: Exploring contextual information in neural machine translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809278</td>
<td>0.997612</td>
<td><a href="https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5">19041: Neural Machine Translation by Jointly Learning to Align and Translate</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793960</td>
<td>0.993028</td>
<td><a href="https://www.semanticscholar.org/paper/4550a4c714920ef57d19878e31c9ebae37b049b2">410: Massive Exploration of Neural Machine Translation Architectures</a></td>
</tr>
<tr>
<td>0</td>
<td>0.795564</td>
<td>0.987524</td>
<td><a href="https://www.semanticscholar.org/paper/68b626eb3712d5aa0ba3dd1f47d590351ce0009d">15: Improving tree-based neural machine translation with dynamic lexicalized dependency encoding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.744081</td>
<td>0.985533</td>
<td><a href="https://www.semanticscholar.org/paper/5507dc32b368c8afd3b9507e9b5888da7bd7d7cd">455: Sequence-to-Sequence Learning as Beam-Search Optimization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800705</td>
<td>0.983944</td>
<td><a href="https://www.semanticscholar.org/paper/9ac5ada0654dc1892bf8022fddf5ba528d85b04a">16: Combining Character and Word Information in Neural Machine Translation Using a Multi-Level Attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.772508</td>
<td>0.982919</td>
<td><a href="https://www.semanticscholar.org/paper/7145dae1bfa95c614b6d0ff8338a58c56b31405e">59: Bag-of-Words as Target for Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.739562</td>
<td>0.982836</td>
<td><a href="https://www.semanticscholar.org/paper/db393f34e2f25baea428629c0654929536f824b3">40: Hard Non-Monotonic Attention for Character-Level Transduction</a></td>
</tr>
<tr>
<td>0</td>
<td>0.754815</td>
<td>0.982583</td>
<td><a href="https://www.semanticscholar.org/paper/13d9323a8716131911bfda048a40e2cde1a76a46">305: Structured Attention Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.807362</td>
<td>0.982470</td>
<td><a href="https://www.semanticscholar.org/paper/f958d4921951e394057a1c4ec33bad9a34e5dad1">341: A Convolutional Encoder Model for Neural Machine Translation</a></td>
</tr>
</table></html>
