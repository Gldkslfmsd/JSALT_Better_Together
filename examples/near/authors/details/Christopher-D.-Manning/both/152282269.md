<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/1ab7f7c1d328589f25c79515b9a5d824d7ffbbd1">430: GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering</a></td>
</tr>
<tr>
<td>1</td>
<td>0.982967</td>
<td>0.989868</td>
<td><a href="https://www.semanticscholar.org/paper/c122fa378a774ba202d418cf71c5c356cf2f902f">86: GQA: a new dataset for compositional question answering over real-world images</a></td>
</tr>
<tr>
<td>0</td>
<td>0.819459</td>
<td>0.711139</td>
<td><a href="https://www.semanticscholar.org/paper/1bf4b8e867f929de1ec53ba7cf0f9d9ede58b11a">1: Seeing is Knowing! Fact-based Visual Question Answering using Knowledge Graph Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.812433</td>
<td>0.747360</td>
<td><a href="https://www.semanticscholar.org/paper/c21a4d70d83e0f6eb2a9e1c41d034842dd561e47">369: CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811761</td>
<td>0.971554</td>
<td><a href="https://www.semanticscholar.org/paper/1d660fdc8e7b23b5fa877a735744d1323a196cdb">0: A Simple Baseline for Visual Commonsense Reasoning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809124</td>
<td>-0.016148</td>
<td>NA:204904351</td>
</tr>
<tr>
<td>0</td>
<td>0.808352</td>
<td>0.633084</td>
<td><a href="https://www.semanticscholar.org/paper/95280565aa3d120c6d7e8d87ea3423f16977f19a">218: Variational Reasoning for Question Answering with Knowledge Graph</a></td>
</tr>
<tr>
<td>0</td>
<td>0.807260</td>
<td>0.959363</td>
<td><a href="https://www.semanticscholar.org/paper/d9344534ab39544a3a3c173b27628e0d9c5d4dc5">53: Answer Them All! Toward Universal Visual Question Answering Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.806093</td>
<td>0.918031</td>
<td><a href="https://www.semanticscholar.org/paper/4bce0e394c2bfdcbfbe5910a7740a653af3284d6">14: AQuA: ASP-Based Visual Question Answering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.805284</td>
<td>-0.016148</td>
<td>NA:229348766</td>
</tr>
<tr>
<td>0</td>
<td>0.760219</td>
<td>0.993157</td>
<td><a href="https://www.semanticscholar.org/paper/95cb3e6b628a3a0220b8c80fde4f9f4a3d2e7221">24: Roses are Red, Violets are Blueâ€¦ But Should VQA expect Them To?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.745145</td>
<td>0.992889</td>
<td><a href="https://www.semanticscholar.org/paper/af1f7739283bdbd2b7a94903041f6d6afd991907">174: Towards VQA Models That Can Read</a></td>
</tr>
<tr>
<td>0</td>
<td>0.736534</td>
<td>0.990814</td>
<td><a href="https://www.semanticscholar.org/paper/320464aa0231bc728c7d9ab7e71e552c12a7486b">31: Meta Module Network for Compositional Visual Reasoning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.680509</td>
<td>0.990640</td>
<td><a href="https://www.semanticscholar.org/paper/36c3972569a6949ecca90bfa6f8e99883e092845">137: Pythia v0.1: the Winning Entry to the VQA Challenge 2018</a></td>
</tr>
<tr>
<td>0</td>
<td>0.739376</td>
<td>0.989989</td>
<td><a href="https://www.semanticscholar.org/paper/8a1744da011375d711ed75fc2d160c6fdca2cf89">314: Deep Modular Co-Attention Networks for Visual Question Answering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763623</td>
<td>0.989276</td>
<td><a href="https://www.semanticscholar.org/paper/3e92f6ad7b1a0d59d5ffa30fc6e617e885f7be1a">103: In Defense of Grid Features for Visual Question Answering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.788674</td>
<td>0.988725</td>
<td><a href="https://www.semanticscholar.org/paper/a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c">1132: Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811770</td>
<td>0.988668</td>
<td><a href="https://www.semanticscholar.org/paper/28ad018c39d1578bea84e7cedf94459e3dbe1e70">130: OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge</a></td>
</tr>
</table></html>
