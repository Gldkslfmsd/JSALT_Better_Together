<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/95a251513853c6032bdecebd4b74e15795662986">736: What Does BERT Look at? An Analysis of BERTâ€™s Attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.812702</td>
<td>0.991415</td>
<td><a href="https://www.semanticscholar.org/paper/b2fd96a52ded7a64f60c1e54f5bb488c787629c0">49: What Happens To BERT Embeddings During Fine-tuning?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.774041</td>
<td>0.977996</td>
<td>NA:232306816</td>
</tr>
<tr>
<td>0</td>
<td>0.769299</td>
<td>-0.000142</td>
<td><a href="https://www.semanticscholar.org/paper/abc1ef407ead6d3e21d2f8d7163ecad56f3983b5">0: Correlation Between Attention Heads of BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.768022</td>
<td>0.678251</td>
<td><a href="https://www.semanticscholar.org/paper/a3238aa7503d8bc3ce01e285bca7bce13b22f5d2">1: Recognizing Textual Entailment with Attentive Reading and Writing Operations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763467</td>
<td>0.779811</td>
<td><a href="https://www.semanticscholar.org/paper/0c47b149f1e0cce163e3bb5eb4135cfec5f6d16b">1: The Limitations of Limited Context for Constituency Parsing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.748944</td>
<td>0.971707</td>
<td><a href="https://www.semanticscholar.org/paper/065715bf35d1dba5bf06f59f7e1e8390b93e6adf">1: Evaluation of contextual embeddings on less-resourced languages</a></td>
</tr>
<tr>
<td>0</td>
<td>0.740320</td>
<td>0.026276</td>
<td><a href="https://www.semanticscholar.org/paper/49042cf43bb93c136ccffbf31593296491099327">11: Discovering linguistic structures in speech: models and applications</a></td>
</tr>
<tr>
<td>0</td>
<td>0.739125</td>
<td>0.951860</td>
<td><a href="https://www.semanticscholar.org/paper/7ec8526c6b8831192863296d6d320f4a50fb3ddf">0: UoR at SemEval-2021 Task 4: Using Pre-trained BERT Token Embeddings for Question Answering of Abstract Meaning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.737407</td>
<td>0.958638</td>
<td><a href="https://www.semanticscholar.org/paper/0886f2ea14ccc72c9c76372dc690d105a2c617db">0: Evaluation Benchmarks for Spanish Sentence Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.619884</td>
<td>0.997955</td>
<td><a href="https://www.semanticscholar.org/paper/0de0a44b859a3719d11834479112314b4caba669">230: A Multiscale Visualization of Attention in the Transformer Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.738870</td>
<td>0.997847</td>
<td><a href="https://www.semanticscholar.org/paper/97906df07855b029b7aae7c2a1c6c5e8df1d531c">653: BERT Rediscovers the Classical NLP Pipeline</a></td>
</tr>
<tr>
<td>0</td>
<td>0.725621</td>
<td>0.997133</td>
<td><a href="https://www.semanticscholar.org/paper/e0c6abdbdecf04ffac65c440da77fb9d66bb474c">4103: XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.674373</td>
<td>0.997071</td>
<td><a href="https://www.semanticscholar.org/paper/efeab0dcdb4c1cce5e537e57745d84774be99b9a">311: Assessing BERT's Syntactic Abilities</a></td>
</tr>
<tr>
<td>0</td>
<td>0.674584</td>
<td>0.996822</td>
<td><a href="https://www.semanticscholar.org/paper/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2">2153: Transformers: State-of-the-Art Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.794963</td>
<td>0.996817</td>
<td><a href="https://www.semanticscholar.org/paper/335613303ebc5eac98de757ed02a56377d99e03a">544: What Does BERT Learn about the Structure of Language?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.756366</td>
<td>0.996808</td>
<td><a href="https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035">3381: Improving Language Understanding by Generative Pre-Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.687426</td>
<td>0.996796</td>
<td><a href="https://www.semanticscholar.org/paper/1fa9ed2bea208511ae698a967875e943049f16b6">2630: HuggingFace's Transformers: State-of-the-art Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.612661</td>
<td>0.996730</td>
<td><a href="https://www.semanticscholar.org/paper/031e4e43aaffd7a479738dcea69a2d5be7957aa3">381: ERNIE: Enhanced Representation through Knowledge Integration</a></td>
</tr>
</table></html>
