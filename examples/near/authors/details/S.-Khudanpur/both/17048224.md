<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/9819b600a828a57e1cde047bbe710d3446b30da5">4844: Recurrent neural network based language model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.954182</td>
<td>0.818182</td>
<td><a href="https://www.semanticscholar.org/paper/9087c0f2787314d0e5015d32579d8ee30bde000c">0: Language models using feedforward neural networks with fixed length context</a></td>
</tr>
<tr>
<td>0</td>
<td>0.894666</td>
<td>0.974573</td>
<td><a href="https://www.semanticscholar.org/paper/b4fc91e543ec868658cde6170f1e59c33292e595">146: Recurrent Neural Network Based Language Modeling in Meeting Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.878239</td>
<td>0.910829</td>
<td><a href="https://www.semanticscholar.org/paper/01857841165d9d56f92135be56b60b6de0427966">3: An improved recurrent neural network language model with context vector features</a></td>
</tr>
<tr>
<td>0</td>
<td>0.877165</td>
<td>0.884324</td>
<td><a href="https://www.semanticscholar.org/paper/58e3e38b483c3446e9b88c36f89fe773b6794fcb">2: RECURRENT NEURAL NETWORK LANGUAGE MODEL WITH VECTOR-SPACE WORD REPRESENTATIONS</a></td>
</tr>
<tr>
<td>0</td>
<td>0.872757</td>
<td>0.953680</td>
<td><a href="https://www.semanticscholar.org/paper/7fe37b79f80e8937ecba653b57ebc989a56b29f9">35: Investigating Bidirectional Recurrent Neural Network Language Models for Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.836423</td>
<td>0.751630</td>
<td><a href="https://www.semanticscholar.org/paper/e32b767e156fe038711409714368c2ab1e658e4e">3: Learning Recurrent Neural Network Language Models With Context-Sensitive Label Smoothing for Automatic Speech Recognition</a></td>
</tr>
<tr>
<td>1</td>
<td>0.834959</td>
<td>0.977237</td>
<td><a href="https://www.semanticscholar.org/paper/4d1d7562e077e593f985c6a9103a414e0deb5b4c">282: Recurrent neural networks for language understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.834168</td>
<td>0.927031</td>
<td><a href="https://www.semanticscholar.org/paper/e1bb8d3413ad688bd6dfefa0a2a0c1db098cfa80">12: Improvements to N-gram Language Model Using Text Generated from Neural Language Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.832337</td>
<td>0.842184</td>
<td><a href="https://www.semanticscholar.org/paper/c9486428ecfc30ba5df2770575b24d12003680f6">10: Prosodically-enhanced recurrent neural network language models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.874077</td>
<td>0.989287</td>
<td><a href="https://www.semanticscholar.org/paper/f9a1b3850dfd837793743565a8af95973d395a4e">1464: LSTM Neural Networks for Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.658694</td>
<td>0.989252</td>
<td><a href="https://www.semanticscholar.org/paper/6067628004373e61b962bd4b470308882e57448b">171: Contextual LSTM (CLSTM) models for Large scale NLP tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.828560</td>
<td>0.988772</td>
<td><a href="https://www.semanticscholar.org/paper/07ca885cb5cc4328895bfaec9ab752d5801b14cd">1417: Extensions of recurrent neural network language model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.853498</td>
<td>0.987089</td>
<td><a href="https://www.semanticscholar.org/paper/d1275b2a2ab53013310e759e5c6878b96df643d4">547: Context dependent recurrent neural network language model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.713268</td>
<td>0.981063</td>
<td><a href="https://www.semanticscholar.org/paper/e97ef4f79078c62643a772cd28d4736c193aa04b">87: Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.834164</td>
<td>0.978983</td>
<td><a href="https://www.semanticscholar.org/paper/dffe530167186edf2a8713f286732adc03907c17">51: Bidirectional recurrent neural network language models for automatic speech recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.784592</td>
<td>0.978743</td>
<td><a href="https://www.semanticscholar.org/paper/b8af97be6639d3f0b1da300896dedd94c1434550">17: On training bi-directional neural network language model with noise contrastive estimation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.834257</td>
<td>0.977556</td>
<td><a href="https://www.semanticscholar.org/paper/96364af2d208ea75ca3aeb71892d2f7ce7326b55">566: Statistical Language Models Based on Neural Networks</a></td>
</tr>
</table></html>
