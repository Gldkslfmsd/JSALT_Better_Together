<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/aa7bfd2304201afbb19971ebde87b17e40242e91">3504: On the importance of initialization and momentum in deep learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.828487</td>
<td>0.709074</td>
<td><a href="https://www.semanticscholar.org/paper/32954b976c86a8e35e9edf24b95a023bdbb89a76">7: Performance comparison of different momentum techniques on deep reinforcement learning*</a></td>
</tr>
<tr>
<td>0</td>
<td>0.824670</td>
<td>0.860669</td>
<td><a href="https://www.semanticscholar.org/paper/4ae8685db5a0cad3671a01523585dda01a356fce">1: Revisiting Initialization of Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.824468</td>
<td>0.908105</td>
<td><a href="https://www.semanticscholar.org/paper/419fe90cf606e1cd86820527a0c3b58c980389fe">70: Accelerating Deep Neural Network Training with Inconsistent Stochastic Gradient Descent</a></td>
</tr>
<tr>
<td>0</td>
<td>0.814275</td>
<td>0.748507</td>
<td><a href="https://www.semanticscholar.org/paper/bceb733df999726cec7b5bc8e7c6e4e27e2c3e65">3: Derivative-Free Optimization of Neural Networks using Local Search</a></td>
</tr>
<tr>
<td>0</td>
<td>0.803310</td>
<td>0.787176</td>
<td><a href="https://www.semanticscholar.org/paper/edccc38cbd8765c658b3880facec76e9f4a8ee5c">52: DiracNets: Training Very Deep Neural Networks Without Skip-Connections</a></td>
</tr>
<tr>
<td>0</td>
<td>0.801494</td>
<td>0.531258</td>
<td><a href="https://www.semanticscholar.org/paper/1d17baa369d635c817875dbe32b5e5264926a15f">0: WARDS BETTER OPTIMIZATION</a></td>
</tr>
<tr>
<td>0</td>
<td>0.799448</td>
<td>0.765590</td>
<td><a href="https://www.semanticscholar.org/paper/d205595e3d1fa4342f29c9517f3b56fffe785d06">57: Generalization in Deep Networks: The Role of Distance from Initialization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.795704</td>
<td>0.805914</td>
<td><a href="https://www.semanticscholar.org/paper/a26e00960bcdb0f0f775d351ed6872d479ce2634">1: Long-term temporal averaging for stochastic optimization of deep neural networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.791172</td>
<td>0.668090</td>
<td><a href="https://www.semanticscholar.org/paper/26744bcd78eef332a3f588b72f73c9cf92691aad">65: Improving performance of recurrent neural network with relu nonlinearity</a></td>
</tr>
<tr>
<td>0</td>
<td>0.482298</td>
<td>0.973655</td>
<td><a href="https://www.semanticscholar.org/paper/5df0a0e9ceec70a9321b0555288222bf53216342">274: Neural Networks in Machine Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.686987</td>
<td>0.971547</td>
<td><a href="https://www.semanticscholar.org/paper/735d4220d5579cc6afe956d9f6ea501a96ae99e2">1500: On the momentum term in gradient descent learning algorithms</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741007</td>
<td>0.970047</td>
<td><a href="https://www.semanticscholar.org/paper/81607da4b18bd7ee838afc1ab9894e3c1d836ccc">96: On weight initialization in deep neural networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.775222</td>
<td>0.970028</td>
<td><a href="https://www.semanticscholar.org/paper/9dae98ccf0c885bf687b9b71f8ff145648a85c63">38: Layer-Specific Adaptive Learning Rates for Deep Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.769166</td>
<td>0.969388</td>
<td><a href="https://www.semanticscholar.org/paper/03cf148638e007ddb42ac49f91225712b6c66a08">395: Revisiting Small Batch Training for Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.754824</td>
<td>0.965264</td>
<td><a href="https://www.semanticscholar.org/paper/99c970348b8f70ce23d6641e201904ea49266b6e">1258: Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.720176</td>
<td>0.959853</td>
<td><a href="https://www.semanticscholar.org/paper/568374ac9433e29b812008b2a01f81e657bdbd34">196: Noisy Activation Functions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.716127</td>
<td>0.959746</td>
<td><a href="https://www.semanticscholar.org/paper/941745ff77fa2cce8c1ed9692b4559492cd5d3ce">69: Survey of Dropout Methods for Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.812292</td>
<td>0.958916</td>
<td><a href="https://www.semanticscholar.org/paper/ecd29385eb214d75fc4b310489ab11977a5d1181">63: Random Walk Initialization for Training Very Deep Feedforward Networks</a></td>
</tr>
</table></html>
