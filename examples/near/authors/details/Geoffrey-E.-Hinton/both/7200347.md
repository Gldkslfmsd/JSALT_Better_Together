<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/0c908739fbff75f03469d13d4a1a07de3414ee19">8429: Distilling the Knowledge in a Neural Network</a></td>
</tr>
<tr>
<td>0</td>
<td>0.796923</td>
<td>0.360163</td>
<td><a href="https://www.semanticscholar.org/paper/2d0decf3151c68f9037aed9bf4a89f3be5beb84c">2: Drop : A Simple Way to Prevent Neural Network by Overfitting</a></td>
</tr>
<tr>
<td>0</td>
<td>0.796404</td>
<td>0.378703</td>
<td><a href="https://www.semanticscholar.org/paper/4645a414ee8fcacdf59239d6e7f57df7cf0afb0d">3: An Analysis of Machine Learning Intelligence</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792803</td>
<td>0.008783</td>
<td><a href="https://www.semanticscholar.org/paper/aeaafd20f12faf27e21ea4aa9453e11b0b63b649">0: Characterization of Internal Learning Parameters in Artificial Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.790130</td>
<td>0.303365</td>
<td><a href="https://www.semanticscholar.org/paper/0c01928c685098d4be96c18c6243a0e8c6f66074">23: Mixture density networks for distribution and uncertainty estimation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.787701</td>
<td>0.158868</td>
<td><a href="https://www.semanticscholar.org/paper/16e77ec987bf29a11d2acf80f307a5ba94d7c807">3: Map and Relabel: Towards Almost-Zero Resource Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.786397</td>
<td>0.688966</td>
<td><a href="https://www.semanticscholar.org/paper/7d7284985e1a2b1380121a1bbabe8b5050bedabf">0: Learning to Learn Image Classifiers with Informative Visual Analogy</a></td>
</tr>
<tr>
<td>0</td>
<td>0.775577</td>
<td>0.033796</td>
<td><a href="https://www.semanticscholar.org/paper/64998dbc7ce4c6c276d2cc088ae166c5b98c9826">0: Combining Classifiers and Learning Mixture-of-Experts</a></td>
</tr>
<tr>
<td>0</td>
<td>0.770974</td>
<td>0.033796</td>
<td><a href="https://www.semanticscholar.org/paper/cda82a499e525614b0e640ecb9b18b52dbf2cef7">0: Lecture 5 : Distributed Representations</a></td>
</tr>
<tr>
<td>1</td>
<td>0.766573</td>
<td>0.974295</td>
<td><a href="https://www.semanticscholar.org/paper/f8de25118af2abc4c48afb947d6ec298e05ef1e5">611: When Does Label Smoothing Help?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.784562</td>
<td>0.986926</td>
<td><a href="https://www.semanticscholar.org/paper/2444be7584d1f5a7e2aa9f65078de09154f14ea1">500: Born Again Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.795033</td>
<td>0.980819</td>
<td><a href="https://www.semanticscholar.org/paper/cc59b4b1eb7d4629f753bc24f029c5cced301381">239: Large scale distributed neural network training through online distillation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.781831</td>
<td>0.978801</td>
<td><a href="https://www.semanticscholar.org/paper/1728cb805a9573b59330890ba9723e73d6c3c974">256: Knowledge Distillation: A Survey</a></td>
</tr>
<tr>
<td>0</td>
<td>0.712233</td>
<td>0.976872</td>
<td><a href="https://www.semanticscholar.org/paper/30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9">1415: Model compression</a></td>
</tr>
<tr>
<td>0</td>
<td>0.696685</td>
<td>0.976220</td>
<td><a href="https://www.semanticscholar.org/paper/99716c3a0bcd2f587e3605f09888dcdcd3b4076e">51: Understanding and Improving Knowledge Distillation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.697547</td>
<td>0.975929</td>
<td><a href="https://www.semanticscholar.org/paper/2bdfc6d8f6d03b38b80b8aa4112088323b6b552f">27: Self-Distillation as Instance-Specific Label Smoothing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.738567</td>
<td>0.974454</td>
<td><a href="https://www.semanticscholar.org/paper/247d6c9cb92d4afabee20b0ea29ac3d8ea92f120">106: Towards Understanding Knowledge Distillation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.753816</td>
<td>0.972820</td>
<td><a href="https://www.semanticscholar.org/paper/037937410e07be72e1a2f4cc56726afc564f1389">118: Improved Knowledge Distillation via Teacher Assistant: Bridging the Gap Between Student and Teacher</a></td>
</tr>
</table></html>
<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/0c908739fbff75f03469d13d4a1a07de3414ee19">8429: Distilling the Knowledge in a Neural Network</a></td>
</tr>
<tr>
<td>0</td>
<td>0.796923</td>
<td>0.360163</td>
<td><a href="https://www.semanticscholar.org/paper/2d0decf3151c68f9037aed9bf4a89f3be5beb84c">2: Drop : A Simple Way to Prevent Neural Network by Overfitting</a></td>
</tr>
<tr>
<td>0</td>
<td>0.796404</td>
<td>0.378703</td>
<td><a href="https://www.semanticscholar.org/paper/4645a414ee8fcacdf59239d6e7f57df7cf0afb0d">3: An Analysis of Machine Learning Intelligence</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792803</td>
<td>0.008783</td>
<td><a href="https://www.semanticscholar.org/paper/aeaafd20f12faf27e21ea4aa9453e11b0b63b649">0: Characterization of Internal Learning Parameters in Artificial Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.790130</td>
<td>0.303365</td>
<td><a href="https://www.semanticscholar.org/paper/0c01928c685098d4be96c18c6243a0e8c6f66074">23: Mixture density networks for distribution and uncertainty estimation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.787701</td>
<td>0.158868</td>
<td><a href="https://www.semanticscholar.org/paper/16e77ec987bf29a11d2acf80f307a5ba94d7c807">3: Map and Relabel: Towards Almost-Zero Resource Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.786397</td>
<td>0.688966</td>
<td><a href="https://www.semanticscholar.org/paper/7d7284985e1a2b1380121a1bbabe8b5050bedabf">0: Learning to Learn Image Classifiers with Informative Visual Analogy</a></td>
</tr>
<tr>
<td>0</td>
<td>0.775577</td>
<td>0.033796</td>
<td><a href="https://www.semanticscholar.org/paper/64998dbc7ce4c6c276d2cc088ae166c5b98c9826">0: Combining Classifiers and Learning Mixture-of-Experts</a></td>
</tr>
<tr>
<td>0</td>
<td>0.770974</td>
<td>0.033796</td>
<td><a href="https://www.semanticscholar.org/paper/cda82a499e525614b0e640ecb9b18b52dbf2cef7">0: Lecture 5 : Distributed Representations</a></td>
</tr>
<tr>
<td>1</td>
<td>0.766573</td>
<td>0.974295</td>
<td><a href="https://www.semanticscholar.org/paper/f8de25118af2abc4c48afb947d6ec298e05ef1e5">611: When Does Label Smoothing Help?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.784562</td>
<td>0.986926</td>
<td><a href="https://www.semanticscholar.org/paper/2444be7584d1f5a7e2aa9f65078de09154f14ea1">500: Born Again Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.795033</td>
<td>0.980819</td>
<td><a href="https://www.semanticscholar.org/paper/cc59b4b1eb7d4629f753bc24f029c5cced301381">239: Large scale distributed neural network training through online distillation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.781831</td>
<td>0.978801</td>
<td><a href="https://www.semanticscholar.org/paper/1728cb805a9573b59330890ba9723e73d6c3c974">256: Knowledge Distillation: A Survey</a></td>
</tr>
<tr>
<td>0</td>
<td>0.712233</td>
<td>0.976872</td>
<td><a href="https://www.semanticscholar.org/paper/30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9">1415: Model compression</a></td>
</tr>
<tr>
<td>0</td>
<td>0.696685</td>
<td>0.976220</td>
<td><a href="https://www.semanticscholar.org/paper/99716c3a0bcd2f587e3605f09888dcdcd3b4076e">51: Understanding and Improving Knowledge Distillation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.697547</td>
<td>0.975929</td>
<td><a href="https://www.semanticscholar.org/paper/2bdfc6d8f6d03b38b80b8aa4112088323b6b552f">27: Self-Distillation as Instance-Specific Label Smoothing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.738567</td>
<td>0.974454</td>
<td><a href="https://www.semanticscholar.org/paper/247d6c9cb92d4afabee20b0ea29ac3d8ea92f120">106: Towards Understanding Knowledge Distillation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.753816</td>
<td>0.972820</td>
<td><a href="https://www.semanticscholar.org/paper/037937410e07be72e1a2f4cc56726afc564f1389">118: Improved Knowledge Distillation via Teacher Assistant: Bridging the Gap Between Student and Teacher</a></td>
</tr>
</table></html>
