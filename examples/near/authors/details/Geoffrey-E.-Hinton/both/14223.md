<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/47570e7f63e296f224a0e7f9a0d08b0de3cbaf40">842: Grammar as a Foreign Language</a></td>
</tr>
<tr>
<td>0</td>
<td>0.856190</td>
<td>0.783627</td>
<td><a href="https://www.semanticscholar.org/paper/e74d7523d7d96ab65f05f059284f9d0a994bb074">9: The NAIST-NTT TED talk treebank</a></td>
</tr>
<tr>
<td>0</td>
<td>0.855005</td>
<td>0.799609</td>
<td><a href="https://www.semanticscholar.org/paper/c26dae87273462a8dd7e8d5d3070385150d0f520">1: A neural parser as a direct classifier for head-final languages</a></td>
</tr>
<tr>
<td>0</td>
<td>0.848532</td>
<td>-0.031791</td>
<td>NA:195349458</td>
</tr>
<tr>
<td>0</td>
<td>0.840872</td>
<td>0.527508</td>
<td><a href="https://www.semanticscholar.org/paper/843c251a5b7655cc7cf950009533ad0293424872">1: Deep Grammars in a Tree Labeling Approach to Syntax-based Statistical Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.839362</td>
<td>0.699063</td>
<td><a href="https://www.semanticscholar.org/paper/01e53c7bf8a1f82f3ff76c8f02e0b87bebc9de94">4: Sentence Realization with Unlexicalized Tree Linearization Grammars</a></td>
</tr>
<tr>
<td>0</td>
<td>0.835853</td>
<td>0.412687</td>
<td><a href="https://www.semanticscholar.org/paper/9a4e538b05cd0f0488bfaea6a9e3cb98c6ddaeb6">2: Theoretical and methodological issues of tagging noun phrase structures following dependency grammar formalism</a></td>
</tr>
<tr>
<td>0</td>
<td>0.835382</td>
<td>0.388852</td>
<td><a href="https://www.semanticscholar.org/paper/b3640a5823698fd8d161308a6c7dc622b27042f5">16: A Statistical Dependency Parser of Chinese under Small Training Data *</a></td>
</tr>
<tr>
<td>0</td>
<td>0.832733</td>
<td>0.510605</td>
<td><a href="https://www.semanticscholar.org/paper/2466a9810b6716ed2e8377063ed8f658fc98a59f">22: Scalable Discriminative Parsing for German</a></td>
</tr>
<tr>
<td>0</td>
<td>0.830811</td>
<td>0.414512</td>
<td><a href="https://www.semanticscholar.org/paper/e53c204088216e2930e01e90b58bac83ff702d39">7: Experiments in German Treebank Parsing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.703214</td>
<td>0.991535</td>
<td><a href="https://www.semanticscholar.org/paper/e43f713e0d2d438a4c0b03eacab58c334e869e6a">68: Lattice-Based Recurrent Neural Network Encoders for Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.682178</td>
<td>0.989708</td>
<td><a href="https://www.semanticscholar.org/paper/e8e76b1062918624e9904e0073e11794d7594593">61: Document Context Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.609134</td>
<td>0.989179</td>
<td><a href="https://www.semanticscholar.org/paper/8a48edc093937a2f8ae665a4e1ecfa38972b234b">21: Attentive Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.737280</td>
<td>0.988544</td>
<td><a href="https://www.semanticscholar.org/paper/6c5325c2b67bf88f2b846cf5a6df6c2e6362d75b">51: Larger-Context Language Modelling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.683269</td>
<td>0.988436</td>
<td><a href="https://www.semanticscholar.org/paper/58eb3a2f0a67acf2f5c7c2cb4a22852b65314eb5">86: Frustratingly Short Attention Spans in Neural Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.728601</td>
<td>0.986077</td>
<td><a href="https://www.semanticscholar.org/paper/434882ed3a5a457e370a3f81a66c20d37277320f">8: ‘Indicatements’ that character language models learn English morpho-syntactic units and regularities</a></td>
</tr>
<tr>
<td>0</td>
<td>0.714971</td>
<td>0.985841</td>
<td><a href="https://www.semanticscholar.org/paper/1938624bb9b0f999536dcc8d8f519810bb4e1b3b">854: On Using Very Large Target Vocabulary for Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.697952</td>
<td>0.984577</td>
<td><a href="https://www.semanticscholar.org/paper/cc628a5d9ab29e5207e9d0c041f78df873e940e9">0: Unsupervised Training of Automatic Dialogue Systems for Customer Support</a></td>
</tr>
<tr>
<td>0</td>
<td>0.691907</td>
<td>0.982608</td>
<td><a href="https://www.semanticscholar.org/paper/0ac70d4d5db5df83ae610723a377a6904c5ada30">35: Incorporating Side Information into Recurrent Neural Network Language Models</a></td>
</tr>
</table></html>
