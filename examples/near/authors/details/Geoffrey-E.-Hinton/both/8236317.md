<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/97fb4e3d45bb098e27e0071448b6152217bd35a5">2933: Layer Normalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.829804</td>
<td>0.298250</td>
<td><a href="https://www.semanticscholar.org/paper/b7b1e626d1839e0d41d2105fc248e3777845d639">1: Improving Batch Normalization with Skewness Reduction for Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.798052</td>
<td>0.463069</td>
<td><a href="https://www.semanticscholar.org/paper/5c6106dd24c393eaf79e4dfd495b0e694bf2670e">3: Stochastic Whitening Batch Normalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.796935</td>
<td>0.582260</td>
<td><a href="https://www.semanticscholar.org/paper/1a9bf293a794642b17f117f80d0183faadcccd28">15: Is normalization indispensable for training deep neural network?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.795333</td>
<td>0.497786</td>
<td><a href="https://www.semanticscholar.org/paper/6f6c966d9d0a8af6d52e71bf12cd8df6da243c2d">0: Orthogonal Recurrent Neural Networks and Batch Normalization in Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.787249</td>
<td>0.326792</td>
<td><a href="https://www.semanticscholar.org/paper/4ac47b1b74ad6619be312c56a0cdb059bbb7b0a9">1: A Comparison of Adaptation Techniques and Recurrent Neural Network Architectures</a></td>
</tr>
<tr>
<td>0</td>
<td>0.783587</td>
<td>0.041551</td>
<td><a href="https://www.semanticscholar.org/paper/a324817401b07a9b5337e77f4054a894486ac378">4: A comparison of dropout and weight decay for regularizing deep neural networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.782485</td>
<td>0.527568</td>
<td><a href="https://www.semanticscholar.org/paper/424a6e62084d919bfc2e39a507c263e5991ebdad">1305: Self-Normalizing Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.770644</td>
<td>0.319597</td>
<td><a href="https://www.semanticscholar.org/paper/864f5e7b3c9cfec7522d6088f4a256545ed55a8d">49: Unfolded recurrent neural networks for speech recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763765</td>
<td>0.533289</td>
<td><a href="https://www.semanticscholar.org/paper/3a2e8145d8b89ba86a47c9e13d202f5465b607d0">0: Deep Learning: a new definition of artificial neuron with double weight</a></td>
</tr>
<tr>
<td>0</td>
<td>0.472441</td>
<td>0.942928</td>
<td><a href="https://www.semanticscholar.org/paper/88bb7b123fc5853644d5327aa99538b58874dd57">11: The NTT DCASE2020 Challenge Task 6 system: Automated Audio Captioning with Keywords and Sentence Length Estimation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.585452</td>
<td>0.942444</td>
<td><a href="https://www.semanticscholar.org/paper/5b8d6dedd06df0a6f0a71eb4744f0b15fb91850c">0: Transformers in Time-series Analysis: A Tutorial</a></td>
</tr>
<tr>
<td>0</td>
<td>0.649195</td>
<td>0.941620</td>
<td><a href="https://www.semanticscholar.org/paper/2fd1312b8507aae41bace1dd89712754a81fbc49">15: PonderNet: Learning to Ponder</a></td>
</tr>
<tr>
<td>0</td>
<td>0.548935</td>
<td>0.939980</td>
<td><a href="https://www.semanticscholar.org/paper/35a9749df07a2ab97c51af4d260b095b00da7676">168: Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</a></td>
</tr>
<tr>
<td>0</td>
<td>0.525261</td>
<td>0.938647</td>
<td><a href="https://www.semanticscholar.org/paper/95f3b5b09e3f0e0d2038c57b57325bfe03279fbf">3: Continual Learning for Task-oriented Dialogue System with Iterative Network Pruning, Expanding and Masking</a></td>
</tr>
<tr>
<td>0</td>
<td>0.516829</td>
<td>0.935744</td>
<td><a href="https://www.semanticscholar.org/paper/5f104a804ed245c79847ad0593e8f86196d697b1">1: Transformers in Time Series: A Survey</a></td>
</tr>
<tr>
<td>0</td>
<td>0.655947</td>
<td>0.932548</td>
<td><a href="https://www.semanticscholar.org/paper/b0fae9fbb4e580d92395eabafe73e317ae6510e3">1443: SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.665436</td>
<td>0.931885</td>
<td><a href="https://www.semanticscholar.org/paper/88caa4a0253a8b0076176745ebc072864eab66e1">1261: Language Modeling with Gated Convolutional Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.554315</td>
<td>0.928372</td>
<td><a href="https://www.semanticscholar.org/paper/59a7d14d20130dc92b6a2551cf1a0115ff9f8bd6">11: Audio Captioning using Pre-Trained Large-Scale Language Model Guided by Audio-based Similar Caption Retrieval</a></td>
</tr>
</table></html>
