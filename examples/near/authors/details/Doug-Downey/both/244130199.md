<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/9a258f42e333ed5ff79037724eb01747ede0bb49">7: Few-Shot Self-Rationalization with Natural Language Prompts</a></td>
</tr>
<tr>
<td>0</td>
<td>0.833705</td>
<td>0.015439</td>
<td><a href="https://www.semanticscholar.org/paper/d6d8bd4a59b617438e3c2226d39acf6d9ea84099">0: FLUTE: Figurative Language Understanding and Textual Explanations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.810127</td>
<td>0.936084</td>
<td><a href="https://www.semanticscholar.org/paper/41b5b8c7c0b9677b9f65283518984726fb199379">3: Extractive NarrativeQA with Heuristic Pre-Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.799016</td>
<td>0.869068</td>
<td><a href="https://www.semanticscholar.org/paper/9f3540baf1f1e5bf3dd4efdd776d6e2ffc3f55e9">0: Are Shortest Rationales the Best Explanations for Human Understanding?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.797729</td>
<td>0.923151</td>
<td><a href="https://www.semanticscholar.org/paper/342ec2f1c1b3d29d3269a2566c44f239f0141aeb">55: Logical Natural Language Generation from Open-Domain Tables</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793637</td>
<td>0.969767</td>
<td><a href="https://www.semanticscholar.org/paper/b6588d413efc23122f690ace43a28db693d857c2">2: WinoLogic: A Zero-Shot Logic-based Diagnostic Dataset for Winograd Schema Challenge</a></td>
</tr>
<tr>
<td>0</td>
<td>0.789300</td>
<td>0.975603</td>
<td><a href="https://www.semanticscholar.org/paper/58947177663d73b4d7809e74482b54aadaee6444">2: P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts</a></td>
</tr>
<tr>
<td>0</td>
<td>0.786851</td>
<td>0.971707</td>
<td><a href="https://www.semanticscholar.org/paper/a81a09b2a4ce36ae5c847fc4e3558c523d301179">0: Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.783580</td>
<td>0.977603</td>
<td><a href="https://www.semanticscholar.org/paper/498d95574dead1619a9580293968a49ba537f362">1: Do Prompts Solve NLP Tasks Using Natural Language?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.781657</td>
<td>0.893917</td>
<td><a href="https://www.semanticscholar.org/paper/5eba8724559f97b824e832451e4f832e319ce2cf">0: Estimating Subjective Crowd-Evaluations as an Additional Objective to Improve Natural Language Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.715318</td>
<td>0.994917</td>
<td><a href="https://www.semanticscholar.org/paper/59d225fcb08ce66935e0285a9936ee158c4fdb97">16: Explaining Answers with Entailment Trees</a></td>
</tr>
<tr>
<td>0</td>
<td>0.692194</td>
<td>0.994895</td>
<td><a href="https://www.semanticscholar.org/paper/5b6c582d51266be9aa7e32bfdc20891e5231eca4">20: When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data</a></td>
</tr>
<tr>
<td>0</td>
<td>0.779181</td>
<td>0.994576</td>
<td><a href="https://www.semanticscholar.org/paper/874e9318c09c711ecd48a903b3824a3a03e2cd62">199: Explain Yourself! Leveraging Language Models for Commonsense Reasoning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.774442</td>
<td>0.993913</td>
<td><a href="https://www.semanticscholar.org/paper/7747ecbc26b1688e6cad1a6ce83914efa2a3c04c">10: Prompting Contrastive Explanations for Commonsense Reasoning Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.691830</td>
<td>0.993443</td>
<td><a href="https://www.semanticscholar.org/paper/cd471b5ef162906ef3d9a84398b3f98e9ee4bf56">1: A Review on Language Models as Knowledge Bases</a></td>
</tr>
<tr>
<td>0</td>
<td>0.674910</td>
<td>0.992992</td>
<td><a href="https://www.semanticscholar.org/paper/e1943cbf4817605a1f988fe5fd785f6b707ca233">0: METGEN: A Module-Based Entailment Tree Generation Framework for Answer Explanation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.284681</td>
<td>0.992953</td>
<td><a href="https://www.semanticscholar.org/paper/052ea761dfc1ea0e5891e92d9b18401e77d5f48e">0: Abstracting Influence Paths for Explaining (Contextualization of) BERT Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.752266</td>
<td>0.992448</td>
<td><a href="https://www.semanticscholar.org/paper/6160d89955aeb79424cb8bf164e3a797035c1c67">1: Summarize-then-Answer: Generating Concise Explanations for Multi-hop Reading Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.656179</td>
<td>0.992443</td>
<td><a href="https://www.semanticscholar.org/paper/122b75042daae44f93153dedda15b0fb11b3f279">28: What Do Models Learn from Question Answering Datasets?</a></td>
</tr>
</table></html>
