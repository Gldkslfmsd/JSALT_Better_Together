<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/8fa9c9568d8de9cd3536d6f99d99fe957d45e0a1">913: Robotic Grasping of Novel Objects using Vision</a></td>
</tr>
<tr>
<td>0</td>
<td>0.885814</td>
<td>0.874434</td>
<td><a href="https://www.semanticscholar.org/paper/a8f71f1e30883e4cd56fd8b2f3c9c9553d65512a">2: Determining Grasping Regions using Vision</a></td>
</tr>
<tr>
<td>0</td>
<td>0.874724</td>
<td>0.765890</td>
<td><a href="https://www.semanticscholar.org/paper/80cf2d63d240ac66588c26b706ef2df3b08de9ba">0: Grasping with Intent to Place CS 6780 Report</a></td>
</tr>
<tr>
<td>0</td>
<td>0.857581</td>
<td>-0.003872</td>
<td>NA:9245778</td>
</tr>
<tr>
<td>0</td>
<td>0.856478</td>
<td>0.775581</td>
<td><a href="https://www.semanticscholar.org/paper/d6020fccefce643dc563676f706cd664fe37cadf">0: Learning from Successes and Failures to Grasp Objects with a Vacuum Gripper</a></td>
</tr>
<tr>
<td>0</td>
<td>0.854489</td>
<td>0.697030</td>
<td><a href="https://www.semanticscholar.org/paper/845a6fcf93527dca8f84878316f48670d640e6ea">161: Visually guided object grasping</a></td>
</tr>
<tr>
<td>0</td>
<td>0.848473</td>
<td>0.710152</td>
<td><a href="https://www.semanticscholar.org/paper/d027f6d9c703b14e65f90ba7f41529215cdfa886">14: Stereo-based 3D localization for grasping known objects with a robotic arm system</a></td>
</tr>
<tr>
<td>0</td>
<td>0.844570</td>
<td>0.497120</td>
<td><a href="https://www.semanticscholar.org/paper/a3c303eee66a58461d11ec70b02dfd089a8307a0">0: Toward active pose estimation of a grasped object</a></td>
</tr>
<tr>
<td>0</td>
<td>0.842211</td>
<td>0.929224</td>
<td><a href="https://www.semanticscholar.org/paper/cfd8ecca6a07814cfa7c447ce489f68110c0909d">39: Enabling grasping of unknown objects through a synergistic use of edge and surface information</a></td>
</tr>
<tr>
<td>0</td>
<td>0.840714</td>
<td>0.955463</td>
<td><a href="https://www.semanticscholar.org/paper/74d3366da62bac4ee1c3475b2f975970fcc29d0c">24: Grasping of unknown objects on a planar surface using a single depth image</a></td>
</tr>
<tr>
<td>0</td>
<td>0.879507</td>
<td>0.988813</td>
<td><a href="https://www.semanticscholar.org/paper/a1466825412cdec8f853edf14d95de4c5edc0510">150: Robotic Grasping of Novel Objects</a></td>
</tr>
<tr>
<td>0</td>
<td>0.874764</td>
<td>0.988725</td>
<td><a href="https://www.semanticscholar.org/paper/6ad3ccaf8429be7e22f9ff6349f5deba8e928157">159: Learning Grasp Strategies with Partial Shape Information</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826554</td>
<td>0.986600</td>
<td><a href="https://www.semanticscholar.org/paper/a13a054e4cc378317bd0a0aeeb7114e944041298">93: Grasping with application to an autonomous checkout robot</a></td>
</tr>
<tr>
<td>0</td>
<td>0.783239</td>
<td>0.984021</td>
<td><a href="https://www.semanticscholar.org/paper/c5167369aa1bdb7747e840fcefcaa063ef1f3fda">64: Grasping unknown objects using an Early Cognitive Vision system for general scene understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.766943</td>
<td>0.981600</td>
<td><a href="https://www.semanticscholar.org/paper/ae89baa469ee08fccc6202c8e4a685ed54ddbc50">293: The Columbia grasp database</a></td>
</tr>
<tr>
<td>0</td>
<td>0.815276</td>
<td>0.981204</td>
<td><a href="https://www.semanticscholar.org/paper/c8c8fbcbfd3222c27ae4137b8bcb3ff290866c98">47: Learning grasps with topographic features</a></td>
</tr>
<tr>
<td>0</td>
<td>0.781409</td>
<td>0.979409</td>
<td><a href="https://www.semanticscholar.org/paper/b2f1453e30c0849934c6b68e208c51d72bdbfdd8">123: Selection of robot pre-grasps using box-based shape approximation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.739780</td>
<td>0.979406</td>
<td><a href="https://www.semanticscholar.org/paper/7a9d5782b6e6cf00860916f58284293df94adbd1">57: Empty the basket - a shape based learning approach for grasping piles of unknown objects</a></td>
</tr>
<tr>
<td>0</td>
<td>0.733882</td>
<td>0.977120</td>
<td><a href="https://www.semanticscholar.org/paper/122d6fb21ccc7960d67af5f82de3a58ed7c83b68">66: Perceiving, learning, and exploiting object affordances for autonomous pile manipulation</a></td>
</tr>
</table></html>
