<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/8ff840a40d3f1557c55c19d4d636da77103168ce">2179: Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin</a></td>
</tr>
<tr>
<td>0</td>
<td>0.862674</td>
<td>0.016181</td>
<td><a href="https://www.semanticscholar.org/paper/f23e9f7b0d5b384dfa95dd2f252b4ed16d00e204">0: End-to-end speech recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.849040</td>
<td>0.838435</td>
<td><a href="https://www.semanticscholar.org/paper/eb82d0a93471cba5dc203df7a82dd8eaa845ff9a">15: Extending Recurrent Neural Aligner for Streaming End-to-End Speech Recognition in Mandarin</a></td>
</tr>
<tr>
<td>0</td>
<td>0.833045</td>
<td>0.839301</td>
<td><a href="https://www.semanticscholar.org/paper/04d96a75b4383240cb15fb729b29f5775219d724">39: Espresso: A Fast End-to-End Neural Speech Recognition Toolkit</a></td>
</tr>
<tr>
<td>0</td>
<td>0.827142</td>
<td>0.741248</td>
<td><a href="https://www.semanticscholar.org/paper/fc8caac5579d662d86465a9ec84fdb2334130e1a">0: Arranged end-to-end speech recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826158</td>
<td>0.816330</td>
<td><a href="https://www.semanticscholar.org/paper/ea2c03bb5e44c51c148e1a42323e1b7b008dece2">4: End-to-End Speech Recognition: A review for the French Language</a></td>
</tr>
<tr>
<td>0</td>
<td>0.822586</td>
<td>0.632399</td>
<td><a href="https://www.semanticscholar.org/paper/bc5249c2040d187e9dce01c76aa4687fb13d1ce7">1: Supervised Adaptation of Sequence-to-Sequence Speech Recognition Systems using Batch-Weighting</a></td>
</tr>
<tr>
<td>0</td>
<td>0.822412</td>
<td>0.598279</td>
<td><a href="https://www.semanticscholar.org/paper/f75e1c412311573f291252bf33ac6e1ddc18b7ef">6: Adapting End-to-End Speech Recognition for Readable Subtitles</a></td>
</tr>
<tr>
<td>0</td>
<td>0.822215</td>
<td>0.804302</td>
<td><a href="https://www.semanticscholar.org/paper/c3c58177c073d677ec97de834340815b9dd89b42">11: Towards a Competitive End-to-End Speech Recognition for CHiME-6 Dinner Party Transcription</a></td>
</tr>
<tr>
<td>0</td>
<td>0.815208</td>
<td>0.780573</td>
<td><a href="https://www.semanticscholar.org/paper/9ad7f78ff2eee90f097e4a9a9050f914ffe2b1ad">0: End-to-End Language Identification Using a Residual Convolutional Neural Network with Attentive Temporal Pooling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.717506</td>
<td>0.977060</td>
<td><a href="https://www.semanticscholar.org/paper/d5fa1008ceceed3c4398da94477ff8bacb6fce13">354: The Microsoft 2017 Conversational Speech Recognition System</a></td>
</tr>
<tr>
<td>0</td>
<td>0.877420</td>
<td>0.974770</td>
<td><a href="https://www.semanticscholar.org/paper/24741d280869ad9c60321f5ab6e5f01b7852507d">1503: Deep Speech: Scaling up end-to-end speech recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.744082</td>
<td>0.970127</td>
<td><a href="https://www.semanticscholar.org/paper/579e0077a3810510a7965224a8782ecc01766ea0">479: Achieving Human Parity in Conversational Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.751362</td>
<td>0.969990</td>
<td><a href="https://www.semanticscholar.org/paper/ac94ef90be9b0c3bf744d6744e47b38855f9a4c7">278: The microsoft 2016 conversational speech recognition system</a></td>
</tr>
<tr>
<td>0</td>
<td>0.726375</td>
<td>0.961440</td>
<td><a href="https://www.semanticscholar.org/paper/da1231a3a7536010ddb6ef5e163a785d03974af1">58: Residual Convolutional CTC Networks for Automatic Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.791738</td>
<td>0.961257</td>
<td><a href="https://www.semanticscholar.org/paper/d838994020a794b857a4cd356bfbbf7b52da7473">347: Very deep convolutional networks for end-to-end speech recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.737954</td>
<td>0.953613</td>
<td><a href="https://www.semanticscholar.org/paper/edb1e4bd20731b292e36df7f80dc5c1ad61febb6">73: Transferring knowledge from a RNN to a DNN</a></td>
</tr>
<tr>
<td>0</td>
<td>0.756569</td>
<td>0.951650</td>
<td><a href="https://www.semanticscholar.org/paper/c9bd15c7838c1d3cdd5f5113a2efd9440f86b3da">317: English Conversational Telephone Speech Recognition by Humans and Machines</a></td>
</tr>
<tr>
<td>0</td>
<td>0.757091</td>
<td>0.951463</td>
<td><a href="https://www.semanticscholar.org/paper/5a7f3f0fdbdc29fddc7a41098ee8bbc3f7cfd1a1">132: Toward Human Parity in Conversational Speech Recognition</a></td>
</tr>
</table></html>
