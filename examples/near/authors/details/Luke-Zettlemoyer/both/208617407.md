<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/f4cf4246f3882aa6337e9c05d5675a3b8463a32e">181: ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.779100</td>
<td>0.952612</td>
<td><a href="https://www.semanticscholar.org/paper/e23f1585ef7940c2022ecf81767f01a0b6180ee6">18: Multi-modal Discriminative Model for Vision-and-Language Navigation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.765879</td>
<td>0.820707</td>
<td><a href="https://www.semanticscholar.org/paper/7a15950dc71079285a4eaf195de5aadd87c41b40">105: Fine-Tuning Language Models from Human Preferences</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763490</td>
<td>0.955772</td>
<td><a href="https://www.semanticscholar.org/paper/df51fb60e8587520ea504d8a15686515101899c7">4: CALVIN: A Benchmark for Language-conditioned Policy Learning for Long-horizon Robot Manipulation Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741811</td>
<td>0.624521</td>
<td><a href="https://www.semanticscholar.org/paper/2ae7bb3b5ded27f58ee04e46305f6d855c16570a">0: Learning from Observation-Only Demonstration for Task-Oriented Language Grounding via Self-Examination</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741495</td>
<td>0.979748</td>
<td><a href="https://www.semanticscholar.org/paper/9727b7b913610e7614fdfe3f996396925ab1fc36">2: Visual-and-Language Navigation: A Survey and Taxonomy</a></td>
</tr>
<tr>
<td>0</td>
<td>0.734012</td>
<td>0.920355</td>
<td><a href="https://www.semanticscholar.org/paper/2a273479d5ede7a27cf144a18cfbf0542b92fa12">3: Cross-Lingual Vision-Language Navigation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.728503</td>
<td>0.162618</td>
<td><a href="https://www.semanticscholar.org/paper/3c21f21e4fa0f677db0497348372c415025a78ce">15: Grounding Words in Perception and Action : Insights from Computational Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.726568</td>
<td>0.781447</td>
<td><a href="https://www.semanticscholar.org/paper/fe16c9d223dc09dc8e027d1b441f2845c00363a6">0: On Advances in Text Generation from Images Beyond Captioning: A Case Study in Self-Rationalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.725301</td>
<td>0.346433</td>
<td><a href="https://www.semanticscholar.org/paper/7e8ddbd037f4a770193affe84eb193af0c1d86ab">0: A bilingual cognitive robot that learns like a toddler</a></td>
</tr>
<tr>
<td>0</td>
<td>0.746636</td>
<td>0.993495</td>
<td><a href="https://www.semanticscholar.org/paper/f41e6c832c9e0d5360b66ee7681d3b1ffd2d9c3d">15: Hierarchical Task Learning from Language Instructions with Unified Transformers and Self-Monitoring</a></td>
</tr>
<tr>
<td>0</td>
<td>0.816757</td>
<td>0.992631</td>
<td><a href="https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb">33: ALFWorld: Aligning Text and Embodied Environments for Interactive Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.736650</td>
<td>0.992366</td>
<td><a href="https://www.semanticscholar.org/paper/79bc6e1fe465aec49d7f0252f295c0ad9cdaf389">66: Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.798879</td>
<td>0.986589</td>
<td><a href="https://www.semanticscholar.org/paper/6bd9642470ff8c2089427f7a6392cd17d213a334">544: Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments</a></td>
</tr>
<tr>
<td>0</td>
<td>0.734594</td>
<td>0.986073</td>
<td><a href="https://www.semanticscholar.org/paper/077dbc662622cb3419415320d06de0602c50a8d0">33: BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps</a></td>
</tr>
<tr>
<td>0</td>
<td>0.702846</td>
<td>0.984462</td>
<td><a href="https://www.semanticscholar.org/paper/8351f660e67b6e792a4791b7a9ce27e6b1720236">9: Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.716091</td>
<td>0.983118</td>
<td><a href="https://www.semanticscholar.org/paper/0f01088765729402e903ec560f3246f884d324f8">0: Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.695333</td>
<td>0.982645</td>
<td><a href="https://www.semanticscholar.org/paper/c66b8e508718f4b7f14829e5c2cde0add31d2693">250: Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.698322</td>
<td>0.982046</td>
<td><a href="https://www.semanticscholar.org/paper/b5cc6634724b2238c88bcc324ec01a2c91c1b909">190: TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments</a></td>
</tr>
</table></html>
