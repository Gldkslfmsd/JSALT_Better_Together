<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/9f1c5777a193b2c3bb2b25e248a156348e5ba56d">151: Cloze-driven Pretraining of Self-attention Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.812059</td>
<td>0.900810</td>
<td><a href="https://www.semanticscholar.org/paper/7cb8b70ab4a45b0d52e17d243cb880158bc4b528">30: Self-Attention Architectures for Answer-Agnostic Neural Question Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.786977</td>
<td>-0.011499</td>
<td><a href="https://www.semanticscholar.org/paper/dbf7c043e20c9b48d8e563b34a0a7db468cd5a0d">20: A recurrent network that learns to pronounce English text</a></td>
</tr>
<tr>
<td>0</td>
<td>0.769923</td>
<td>0.903505</td>
<td><a href="https://www.semanticscholar.org/paper/d7baaa250fbe7a9f5da4cafa8d0ba4e5f1b903a7">13: Cut to the Chase: A Context Zoom-in Network for Reading Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.769517</td>
<td>0.791476</td>
<td><a href="https://www.semanticscholar.org/paper/b7231a746850d9cb98b7762ba151a456404a54e0">3: A Survey of Vision-Language Pre-Trained Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.764815</td>
<td>0.918779</td>
<td><a href="https://www.semanticscholar.org/paper/be60cdc4f3202f44afdfb90bff005e3005dacf9a">45: Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763275</td>
<td>0.926009</td>
<td><a href="https://www.semanticscholar.org/paper/aa68ea557777908e76f02c433f14ef6b968d4a82">35: Paraphrasing with Large Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763129</td>
<td>0.911377</td>
<td><a href="https://www.semanticscholar.org/paper/2095023dadad18707eb2a96f465ced8e86094235">0: Meta-learning for downstream aware and agnostic pretraining</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763042</td>
<td>0.899966</td>
<td><a href="https://www.semanticscholar.org/paper/a06b24d3ab230a0d4154a9b08ede828b03a0d5c9">13: Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies</a></td>
</tr>
<tr>
<td>0</td>
<td>0.761026</td>
<td>0.812730</td>
<td><a href="https://www.semanticscholar.org/paper/464b47a6a395fa1338e230254965cf5f669e715c">1: Moses and the Character-Based Random Babbling Baseline: CoAStaL at AmericasNLP 2021 Shared Task</a></td>
</tr>
<tr>
<td>0</td>
<td>0.708469</td>
<td>0.997667</td>
<td><a href="https://www.semanticscholar.org/paper/473921de1b52f98f34f37afd507e57366ff7d1ca">50: CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters</a></td>
</tr>
<tr>
<td>0</td>
<td>0.733097</td>
<td>0.996484</td>
<td><a href="https://www.semanticscholar.org/paper/3f9df96b26c42dea6dd6cad64557a3b7d698ea90">58: MultiFiT: Efficient Multi-lingual Language Model Fine-tuning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.749201</td>
<td>0.995707</td>
<td><a href="https://www.semanticscholar.org/paper/8659bf379ca8756755125a487c43cfe8611ce842">261: To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.771407</td>
<td>0.995562</td>
<td><a href="https://www.semanticscholar.org/paper/ac11062f1f368d97f4c826c317bf50dcc13fdb59">257: Dissecting Contextual Word Embeddings: Architecture and Representation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.745019</td>
<td>0.995375</td>
<td><a href="https://www.semanticscholar.org/paper/f6fbb6809374ca57205bd2cf1421d4f4fa04f975">429: Linguistic Knowledge and Transferability of Contextual Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.587593</td>
<td>0.994635</td>
<td><a href="https://www.semanticscholar.org/paper/e6fc6a23c057ce90c950adff480afceb07979386">18: Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.649555</td>
<td>0.994365</td>
<td><a href="https://www.semanticscholar.org/paper/93b4cc549a1bc4bc112189da36c318193d05d806">880: AllenNLP: A Deep Semantic Natural Language Processing Platform</a></td>
</tr>
<tr>
<td>0</td>
<td>0.625689</td>
<td>0.994049</td>
<td><a href="https://www.semanticscholar.org/paper/ce498651107588db67adcfbb5479bdb416f4de2f">42: Template-Based Named Entity Recognition Using BART</a></td>
</tr>
<tr>
<td>0</td>
<td>0.707492</td>
<td>0.993608</td>
<td><a href="https://www.semanticscholar.org/paper/c89a572ae5a1ecbf903bb57e300ffc6a63dde53f">1: SkoltechNLP at SemEval-2021 Task 5: Leveraging Sentence-level Pre-training for Toxic Span Detection</a></td>
</tr>
</table></html>
