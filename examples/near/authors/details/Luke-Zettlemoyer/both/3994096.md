<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/93b4cc549a1bc4bc112189da36c318193d05d806">880: AllenNLP: A Deep Semantic Natural Language Processing Platform</a></td>
</tr>
<tr>
<td>0</td>
<td>0.985767</td>
<td>0.920003</td>
<td><a href="https://www.semanticscholar.org/paper/a5502187140cdd98d76ae711973dbcdaf1fef46d">16: A Deep Semantic Natural Language Processing Platform</a></td>
</tr>
<tr>
<td>0</td>
<td>0.819304</td>
<td>0.959917</td>
<td><a href="https://www.semanticscholar.org/paper/c6dc2f21e943c5a1dd35fb3d6ff18525c9c86ca0">17: Infusing Finetuning with Semantic Dependencies</a></td>
</tr>
<tr>
<td>0</td>
<td>0.790888</td>
<td>0.006801</td>
<td><a href="https://www.semanticscholar.org/paper/65cb5c1aead601e9f1cf686ce051c211d4fec9bc">0: Leveraging Syntactic Structures in Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.781843</td>
<td>0.721541</td>
<td><a href="https://www.semanticscholar.org/paper/24433d65fb2b3df2bfc500861fa801ba04622028">2: Improving Semantic Parsing with Neural Generator-Reranker Architecture</a></td>
</tr>
<tr>
<td>0</td>
<td>0.780497</td>
<td>0.006801</td>
<td><a href="https://www.semanticscholar.org/paper/6a15575f9117967303924922e8cf26fbbb649d80">0: Dive into Deep Learning for Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.777583</td>
<td>0.967342</td>
<td><a href="https://www.semanticscholar.org/paper/a779b9b0fc99f51a0ab4b765e7f775133c81bcba">4: AttViz: Online exploration of self-attention for transparent neural language modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.773225</td>
<td>0.978660</td>
<td><a href="https://www.semanticscholar.org/paper/a5869e97109e79f216746f55f222c5cd649bef32">2: ExBERT: An External Knowledge Enhanced BERT for Natural Language Inference</a></td>
</tr>
<tr>
<td>0</td>
<td>0.772431</td>
<td>0.945811</td>
<td><a href="https://www.semanticscholar.org/paper/714b3d068e48f825b01bde268043cc0c60f2be71">21: Learning from Explanations with Neural Execution Tree</a></td>
</tr>
<tr>
<td>0</td>
<td>0.770648</td>
<td>0.148444</td>
<td><a href="https://www.semanticscholar.org/paper/fe54ec491039d450d43bc96fecfa9f1450d83b97">0: Year : 2003 Multilayer annotations in</a></td>
</tr>
<tr>
<td>0</td>
<td>0.727087</td>
<td>0.997713</td>
<td><a href="https://www.semanticscholar.org/paper/ac11062f1f368d97f4c826c317bf50dcc13fdb59">257: Dissecting Contextual Word Embeddings: Architecture and Representation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.647991</td>
<td>0.997253</td>
<td><a href="https://www.semanticscholar.org/paper/c41516420ddbd0f29e010ca259a74c1fc2da0466">534: What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties</a></td>
</tr>
<tr>
<td>0</td>
<td>0.773671</td>
<td>0.996340</td>
<td><a href="https://www.semanticscholar.org/paper/f04df4e20a18358ea2f689b4c129781628ef7fc1">2477: A large annotated corpus for learning natural language inference</a></td>
</tr>
<tr>
<td>0</td>
<td>0.694438</td>
<td>0.995186</td>
<td><a href="https://www.semanticscholar.org/paper/fddbcabe0fc9be0684855ae3dd059fb525a69e5b">194: Simple BERT Models for Relation Extraction and Semantic Role Labeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.644679</td>
<td>0.995162</td>
<td><a href="https://www.semanticscholar.org/paper/a0cabbd2572118bfaa1f0372a02514c96fcc99db">59: Humor Detection: A Transformer Gets the Last Laugh</a></td>
</tr>
<tr>
<td>0</td>
<td>0.649555</td>
<td>0.994365</td>
<td><a href="https://www.semanticscholar.org/paper/9f1c5777a193b2c3bb2b25e248a156348e5ba56d">151: Cloze-driven Pretraining of Self-attention Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.621484</td>
<td>0.994098</td>
<td><a href="https://www.semanticscholar.org/paper/549c1a581b61f9ea47afc6f6871845392eaebbc4">89: LCQMC:A Large-scale Chinese Question Matching Corpus</a></td>
</tr>
<tr>
<td>0</td>
<td>0.626936</td>
<td>0.994097</td>
<td><a href="https://www.semanticscholar.org/paper/6007bd2a34385132a7885b934d90b519a1f65bba">45: ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.732481</td>
<td>0.994054</td>
<td><a href="https://www.semanticscholar.org/paper/096c953c46d6e347f20927cf6efef9211e45a306">1: NMT Enhancement based on Knowledge Graph Mining with Pre-trained Language Model</a></td>
</tr>
</table></html>
