<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/495da6f19baa09c6db3697d839e10432cdc25934">518: Multilingual Denoising Pre-training for Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.889082</td>
<td>0.989978</td>
<td><a href="https://www.semanticscholar.org/paper/c9970491141187a446fe5f34d6507a52d383ae1a">1: Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.861203</td>
<td>0.879131</td>
<td><a href="https://www.semanticscholar.org/paper/7b3d5c2b253c602f88ffba9631b93be1d2da66dd">2: Fine-Tuning Self-Supervised Multilingual Sequence-To-Sequence Models for Extremely Low-Resource NMT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.849555</td>
<td>0.992714</td>
<td><a href="https://www.semanticscholar.org/paper/10b9a1f65d70964f85430d8edf419de736939d41">6: Multilingual Translation via Grafting Pre-trained Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.837591</td>
<td>0.858267</td>
<td><a href="https://www.semanticscholar.org/paper/033b0e39338a89b12e9a4d62dc73aa07b597308e">0: Regressing Word and Sentence Embeddings for Regularization of Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.836103</td>
<td>0.973895</td>
<td><a href="https://www.semanticscholar.org/paper/c00ba15810496669d47d2ed5b627e6c7d2b1f6aa">92: Pre-training via Paraphrasing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.833971</td>
<td>0.905840</td>
<td><a href="https://www.semanticscholar.org/paper/cd67c6fbdf2eeaa5f2c4cd17141182036be944ba">1: Keeping Models Consistent between Pretraining and Translation for Low-Resource Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.833877</td>
<td>0.985668</td>
<td><a href="https://www.semanticscholar.org/paper/1a2b7cd531e85450b83c595e85115139152c6741">2: Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.828400</td>
<td>0.941128</td>
<td><a href="https://www.semanticscholar.org/paper/2d2677460249bdcfad72bf3f95ca1d9a75dda502">32: Multilingual Unsupervised NMT using Shared Encoder and Language-Specific Decoders</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826193</td>
<td>0.781607</td>
<td><a href="https://www.semanticscholar.org/paper/25c32d28559f1f27be141f5d4a3eaa5176672626">7: Monolingual Embeddings for Low Resourced Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.808656</td>
<td>0.998465</td>
<td><a href="https://www.semanticscholar.org/paper/1f58c42f44113f1c3c8a97c538e78f37f839f4b8">112: Multilingual Translation with Extensible Multilingual Pretraining and Finetuning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.720002</td>
<td>0.998258</td>
<td><a href="https://www.semanticscholar.org/paper/9f6b659033da6fff11da1af64fea7c0d728ab433">77: GECToR – Grammatical Error Correction: Tag, Not Rewrite</a></td>
</tr>
<tr>
<td>0</td>
<td>0.813493</td>
<td>0.997581</td>
<td><a href="https://www.semanticscholar.org/paper/674a7fd6223de0ff3bbd032f0d84186fb85404ea">17: A Simple Recipe for Multilingual Grammatical Error Correction</a></td>
</tr>
<tr>
<td>0</td>
<td>0.784825</td>
<td>0.997248</td>
<td><a href="https://www.semanticscholar.org/paper/5714c48293bf77e399fd1ba8ad62e74e252fdb9e">54: Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction</a></td>
</tr>
<tr>
<td>0</td>
<td>0.845072</td>
<td>0.996995</td>
<td><a href="https://www.semanticscholar.org/paper/226a962e9e2e01cafc3803018bb8bf511d549e9f">51: Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information</a></td>
</tr>
<tr>
<td>0</td>
<td>0.817836</td>
<td>0.996921</td>
<td><a href="https://www.semanticscholar.org/paper/f7cc76c5e563ad89762feea7489114c98eb4b752">1: Understanding and Improving Sequence-to-Sequence Pretraining for Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.779663</td>
<td>0.996572</td>
<td><a href="https://www.semanticscholar.org/paper/1dc513a688ca92809e504144c3d1e361d1df9927">6: Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.861391</td>
<td>0.996419</td>
<td><a href="https://www.semanticscholar.org/paper/0c6b06a0498ad8156af499a3b9b2b612951b3504">7: Continual Mixed-Language Pre-Training for Extremely Low-Resource Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.656788</td>
<td>0.996345</td>
<td><a href="https://www.semanticscholar.org/paper/f4af3fe736b616452424d50cbd47d52f0a210582">111: OPUS-MT – Building open translation services for the World</a></td>
</tr>
</table></html>
