<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f">603: Attention is not Explanation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.798316</td>
<td>0.934764</td>
<td><a href="https://www.semanticscholar.org/paper/0696ad8beb0d765973aa5cdbc6e118889d3583b0">54: Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.781010</td>
<td>0.581488</td>
<td><a href="https://www.semanticscholar.org/paper/65c23a1d3c5544f77e78636d2cd3af211ec3e41a">81: Interpretations are useful: penalizing explanations to align neural networks with prior knowledge</a></td>
</tr>
<tr>
<td>0</td>
<td>0.755470</td>
<td>0.739109</td>
<td><a href="https://www.semanticscholar.org/paper/2b5d6aff7c19d879c12e9757ba8a65967d27da93">0: Predictive Representation Learning for Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.749012</td>
<td>0.654283</td>
<td><a href="https://www.semanticscholar.org/paper/27a591ca871b22dfd6dd0c7d59fed69cbe6d96da">0: 2 Background : Latent Alignment and Neural Attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.748697</td>
<td>0.837736</td>
<td><a href="https://www.semanticscholar.org/paper/8c0d8027037a71e63cc12e060066ed417b82b429">4: Relation Classification with Cognitive Attention Supervision</a></td>
</tr>
<tr>
<td>0</td>
<td>0.742459</td>
<td>0.972164</td>
<td><a href="https://www.semanticscholar.org/paper/b59646ddc6c102da27d42097d99f1deada65c84a">67: Towards Explainable NLP: A Generative Explanation Framework for Text Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.740890</td>
<td>0.554861</td>
<td><a href="https://www.semanticscholar.org/paper/72cebd7d046080899703ed3cd96e3019a9f60f13">9: Interpreting Visual Question Answering Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.735446</td>
<td>0.031516</td>
<td><a href="https://www.semanticscholar.org/paper/0d0a810f622a0d753ef41f32cf963254ba9926b8">28: Guided Attention Inference Network</a></td>
</tr>
<tr>
<td>0</td>
<td>0.725312</td>
<td>0.935198</td>
<td><a href="https://www.semanticscholar.org/paper/a9c07a60696a165aed09f0f772cc51ff52574a07">1: Exploring Neural Language Models via Analysis of Local and Global Self-Attention Spaces</a></td>
</tr>
<tr>
<td>0</td>
<td>0.879232</td>
<td>0.998277</td>
<td><a href="https://www.semanticscholar.org/paper/135112c7ba1762d65f39b1a61777f26ae4dfd8ad">247: Is Attention Interpretable?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.831990</td>
<td>0.998269</td>
<td><a href="https://www.semanticscholar.org/paper/ce177672b00ddf46e4906157a7e997ca9338b8b9">407: Attention is not not Explanation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.771076</td>
<td>0.996076</td>
<td><a href="https://www.semanticscholar.org/paper/cf2fcb73e2effff29ceb5a5b89bbca34d2d27c1a">103: Learning to Deceive with Attention-Based Explanations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.791903</td>
<td>0.993279</td>
<td><a href="https://www.semanticscholar.org/paper/508884a136a461869be128027950d2aa1778518c">58: The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.803222</td>
<td>0.988632</td>
<td><a href="https://www.semanticscholar.org/paper/e969778bced13a339f3d0465cea4e10c489ee1cc">0: Is Attention Explanation? An Introduction to the Debate</a></td>
</tr>
<tr>
<td>0</td>
<td>0.851062</td>
<td>0.986455</td>
<td><a href="https://www.semanticscholar.org/paper/3d4dfbdcb11d7b495e066435a9a98f02eb0cb369">68: Attention Interpretability Across NLP Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.732747</td>
<td>0.985716</td>
<td><a href="https://www.semanticscholar.org/paper/61d4041d8ba9ce24074ddeba55fe5381c69cb7a5">0: Reproducibility study-Does enforcing diversity in hidden states of LSTM-Attention models improve transparency?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.668751</td>
<td>0.984329</td>
<td><a href="https://www.semanticscholar.org/paper/1a066314e819d34aa4a836ef59a730ff1a11a1b0">1: An Empirical Study on Explanations in Out-of-Domain Settings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.666971</td>
<td>0.983350</td>
<td><a href="https://www.semanticscholar.org/paper/c242438dac5aa4d9b13766c14240bb8426690d58">208: e-SNLI: Natural Language Inference with Natural Language Explanations</a></td>
</tr>
</table></html>
