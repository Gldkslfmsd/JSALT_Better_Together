<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/1258db72eec4bbf02e29edf5bb0c300491a01242">92: MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.845284</td>
<td>0.921768</td>
<td><a href="https://www.semanticscholar.org/paper/cafcfb62be3a83009e755fa11ff2b1bed5e0b937">0: Combining word embeddings and convolutional neural networks to detect duplicated questions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.825112</td>
<td>0.975706</td>
<td><a href="https://www.semanticscholar.org/paper/27725a2d2a8cee9bf9fffc6c2167017103aba0fa">2962: A Convolutional Neural Network for Modelling Sentences</a></td>
</tr>
<tr>
<td>0</td>
<td>0.822981</td>
<td>0.827039</td>
<td><a href="https://www.semanticscholar.org/paper/47460de15d94246b46e3e417176804bd720abcb1">5: Unsupervised Document Embedding With CNNs</a></td>
</tr>
<tr>
<td>0</td>
<td>0.820662</td>
<td>0.971209</td>
<td><a href="https://www.semanticscholar.org/paper/75ff20c21d4ab56917286b429643db4c216f51b5">48: Word Embeddings and Their Use In Sentence Classification Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.820074</td>
<td>0.683933</td>
<td><a href="https://www.semanticscholar.org/paper/32a14843124f73e9f94f6c5babb79a90b3530c42">0: A Neural Model for Compositional Word Embeddings and Sentence Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.814537</td>
<td>0.869735</td>
<td><a href="https://www.semanticscholar.org/paper/6f9c943912701d6f7a65aa3dcff6129c2d8f53db">0: Revisiting Supervised Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.812124</td>
<td>0.789690</td>
<td><a href="https://www.semanticscholar.org/paper/1636535265eea760954a6176ae8058f2db18307e">1: Learning Document Embeddings With CNNs</a></td>
</tr>
<tr>
<td>0</td>
<td>0.807399</td>
<td>0.903844</td>
<td><a href="https://www.semanticscholar.org/paper/bcf165bfe691b72188868ad44d68aec9e6397d0b">77: Concatenated p-mean Word Embeddings as Universal Cross-Lingual Sentence Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.806986</td>
<td>0.865009</td>
<td><a href="https://www.semanticscholar.org/paper/b7239a064d759615f602b8441e338843b07dfba2">3: Sentence Similarity Prediction based on Siamese CNN-Bidirectional LSTM with Self-attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.737707</td>
<td>0.993219</td>
<td><a href="https://www.semanticscholar.org/paper/c6d6e655f52862de14a17b524c553692f18458f5">26: Radical-Based Hierarchical Embeddings for Chinese Sentiment Analysis at Sentence Level</a></td>
</tr>
<tr>
<td>0</td>
<td>0.829559</td>
<td>0.991898</td>
<td><a href="https://www.semanticscholar.org/paper/b4b78f1823cc0cfce13d52faf7e68f1f7e46f993">20: Modelling the Combination of Generic and Target Domain Embeddings in a Convolutional Neural Network for Sentence Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.755404</td>
<td>0.991893</td>
<td><a href="https://www.semanticscholar.org/paper/b343876641d44e54cede21afad13116cf136405e">3: SFV-CNN: Deep Text Sentiment Classification with Scenario Feature Representation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.684663</td>
<td>0.990298</td>
<td><a href="https://www.semanticscholar.org/paper/5c6ee6707ed3ee298a57064be0acee7a464bdce7">89: Radical-Enhanced Chinese Character Embedding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.688670</td>
<td>0.990246</td>
<td><a href="https://www.semanticscholar.org/paper/1aff70ba25a168cbac4df48fc2a14f8002d49ff4">2: Recursive Nested Neural Network for Sentiment Analysis</a></td>
</tr>
<tr>
<td>0</td>
<td>0.739974</td>
<td>0.989829</td>
<td><a href="https://www.semanticscholar.org/paper/93b55ee4df904f824fdf1cc3b001b76e03c72215">9: Towards better Sentence Classification for Morphologically Rich Languages by</a></td>
</tr>
<tr>
<td>0</td>
<td>0.724496</td>
<td>0.989676</td>
<td><a href="https://www.semanticscholar.org/paper/bc1022b031dc6c7019696492e8116598097a8c12">6633: Natural Language Processing (Almost) from Scratch</a></td>
</tr>
<tr>
<td>0</td>
<td>0.758637</td>
<td>0.989420</td>
<td><a href="https://www.semanticscholar.org/paper/190ad72e271d7dc9e53d9bc7f7ab31acc332e6e1">24: Learning Sentiment-Specific Word Embedding via Global Sentiment Representation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.764950</td>
<td>0.988289</td>
<td><a href="https://www.semanticscholar.org/paper/1e662c87a36779dbe9f56f7ffd3ade756059d094">233: Joint Learning of Character and Word Embeddings</a></td>
</tr>
</table></html>
