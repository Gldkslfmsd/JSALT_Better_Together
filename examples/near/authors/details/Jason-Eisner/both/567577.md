<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/1299732e905399066a36d11990a3921911de63b7">92: Imitation Learning by Coaching</a></td>
</tr>
<tr>
<td>0</td>
<td>0.892643</td>
<td>0.923441</td>
<td><a href="https://www.semanticscholar.org/paper/70e10a5459c6f1aaf346ee4f2dcc837151fbe75c">522: Efficient Reductions for Imitation Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.854642</td>
<td>0.759104</td>
<td><a href="https://www.semanticscholar.org/paper/d51a4bfa0c592dcc93a9ed7c63f94f5681620e39">1: Deterministic Policy Imitation Gradient Algorithm</a></td>
</tr>
<tr>
<td>0</td>
<td>0.852345</td>
<td>0.838219</td>
<td><a href="https://www.semanticscholar.org/paper/7eb26b512148094553db2c5845a8e04f17862f13">0: VILD: VARIATIONAL IMITATION LEARNING</a></td>
</tr>
<tr>
<td>0</td>
<td>0.849354</td>
<td>0.881035</td>
<td><a href="https://www.semanticscholar.org/paper/6e81e3b82c875063dca2aee551aa5d50cc3c69cc">54: Provably Efficient Imitation Learning from Observation Alone</a></td>
</tr>
<tr>
<td>0</td>
<td>0.847276</td>
<td>0.836399</td>
<td><a href="https://www.semanticscholar.org/paper/bd2d5dfd424e6342f33ebeeebcdaf7974db684c9">0: Sample Efficient Imitation Learning via Reward Function Trained in Advance</a></td>
</tr>
<tr>
<td>0</td>
<td>0.843662</td>
<td>0.902933</td>
<td><a href="https://www.semanticscholar.org/paper/58a44c2fcbe282a0b8fe354e33703458640b4c28">59: Fast Policy Learning through Imitation and Reinforcement</a></td>
</tr>
<tr>
<td>0</td>
<td>0.840482</td>
<td>0.173245</td>
<td><a href="https://www.semanticscholar.org/paper/ea6b9d2e766697229d345279da899424e0bb7d66">0: Efficient Off-policy Adversarial Imitation Learning with Imperfect Demonstrations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.831459</td>
<td>0.815065</td>
<td><a href="https://www.semanticscholar.org/paper/70ad49d0d703c93cb7045105e950033b2b347821">0: Robust Adversarial Imitation Learning via Adaptively-Selected Demonstrations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.829490</td>
<td>0.880047</td>
<td><a href="https://www.semanticscholar.org/paper/4f7dc1ce7045d13972dbef2f59694227a53127ed">0: KWIK Inverse Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.555364</td>
<td>0.951440</td>
<td><a href="https://www.semanticscholar.org/paper/c8936876710d54e2dc33f6b9d6e5e439afa3717b">0: Optimising spoken dialogue systems using Gaussian process reinforcement learning for a large action set</a></td>
</tr>
<tr>
<td>0</td>
<td>0.439836</td>
<td>0.947866</td>
<td><a href="https://www.semanticscholar.org/paper/074b4702465630da81a3b40600183e58a42c404d">0: Apprentissage séquentiel budgétisé pour la classification extrême et la découverte de hiérarchie en apprentissage par renforcement. (Budgeted sequential learning for extreme classification and for the discovery of hierarchy in reinforcement learning)</a></td>
</tr>
<tr>
<td>0</td>
<td>0.752517</td>
<td>0.946985</td>
<td><a href="https://www.semanticscholar.org/paper/849975faef27739792a08c4fccf8d75a54905565">0: Practical Techniques for Leveraging Experts for Sequential Decisions and Predictions Preliminary Oral Exam (Thesis Proposal)</a></td>
</tr>
<tr>
<td>0</td>
<td>0.747349</td>
<td>0.945524</td>
<td><a href="https://www.semanticscholar.org/paper/94373b07e4f6712b1916f9d522a224397644117d">2: Towards Generalization and Efficiency of Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.794082</td>
<td>0.944621</td>
<td><a href="https://www.semanticscholar.org/paper/c44c4d26d157c09da76ff6ecca9da13e85f86b0e">0: Score-based Inverse Reinforcement Learning Conference</a></td>
</tr>
<tr>
<td>0</td>
<td>0.596784</td>
<td>0.944008</td>
<td><a href="https://www.semanticscholar.org/paper/04213b2da00939e9fb72efcd8b304c18b20b6f4a">81: Gaussian Processes for Fast Policy Optimisation of POMDP-based Dialogue Managers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.561617</td>
<td>0.943168</td>
<td><a href="https://www.semanticscholar.org/paper/50af61956a3f5061154125983de81d8a4338e357">29: Reward Shaping for Statistical Optimisation of Dialogue Management</a></td>
</tr>
<tr>
<td>0</td>
<td>0.802581</td>
<td>0.943064</td>
<td><a href="https://www.semanticscholar.org/paper/ae35a803967981b96d03dc1f90a1bedc43025cdb">28: Smooth Imitation Learning for Online Sequence Prediction</a></td>
</tr>
<tr>
<td>0</td>
<td>0.627459</td>
<td>0.942468</td>
<td><a href="https://www.semanticscholar.org/paper/544a4861653847d6777fb83b7b46e3e087dbc3f6">9: Active Information Acquisition</a></td>
</tr>
</table></html>
