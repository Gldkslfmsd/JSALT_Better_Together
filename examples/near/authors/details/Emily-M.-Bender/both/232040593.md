<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/6d9727f1f058614cada3fe296eeebd8ec4fc512a">522: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ</a></td>
</tr>
<tr>
<td>0</td>
<td>0.794070</td>
<td>0.956657</td>
<td><a href="https://www.semanticscholar.org/paper/bb15f3727f827a3cb88b5d3ca48415c09b40a88f">3: What Language Model to Train if You Have One Million GPU Hours?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.778661</td>
<td>0.976094</td>
<td><a href="https://www.semanticscholar.org/paper/8b9d77d5e52a70af37451d3db3d32781b83ea054">117: On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines</a></td>
</tr>
<tr>
<td>0</td>
<td>0.776603</td>
<td>0.961285</td>
<td><a href="https://www.semanticscholar.org/paper/79fdff5339017ec92b979efa4dff33d21a69b66e">1: Emergent Properties of Finetuned Language Representation Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.774066</td>
<td>0.792143</td>
<td><a href="https://www.semanticscholar.org/paper/424c932fe14cb40e95ed18f3184aa76331026942">0: Dim Wihl Gat Tun: The Case for Linguistic Expertise in NLP for Under-Documented Languages</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763490</td>
<td>0.244311</td>
<td><a href="https://www.semanticscholar.org/paper/9a4dfe28c7da2e5e709d97528e68177a7eb616d8">8: Challenges in Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.755183</td>
<td>0.333365</td>
<td><a href="https://www.semanticscholar.org/paper/8a35a8fa182d61b490c1b2dc87d56e40d01a4fdd">1: Sequential Decisions and Predictions in Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.751378</td>
<td>0.838190</td>
<td><a href="https://www.semanticscholar.org/paper/3cb0130c34a808e37a404840627ab6a72cc3734e">0: estion Answering over Curated and OpenWeb Sources</a></td>
</tr>
<tr>
<td>0</td>
<td>0.749450</td>
<td>0.511339</td>
<td><a href="https://www.semanticscholar.org/paper/44cd48cbcc44373329e173aebf4807b5254f0621">4: Recent Progress in Deep Learning for NLP</a></td>
</tr>
<tr>
<td>0</td>
<td>0.747425</td>
<td>0.956129</td>
<td><a href="https://www.semanticscholar.org/paper/32ead456a7316d26893861169f57ddaeb512dabb">2: CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.615080</td>
<td>0.991951</td>
<td><a href="https://www.semanticscholar.org/paper/399e7d8129c60818ee208f236c8dda17e876d21f">139: RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.750178</td>
<td>0.991847</td>
<td><a href="https://www.semanticscholar.org/paper/02fde8bfd9259a4f53316579eb0bf97213559e5c">49: The Radicalization Risks of GPT-3 and Advanced Neural Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.541940</td>
<td>0.991560</td>
<td><a href="https://www.semanticscholar.org/paper/4c2733d191e347753bb28afa46a1c55c65e085be">52: Persistent Anti-Muslim Bias in Large Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.694656</td>
<td>0.990032</td>
<td><a href="https://www.semanticscholar.org/paper/d624bc273821c871f899d8256a34be40c09fc3cd">29: Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets</a></td>
</tr>
<tr>
<td>0</td>
<td>0.688788</td>
<td>0.989711</td>
<td><a href="https://www.semanticscholar.org/paper/0abcbdf40f872e6baf1c082811d4ae93df787698">173: Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets</a></td>
</tr>
<tr>
<td>0</td>
<td>0.287222</td>
<td>0.988981</td>
<td><a href="https://www.semanticscholar.org/paper/4eda2b9eaef3ae892382acc21593eed6f56f2ea1">11: Large language models associate Muslims with violence</a></td>
</tr>
<tr>
<td>0</td>
<td>0.675966</td>
<td>0.988787</td>
<td><a href="https://www.semanticscholar.org/paper/6260e0e330d4f913d8c4dda7aa42043c05b07a6d">53: Utility is in the Eye of the User: A Critique of NLP Leaderboards</a></td>
</tr>
<tr>
<td>0</td>
<td>0.617555</td>
<td>0.988130</td>
<td><a href="https://www.semanticscholar.org/paper/4ae632b89089b38ce41d307a6cda4727e42aaab3">20: Detoxifying Language Models Risks Marginalizing Minority Voices</a></td>
</tr>
<tr>
<td>0</td>
<td>0.702531</td>
<td>0.987717</td>
<td><a href="https://www.semanticscholar.org/paper/8eda3fd907afcbad277fa3e3bf7f23e3cbfbbc84">315: Adversarial NLI: A New Benchmark for Natural Language Understanding</a></td>
</tr>
</table></html>
