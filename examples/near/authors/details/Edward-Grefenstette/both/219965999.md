<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/df8a0bed6d685fee9c5ad418f4834e9537878de2">56: Learning with AMIGo: Adversarially Motivated Intrinsic Goals</a></td>
</tr>
<tr>
<td>0</td>
<td>0.856205</td>
<td>0.991243</td>
<td><a href="https://www.semanticscholar.org/paper/92edf4d699a139b757e6cfb5407872af5472a758">1: Explore and Control with Adversarial Surprise</a></td>
</tr>
<tr>
<td>0</td>
<td>0.841701</td>
<td>0.982541</td>
<td><a href="https://www.semanticscholar.org/paper/a059870910339596c5c57969bc2397a7e268435e">0: Learning Embodied Agents with Scalably-Supervised Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.829458</td>
<td>0.991771</td>
<td><a href="https://www.semanticscholar.org/paper/0124db44bb52cc8dd4c56f311464247fcd2336cc">1: Adversarial Goal Generation for Intrinsic Motivation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.822053</td>
<td>0.971875</td>
<td><a href="https://www.semanticscholar.org/paper/e5afacab42153f24b8e4ca543bd0ecae4e810f10">53: Multi-Agent Adversarial Inverse Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.820077</td>
<td>0.983514</td>
<td><a href="https://www.semanticscholar.org/paper/a66df6a55c25e8f131502c922000b9197debe14a">4: Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations using Generative Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.818911</td>
<td>0.872076</td>
<td><a href="https://www.semanticscholar.org/paper/d275d850c7423f4800a422d4ed8aa8397d20a3a2">20: Learning and Reusing Goal-Specific Policies for Goal-Driven Autonomy</a></td>
</tr>
<tr>
<td>0</td>
<td>0.812493</td>
<td>0.983296</td>
<td><a href="https://www.semanticscholar.org/paper/b5853151f56a2b9779d123904bc719e8c549fac3">0: Deep Reinforcement Learning for Complex Manipulation Tasks with Sparse Feedback</a></td>
</tr>
<tr>
<td>0</td>
<td>0.805744</td>
<td>0.973902</td>
<td><a href="https://www.semanticscholar.org/paper/33fb9b6964138853bb2870dc045b1b5a0b9cde94">2: Hindsight Generative Adversarial Imitation Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.804055</td>
<td>0.981091</td>
<td><a href="https://www.semanticscholar.org/paper/0d03470691ccc9be612c3e3a842a9a7b50f78087">1: Adversarial Active Exploration for Inverse Dynamics Model Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.742907</td>
<td>0.998146</td>
<td><a href="https://www.semanticscholar.org/paper/60a5be13ba59e21e4b8f335b3bcc341d93d8deea">53: Automatic Curriculum Learning For Deep RL: A Short Survey</a></td>
</tr>
<tr>
<td>0</td>
<td>0.815917</td>
<td>0.997961</td>
<td><a href="https://www.semanticscholar.org/paper/bebe8ffb0c357ac0c7eea2556f817b03ee22b570">61: RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments</a></td>
</tr>
<tr>
<td>0</td>
<td>0.723026</td>
<td>0.997476</td>
<td><a href="https://www.semanticscholar.org/paper/43907a7a3c607cc463907a2a8a8ebfc5626202bb">2: Goal-Conditioned Reinforcement Learning: Problems and Solutions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.779529</td>
<td>0.997254</td>
<td><a href="https://www.semanticscholar.org/paper/ffba26a38d4b3c25d2984266f72f72889b2413ff">21: Learning To Reach Goals Without Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.785026</td>
<td>0.997009</td>
<td><a href="https://www.semanticscholar.org/paper/831e7cbafed2dca05db1e7f5ef16d1a7614f44ec">36: Learning to Reach Goals via Iterated Supervised Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.766519</td>
<td>0.996996</td>
<td><a href="https://www.semanticscholar.org/paper/ef5c29f6d78f81c78b2d7a625d4d4053f51fcd6e">1: Variational Automatic Curriculum Learning for Sparse-Reward Cooperative Multi-Agent Problems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.734730</td>
<td>0.996892</td>
<td><a href="https://www.semanticscholar.org/paper/2456208277f5d7c35b9602fee39fea51a3785b17">99: Never Give Up: Learning Directed Exploration Strategies</a></td>
</tr>
<tr>
<td>0</td>
<td>0.757752</td>
<td>0.996787</td>
<td><a href="https://www.semanticscholar.org/paper/114b52291b549466a4b1027f4248a122c1c3920c">3: Learning Compositional Neural Programs for Continuous Control</a></td>
</tr>
<tr>
<td>0</td>
<td>0.732946</td>
<td>0.996735</td>
<td><a href="https://www.semanticscholar.org/paper/7388826b5ee00efe17cb7f19a623d9b5e955ae70">142: Skew-Fit: State-Covering Self-Supervised Reinforcement Learning</a></td>
</tr>
</table></html>
