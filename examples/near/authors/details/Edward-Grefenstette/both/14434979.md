<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/e957747f4f8600940be4c5bb001aa70c84e53a53">279: Latent Predictor Networks for Code Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.798157</td>
<td>0.788891</td>
<td><a href="https://www.semanticscholar.org/paper/5fbcfccd3736969d95ed660d8e6962c86b7a9113">19: PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793355</td>
<td>0.850262</td>
<td><a href="https://www.semanticscholar.org/paper/463fefdbd81a4a0a32cf59bc58a9545757c8cf2e">34: Pre-trained Contextual Embedding of Source Code</a></td>
</tr>
<tr>
<td>0</td>
<td>0.776730</td>
<td>0.976468</td>
<td><a href="https://www.semanticscholar.org/paper/3c11e117d2996520ae7c06b2b4de3b4bde4f875a">1: Neural Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.770969</td>
<td>0.906955</td>
<td><a href="https://www.semanticscholar.org/paper/fec05fde36dd2eb56792b88ebfac20b248aaff81">0: TOKEN PAIRING TO IMPROVE NEURAL PROGRAM SYNTHESIS MODELS</a></td>
</tr>
<tr>
<td>0</td>
<td>0.761924</td>
<td>0.776019</td>
<td><a href="https://www.semanticscholar.org/paper/7d884b40ef5892f61e0f6f358b8e29983f64a178">45: Controllable Story Generation with External Knowledge Using Large-Scale Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.754931</td>
<td>0.958098</td>
<td><a href="https://www.semanticscholar.org/paper/505c8a58ec6d06e2ce7c1234c91d384a38c07c6a">0: Data-to-text Generation with Variational Sequential Planning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.753996</td>
<td>0.825450</td>
<td><a href="https://www.semanticscholar.org/paper/b5aa927c906101b3f8854a29f374551e3ea64474">87: Pre-trained language model representations for language generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.748188</td>
<td>0.751741</td>
<td><a href="https://www.semanticscholar.org/paper/c0be1f69afddc4493d64e5aac9a05fdf7f2f3184">10: Neural Conversational QA: Learning to Reason vs Exploiting Patterns</a></td>
</tr>
<tr>
<td>0</td>
<td>0.746379</td>
<td>0.215949</td>
<td><a href="https://www.semanticscholar.org/paper/7beeaf688e99db93d83ecba8211b9eafba7022d3">67: Processing Natural Language without Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.734845</td>
<td>0.993726</td>
<td><a href="https://www.semanticscholar.org/paper/2c1e874c3b67510a3215e535f5646b362de5bc89">252: Abstract Syntax Networks for Code Generation and Semantic Parsing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.688123</td>
<td>0.993518</td>
<td><a href="https://www.semanticscholar.org/paper/82dbca10dbf6c7a5b2c13579d35c70ac3e8c509c">90: Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741733</td>
<td>0.992719</td>
<td><a href="https://www.semanticscholar.org/paper/9ad258de58ac237c115fbfa0ee52650ba7a0bef2">14: Natural Answer Generation with Heterogeneous Memory</a></td>
</tr>
<tr>
<td>0</td>
<td>0.708048</td>
<td>0.992624</td>
<td><a href="https://www.semanticscholar.org/paper/26e9eb44ed8065122d37b0c429a8d341bfeea9a5">69: Reference-Aware Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.678979</td>
<td>0.992587</td>
<td><a href="https://www.semanticscholar.org/paper/ba49d3823d43515e447296ca4e1e55d3f1fd8c4d">980: Neural Responding Machine for Short-Text Conversation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.623773</td>
<td>0.992139</td>
<td><a href="https://www.semanticscholar.org/paper/54f5e32631f768674617250e05dc261fe8fa9b69">67: Coherent Dialogue with Attention-Based Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.713414</td>
<td>0.992058</td>
<td><a href="https://www.semanticscholar.org/paper/85315b64a4c73cb86f156ef5b0a085d6ebc8a65d">1499: A Neural Conversational Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.740573</td>
<td>0.992007</td>
<td><a href="https://www.semanticscholar.org/paper/bcf16c08a41009d9f9174c6f72b2ff534232c147">93: Sequence-based Structured Prediction for Semantic Parsing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.736423</td>
<td>0.991982</td>
<td><a href="https://www.semanticscholar.org/paper/558ac446dc26bee9789d660a251b75728cb6eeb2">551: Language to Logical Form with Neural Attention</a></td>
</tr>
</table></html>
