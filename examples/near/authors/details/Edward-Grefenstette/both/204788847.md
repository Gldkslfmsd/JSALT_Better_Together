<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/2b88aefb70cd52a4d6899020f4be97c669a5edcb">31: RTFM: Generalising to Novel Environment Dynamics via Reading</a></td>
</tr>
<tr>
<td>0</td>
<td>0.788008</td>
<td>0.928417</td>
<td><a href="https://www.semanticscholar.org/paper/c117a59fdad8ebf857d01082612093e9e6aa676e">17: Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines</a></td>
</tr>
<tr>
<td>0</td>
<td>0.773975</td>
<td>0.780811</td>
<td><a href="https://www.semanticscholar.org/paper/e705255814756178dba75638c29b602095c3cdf4">5: How Transferable are the Representations Learned by Deep Q Agents?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.772119</td>
<td>0.910033</td>
<td><a href="https://www.semanticscholar.org/paper/9319284ccec7c018114f7171781a3e8f83503022">0: RLMViz: Interpréter la Mémoire du Deep Reinforcement Learning RLMViz: Interpreting Deep Reinforcement Learning Memory</a></td>
</tr>
<tr>
<td>0</td>
<td>0.771687</td>
<td>0.875183</td>
<td><a href="https://www.semanticscholar.org/paper/3222486c25f1c28d6d27cf7c47f78c04ecae7405">4: AMRL: Aggregated Memory For Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.760570</td>
<td>0.800571</td>
<td><a href="https://www.semanticscholar.org/paper/30256fd41d471e8c6731c732e41ba865321ced7d">22: S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.759034</td>
<td>0.579144</td>
<td><a href="https://www.semanticscholar.org/paper/0574dc64c8275b09ed587dc3977f4d3c990bd4df">73: Context-Aware Visual Policy Network for Sequence-Level Image Captioning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.757695</td>
<td>0.450582</td>
<td><a href="https://www.semanticscholar.org/paper/d6cf6c5b746e0f2166fd24a29b01a29535b7d6bb">7: Generation-Distillation for Efficient Natural Language Understanding in Low-Data Settings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.755796</td>
<td>0.406495</td>
<td><a href="https://www.semanticscholar.org/paper/c1ec56fc0388c58b2f4b721a0d79cc1f844d15f4">3: Simulating Early Word Learning in Situated Connectionist Agents</a></td>
</tr>
<tr>
<td>0</td>
<td>0.754674</td>
<td>0.941804</td>
<td><a href="https://www.semanticscholar.org/paper/1d41a0ddda57caa6c8d268dd1703e4c9b35db18b">1: One-Shot Learning from a Demonstration with Hierarchical Latent Language</a></td>
</tr>
<tr>
<td>0</td>
<td>0.676441</td>
<td>0.995324</td>
<td><a href="https://www.semanticscholar.org/paper/171bffb23613e89d60664e2078c9e807d0e523df">40: Hierarchical Decision Making by Generating and Following Natural Language Instructions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.777278</td>
<td>0.989460</td>
<td><a href="https://www.semanticscholar.org/paper/7dc156eb9d84ae8fd521ecac5ccc5b5426a42b50">121: A Survey of Reinforcement Learning Informed by Natural Language</a></td>
</tr>
<tr>
<td>0</td>
<td>0.738912</td>
<td>0.988151</td>
<td><a href="https://www.semanticscholar.org/paper/0fa1c75a452a046e11e775eb6120051c696d9366">63: Using Natural Language for Reward Shaping in Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.750205</td>
<td>0.986284</td>
<td><a href="https://www.semanticscholar.org/paper/787fc2361736cf4d3f9afedd7d3e28ffaf804b92">96: BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.698876</td>
<td>0.985573</td>
<td><a href="https://www.semanticscholar.org/paper/019923afa86036b69c0e423f3c2188bfa7050923">221: Grounded Language Learning in a Simulated 3D World</a></td>
</tr>
<tr>
<td>0</td>
<td>0.748716</td>
<td>0.984121</td>
<td><a href="https://www.semanticscholar.org/paper/2da94e99a8c2eac888d33bedd25e7f653ff231da">94: BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop</a></td>
</tr>
<tr>
<td>0</td>
<td>0.746661</td>
<td>0.983671</td>
<td><a href="https://www.semanticscholar.org/paper/88347f9f12b50590f50aefce4cf71b3a3f0bd138">203: Gated-Attention Architectures for Task-Oriented Language Grounding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.743589</td>
<td>0.983586</td>
<td><a href="https://www.semanticscholar.org/paper/e1a47046a7d255ec783a9d9ca23ad67519fc3117">0: Asking for Knowledge: Training RL Agents to Query External Knowledge Using Language</a></td>
</tr>
<tr>
<td>0</td>
<td>0.688544</td>
<td>0.983478</td>
<td><a href="https://www.semanticscholar.org/paper/ce6a1a1fb3fd09b72d5a8ccddbd07c29d67be1c4">4: Transformer Based Reinforcement Learning For Games</a></td>
</tr>
</table></html>
