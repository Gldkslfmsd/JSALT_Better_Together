<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/4f4837c1f8a194d8b7fe7d6073ce44876f08ad6c">168: Discovering Discrete Latent Topics with Neural Variational Inference</a></td>
</tr>
<tr>
<td>0</td>
<td>0.840011</td>
<td>0.879272</td>
<td><a href="https://www.semanticscholar.org/paper/8695ce295f5504b4d6bd19faec7057f894ed7da6">25: Dirichlet belief networks for topic structure learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.836510</td>
<td>0.716353</td>
<td><a href="https://www.semanticscholar.org/paper/35d4c658291753ae952ea605c796deaecd8553ef">128: Stochastic collapsed variational Bayesian inference for latent Dirichlet allocation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.835823</td>
<td>0.875859</td>
<td><a href="https://www.semanticscholar.org/paper/c84a2ca673f8702c765ed67a39b41bef2f1679ad">0: Neural Topic Model Training with the REBAR Gradient Estimator</a></td>
</tr>
<tr>
<td>0</td>
<td>0.835701</td>
<td>-0.053560</td>
<td>NA:69588816</td>
</tr>
<tr>
<td>0</td>
<td>0.831850</td>
<td>0.875267</td>
<td><a href="https://www.semanticscholar.org/paper/c1491b29bc2fc260677f648d25da33e33e9a6367">0: A Joint Learning Approach for Semi-supervised Neural Topic Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.831771</td>
<td>0.661915</td>
<td><a href="https://www.semanticscholar.org/paper/07e287e6934dbd6f45d10e40f6172a0673b3b511">20: Effective Estimation of Deep Generative Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.831483</td>
<td>0.675080</td>
<td><a href="https://www.semanticscholar.org/paper/350f3b75f78789cd5af8d077b3136c167f85f05f">0: Improving Classification Using Topic Correlation and Expectation Propagation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.823550</td>
<td>0.510101</td>
<td><a href="https://www.semanticscholar.org/paper/1fab230f9fc5b5445796bd5ce3dcdef867020962">0: Asymmetric Variational Autoencoders.</a></td>
</tr>
<tr>
<td>0</td>
<td>0.816389</td>
<td>0.688304</td>
<td><a href="https://www.semanticscholar.org/paper/bb781383d127af69275b3295a989721d5fc36285">5: Scalable inference of topic evolution via models for latent geometric structures</a></td>
</tr>
<tr>
<td>0</td>
<td>0.773746</td>
<td>0.985510</td>
<td><a href="https://www.semanticscholar.org/paper/68768d4dca5b71e88094d63d546da9574b09c7b2">4: TAN-NTM: Topic Attention Networks for Neural Topic Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.728669</td>
<td>0.975700</td>
<td><a href="https://www.semanticscholar.org/paper/9fce00ceb510a5baff43470ed9aa495f6f23aad3">18: Topic Modelling Meets Deep Neural Networks: A Survey</a></td>
</tr>
<tr>
<td>0</td>
<td>0.810663</td>
<td>0.974920</td>
<td><a href="https://www.semanticscholar.org/paper/0c25e5f544167840756950e0ff3edd238a19cfd1">286: Autoencoding Variational Inference For Topic Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.848254</td>
<td>0.971004</td>
<td><a href="https://www.semanticscholar.org/paper/2e152759fcd505fcbd914243411b005b51edee22">9: A Discrete Variational Recurrent Topic Model without the Reparametrization Trick</a></td>
</tr>
<tr>
<td>0</td>
<td>0.854504</td>
<td>0.966214</td>
<td><a href="https://www.semanticscholar.org/paper/783808c4aeef7b7df16a0eb810864a78fdd8c0f1">12: Neural Topic Model with Attention for Supervised Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.764623</td>
<td>0.958713</td>
<td><a href="https://www.semanticscholar.org/paper/d448a2f736dc150deaf26f9bb645e6350117c22d">37: Coherence-Aware Neural Topic Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.770944</td>
<td>0.956845</td>
<td><a href="https://www.semanticscholar.org/paper/2d0dab59b792677b2d96d6f5f9bd22e8d8895d50">59: Neural Models for Documents with Metadata</a></td>
</tr>
<tr>
<td>0</td>
<td>0.852062</td>
<td>0.956670</td>
<td><a href="https://www.semanticscholar.org/paper/8c1d1c1cb110e62cb807b0e56faf5a29d4671638">16: Neural Topic Model with Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.729517</td>
<td>0.951430</td>
<td><a href="https://www.semanticscholar.org/paper/9db03509f2a8917dc4135c0a7dfc4ff7274f533d">1: Neural Attention-Aware Hierarchical Topic Model</a></td>
</tr>
</table></html>
