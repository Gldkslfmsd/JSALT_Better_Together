<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/dbcbf1b92f251151e487204f60dc5bccff805fd8">2: Improving Intrinsic Exploration with Language Abstractions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.838659</td>
<td>0.950057</td>
<td><a href="https://www.semanticscholar.org/paper/d6757aedcb53142bc439ec64bfd0b056d99b1881">147: Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.835094</td>
<td>0.944807</td>
<td><a href="https://www.semanticscholar.org/paper/1f601a78724e9e6c26e0a46333b88b3aae134410">0: Decoupled Reinforcement Learning to Stabilise Intrinsically-Motivated Exploration</a></td>
</tr>
<tr>
<td>0</td>
<td>0.835085</td>
<td>0.938168</td>
<td><a href="https://www.semanticscholar.org/paper/8307a2915f56f48d07158fa335cb49b3959aa738">0: Novelty-Guided Reinforcement Learning via Encoded Behaviors</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826733</td>
<td>0.938598</td>
<td><a href="https://www.semanticscholar.org/paper/9b75d487ac4bead09c7b887ea70187c7dd1364bf">2: Automata-Guided Hierarchical Reinforcement Learning for Skill Composition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826158</td>
<td>0.949963</td>
<td><a href="https://www.semanticscholar.org/paper/3a71c306eb6232658c9e5fd48aed1ef3befe5fbe">127: Planning to Explore via Self-Supervised World Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.820814</td>
<td>0.941414</td>
<td><a href="https://www.semanticscholar.org/paper/fbe22105c3dd0fa3220e58b4a167129f1e548581">2: Generative Exploration and Exploitation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.818865</td>
<td>0.936910</td>
<td><a href="https://www.semanticscholar.org/paper/3fc2f8f0c508d6bf7acc26ea51c81cb55aa71f96">1: Decoupling Exploration and Exploitation in Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.818175</td>
<td>0.830561</td>
<td><a href="https://www.semanticscholar.org/paper/68c2bae04f08ffeb54c68afd436272ee459b1f79">4: EMI: Exploration with Mutual Information Maximizing State and Action Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.817508</td>
<td>0.943050</td>
<td><a href="https://www.semanticscholar.org/paper/7c2bb6d3e9a154335d9af5c7298b587ac15945bd">0: Exploration via State influence Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809151</td>
<td>0.994483</td>
<td><a href="https://www.semanticscholar.org/paper/c2c8482c713b94073f3d59895b373db4398ddfbb">86: Language as an Abstraction for Hierarchical Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.808738</td>
<td>0.992022</td>
<td><a href="https://www.semanticscholar.org/paper/0d6a4e45acde6f47d704ed0752f17f7ab52223af">9: Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.768125</td>
<td>0.991495</td>
<td><a href="https://www.semanticscholar.org/paper/7ab8e3ade4d419d8a5ef4cb1b87329c6e9a1089e">0: Language Grounded Task-Adaptation in Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793875</td>
<td>0.989198</td>
<td><a href="https://www.semanticscholar.org/paper/3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7">288: Modular Multitask Reinforcement Learning with Policy Sketches</a></td>
</tr>
<tr>
<td>0</td>
<td>0.706509</td>
<td>0.987521</td>
<td><a href="https://www.semanticscholar.org/paper/ea18b3279afe3ffd54da5147dd5c1872315725bf">139: TarMAC: Targeted Multi-Agent Communication</a></td>
</tr>
<tr>
<td>0</td>
<td>0.690466</td>
<td>0.987483</td>
<td><a href="https://www.semanticscholar.org/paper/7bac30e24036a111eb0a20bf8e4a9cef3cb79559">0: Concentration Network for Reinforcement Learning of Large-Scale Multi-Agent Systems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.762898</td>
<td>0.987285</td>
<td><a href="https://www.semanticscholar.org/paper/5b7d0f5fde3f8c8ee97a0d8b13dd4d225c48a987">1: In a Nutshell, the Human Asked for This: Latent Goals for Following Temporal Specifications</a></td>
</tr>
<tr>
<td>0</td>
<td>0.687215</td>
<td>0.987081</td>
<td><a href="https://www.semanticscholar.org/paper/d39dd7cdc160f7f8a07c857ed406f1b17a83e223">3: Offline Pre-trained Multi-Agent Decision Transformer: One Big Sequence Model Tackles All SMAC Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.771428</td>
<td>0.986188</td>
<td><a href="https://www.semanticscholar.org/paper/0873aac6e84597e3b5d2730783f6f6fb0e361400">2: Fast Task-Adaptation for Tasks Labeled Using Natural Language in Reinforcement Learning</a></td>
</tr>
</table></html>
