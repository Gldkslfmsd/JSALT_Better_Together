<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/fdb8ccebef9f544f44cd9b88085d8ec5458b38ab">8: Replay-Guided Adversarial Environment Design</a></td>
</tr>
<tr>
<td>0</td>
<td>0.829686</td>
<td>0.009047</td>
<td><a href="https://www.semanticscholar.org/paper/5e7a3ba168635b0a24285bd758b2bcc25fc054f3">0: Adversary Agnostic Robust Deep Reinforcement Learning.</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826771</td>
<td>0.991049</td>
<td><a href="https://www.semanticscholar.org/paper/e94241c59b9caa3efe6950731a164fa787b4d737">5: Adversarial Environment Generation for Learning to Navigate the Web</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826718</td>
<td>0.983673</td>
<td><a href="https://www.semanticscholar.org/paper/33b456eb43e5391761540f17a29e598d7595565b">11: Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble</a></td>
</tr>
<tr>
<td>0</td>
<td>0.822594</td>
<td>0.964507</td>
<td><a href="https://www.semanticscholar.org/paper/76cf947057333b9ecb6e83c066d4f0dbcc278d3e">0: Adversarial Style Transfer for Robust Policy Optimization in Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.816823</td>
<td>0.325831</td>
<td><a href="https://www.semanticscholar.org/paper/47f16229b64ede96dc7ee86ef2aabe64798a8e08">28: Online Adaptative Curriculum Learning for GANs</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809701</td>
<td>0.979952</td>
<td><a href="https://www.semanticscholar.org/paper/c4bc00421ae0e96542ef35a8a3412276565485e5">0: IS DEEP REINFORCEMENT LEARNING REALLY SUPER-</a></td>
</tr>
<tr>
<td>0</td>
<td>0.807782</td>
<td>0.978188</td>
<td><a href="https://www.semanticscholar.org/paper/c615d39ad8ddaff0bcec411ac62028dff7e877e9">2: Adversarially Trained Actor Critic for Offline Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.807072</td>
<td>0.874351</td>
<td><a href="https://www.semanticscholar.org/paper/a2141a5ec0c65ea0a9861ae562f4c9fb8020d197">290: DARLA: Improving Zero-Shot Transfer in Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.803948</td>
<td>0.979597</td>
<td><a href="https://www.semanticscholar.org/paper/0eb1a4c84bf6c2c96decfe53c1e9899c2fb0b7ce">24: Learning from Suboptimal Demonstration via Self-Supervised Reward Regression</a></td>
</tr>
<tr>
<td>0</td>
<td>0.699534</td>
<td>0.994331</td>
<td><a href="https://www.semanticscholar.org/paper/ec2b345267d69e8b4dca550efcd0948e0352acd9">93: Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.762707</td>
<td>0.993849</td>
<td><a href="https://www.semanticscholar.org/paper/300529850ca8e8ad6633a2b566206bf7f2a38fd9">1: Evolving Curricula with Regret-Based Environment Design</a></td>
</tr>
<tr>
<td>0</td>
<td>0.720011</td>
<td>0.993085</td>
<td><a href="https://www.semanticscholar.org/paper/c335ff618991f0a4cdde09271284172a7e5f6b7f">243: Emergent Complexity via Multi-Agent Competition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.771714</td>
<td>0.993025</td>
<td><a href="https://www.semanticscholar.org/paper/ff3f23189c3a7fed677a868ddfc22a02bb6e4ef8">21: Procedural Level Generation Improves Generality of Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.635944</td>
<td>0.992077</td>
<td><a href="https://www.semanticscholar.org/paper/a9a3ed69c94a3e1c08ef1f833d9199f57736238b">297: DeepMind Control Suite</a></td>
</tr>
<tr>
<td>0</td>
<td>0.730523</td>
<td>0.991537</td>
<td><a href="https://www.semanticscholar.org/paper/75f425319694047f763f02f2a07912cd5621cfa4">2: Evaluating the progress of Deep Reinforcement Learning in the real world: aligning domain-agnostic and domain-specific research</a></td>
</tr>
<tr>
<td>0</td>
<td>0.686108</td>
<td>0.991367</td>
<td><a href="https://www.semanticscholar.org/paper/2d0e625fca0efc079bb523baf1ff78c46ff4916e">21: Rotation, Translation, and Cropping for Zero-Shot Generalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763631</td>
<td>0.991338</td>
<td><a href="https://www.semanticscholar.org/paper/b614ce626cc74bc301b00dc22b74a5d9fcfe6c17">0: Maximum Entropy Population Based Training for Zero-Shot Human-AI Coordination</a></td>
</tr>
<tr>
<td>0</td>
<td>0.650196</td>
<td>0.990787</td>
<td><a href="https://www.semanticscholar.org/paper/a7b3b89ef299261e172e8f47a263dee05511e7b4">73: Stochastic Grounded Action Transformation for Robot Learning in Simulation</a></td>
</tr>
</table></html>
