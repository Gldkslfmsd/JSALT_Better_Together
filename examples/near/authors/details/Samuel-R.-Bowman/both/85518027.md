<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/5e9c85235210b59a16bdd84b444a904ae271f7e7">201: On Measuring Social Biases in Sentence Encoders</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792585</td>
<td>0.921896</td>
<td><a href="https://www.semanticscholar.org/paper/d505eb794676927e919bcfeafdd1680a4bf10229">54: Hurtful words: quantifying biases in clinical contextual word embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.773294</td>
<td>0.974291</td>
<td><a href="https://www.semanticscholar.org/paper/b6ffd8ed6a0d35bce6339492fb0e776fe75a04c8">12: Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them</a></td>
</tr>
<tr>
<td>0</td>
<td>0.771197</td>
<td>0.980524</td>
<td><a href="https://www.semanticscholar.org/paper/94cf3f2c4410fcb06a90abebd99f7113c69e1ed9">318: Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them</a></td>
</tr>
<tr>
<td>1</td>
<td>0.762073</td>
<td>0.996088</td>
<td><a href="https://www.semanticscholar.org/paper/e235ad7dcf6e97cd372f09724dc947c5b1efac79">188: Gender Bias in Contextualized Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.760096</td>
<td>0.857867</td>
<td><a href="https://www.semanticscholar.org/paper/4e75c3863e78c86d96619098209e40cd7962ef1c">9: Gender Bias in Multilingual Neural Machine Translation: The Architecture Matters</a></td>
</tr>
<tr>
<td>0</td>
<td>0.759399</td>
<td>0.743087</td>
<td><a href="https://www.semanticscholar.org/paper/9c347056ab9cb42b19e471129e0961c1ed7126a2">4: ValNorm Quantifies Semantics to Reveal Consistent Valence Biases Across Languages and Over Centuries</a></td>
</tr>
<tr>
<td>0</td>
<td>0.749268</td>
<td>0.890235</td>
<td><a href="https://www.semanticscholar.org/paper/b5d7a19bd0bae10917a8e294960fdacf224d64fe">499: Word embeddings quantify 100 years of gender and ethnic stereotypes</a></td>
</tr>
<tr>
<td>0</td>
<td>0.742904</td>
<td>0.575519</td>
<td><a href="https://www.semanticscholar.org/paper/6aa8a777177dd1a730214bcdcf38cad0d77a7150">80: Comparative study of LSA vs Word2vec embeddings in small corpora: a case study in dreams database</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741977</td>
<td>0.713937</td>
<td><a href="https://www.semanticscholar.org/paper/a01f3039dd2ef75b3db08e4cedd0fcf7139f465c">31: Word2Sense: Sparse Interpretable Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792315</td>
<td>0.997574</td>
<td><a href="https://www.semanticscholar.org/paper/a2ce1fb96c0b78bee18bb2cb2c3d55dc48d54cbd">138: Measuring Bias in Contextualized Word Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.715991</td>
<td>0.995542</td>
<td><a href="https://www.semanticscholar.org/paper/2cc0e605470d3ac20aad82c73560b888ecc449cd">12: Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.712219</td>
<td>0.995482</td>
<td><a href="https://www.semanticscholar.org/paper/67ad491b16bf77e9a54a8b8b1dc23dadc5545467">2: Measuring Fairness with Biased Rulers: A Survey on Quantifying Biases in Pretrained Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.789873</td>
<td>0.995463</td>
<td><a href="https://www.semanticscholar.org/paper/69accd35f2ae56aa71ceaa5abeb814fcedc8a58e">68: Evaluating the Underlying Gender Bias in Contextualized Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>-1.000000</td>
<td>0.995070</td>
<td><a href="https://www.semanticscholar.org/paper/effe8668b9ac4f22c8bbc421ba87df102cb807b0">0: Measuring Gender Bias in Contextualized Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.704395</td>
<td>0.994507</td>
<td><a href="https://www.semanticscholar.org/paper/2972ad9cd2f5a8efaffce15fc527e1a8644b081a">3: Pipelines for Social Bias Testing of Large Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.844215</td>
<td>0.994409</td>
<td><a href="https://www.semanticscholar.org/paper/039b1c1210c437f3b3ce6e0275ee2137bf5b951c">86: Assessing Social and Intersectional Biases in Contextualized Word Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.706481</td>
<td>0.994077</td>
<td><a href="https://www.semanticscholar.org/paper/61ca0040d81c5ed71d3f9b9e5f7b528275048440">21: Debiasing Pre-trained Contextualised Embeddings</a></td>
</tr>
</table></html>
