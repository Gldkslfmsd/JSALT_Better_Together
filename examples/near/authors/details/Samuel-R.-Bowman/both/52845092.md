<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/26f7305e4cf293b3daa672f0f75c1b0bac1e873a">102: Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis</a></td>
</tr>
<tr>
<td>0</td>
<td>0.925395</td>
<td>0.941054</td>
<td><a href="https://www.semanticscholar.org/paper/7cdb46dd8ba4440a8e3859a001fd38da93fbba4a">12: Language Modeling Teaches You More than Translation Does : Lessons Learned Through Auxiliary Task Analysis</a></td>
</tr>
<tr>
<td>0</td>
<td>0.843618</td>
<td>0.983460</td>
<td><a href="https://www.semanticscholar.org/paper/b2e8906d0c2999e3b6cbb5bfd7dc687f1a0a751c">0: Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.842471</td>
<td>0.983460</td>
<td><a href="https://www.semanticscholar.org/paper/763887f01361501b77615ce6baea5b9731e0b850">0: Pretraining with Synthetic Language: Studying Transferable Knowledge in Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.838092</td>
<td>0.827825</td>
<td><a href="https://www.semanticscholar.org/paper/df8108f1f803c92e6d00d1244a355a35c3d64fa6">5: Structural Supervision Improves Few-Shot Learning and Syntactic Generalization in Neural Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.832964</td>
<td>0.908667</td>
<td><a href="https://www.semanticscholar.org/paper/babf55e17591ea977e3f88d46dfe757a9ae0fdf2">21: Scalable Syntax-Aware Language Models Using Knowledge Distillation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.825937</td>
<td>0.476744</td>
<td><a href="https://www.semanticscholar.org/paper/65162439e07d9fde14aa5e3b1942034a2bc564c1">0: Survey Paper : Improving Neural Language Modeling with Linguistic Annotation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.821660</td>
<td>0.982447</td>
<td><a href="https://www.semanticscholar.org/paper/e2587eddd57bc4ba286d91b27c185083f16f40ee">484: What do you learn from context? Probing for sentence structure in contextualized word representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.805136</td>
<td>0.971671</td>
<td><a href="https://www.semanticscholar.org/paper/a7f9b9c10e405d18b143b4fc9cf84f9879d7e9ff">0: Syntax-guided Contrastive Learning for Pre-trained Language Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.799804</td>
<td>0.960498</td>
<td><a href="https://www.semanticscholar.org/paper/8e88cd6a52fb51a46ca5d80b75cc22ad959f0321">2: Does Vision-and-Language Pretraining Improve Lexical Grounding?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.749297</td>
<td>0.995154</td>
<td><a href="https://www.semanticscholar.org/paper/e083905b0fc1cf82937630cb8d69093b39e4cf98">17: Evaluating Language Model Finetuning Techniques for Low-resource Languages</a></td>
</tr>
<tr>
<td>0</td>
<td>0.749591</td>
<td>0.993847</td>
<td><a href="https://www.semanticscholar.org/paper/c41516420ddbd0f29e010ca259a74c1fc2da0466">534: What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties</a></td>
</tr>
<tr>
<td>0</td>
<td>0.755468</td>
<td>0.992864</td>
<td><a href="https://www.semanticscholar.org/paper/9101f9c5087155ba04f866bd4837ff4c9d855a0d">12: Paradigm Shift in Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.808183</td>
<td>0.992447</td>
<td><a href="https://www.semanticscholar.org/paper/efef34c1caef102ad5cc052642d75beaaf5adcaf">82: Deep RNNs Encode Soft Hierarchical Syntax</a></td>
</tr>
<tr>
<td>0</td>
<td>0.716575</td>
<td>0.991828</td>
<td><a href="https://www.semanticscholar.org/paper/70087677fd1a6309829b42968934575d05a95f92">3: What do pre-trained code models know about code?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.664726</td>
<td>0.991481</td>
<td><a href="https://www.semanticscholar.org/paper/472a5227279b45f25508017816af34e3cb3ac0d7">118: Semantic Parsing for Task Oriented Dialog using Hierarchical Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.747121</td>
<td>0.991244</td>
<td><a href="https://www.semanticscholar.org/paper/3f9df96b26c42dea6dd6cad64557a3b7d698ea90">58: MultiFiT: Efficient Multi-lingual Language Model Fine-tuning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.798613</td>
<td>0.990995</td>
<td><a href="https://www.semanticscholar.org/paper/c644956d5cfdb7ad7ea24a420608b9b58c148e3d">133: Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog</a></td>
</tr>
<tr>
<td>0</td>
<td>0.632308</td>
<td>0.990856</td>
<td><a href="https://www.semanticscholar.org/paper/49400b3a3ea01772e321e3e010b7b891c3d6cb88">27: Extracting Syntactic Trees from Transformer Encoder Self-Attentions</a></td>
</tr>
</table></html>
