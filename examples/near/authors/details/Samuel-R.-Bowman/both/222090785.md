<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/645bd6eadc247989abc5e0b0aa0be79ec8b11ea6">82: CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.831507</td>
<td>0.978959</td>
<td><a href="https://www.semanticscholar.org/paper/167a8de25cdd67eaaac4585ba006d2b6d8e4fd76">0: Masked Language Models as Stereotype Detectors?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.779322</td>
<td>0.851466</td>
<td><a href="https://www.semanticscholar.org/paper/20328647c38282088dc9dddedcb2e5bdaeeeea78">0: Text Style Transfer for Bias Mitigation using Masked Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.774572</td>
<td>0.990743</td>
<td><a href="https://www.semanticscholar.org/paper/7d5c661fa9a4255ee087e861f820564ea2e2bd6b">5: BBQ: A hand-built bias benchmark for question answering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.734782</td>
<td>0.803595</td>
<td><a href="https://www.semanticscholar.org/paper/b5d7a19bd0bae10917a8e294960fdacf224d64fe">499: Word embeddings quantify 100 years of gender and ethnic stereotypes</a></td>
</tr>
<tr>
<td>0</td>
<td>0.720696</td>
<td>0.021443</td>
<td><a href="https://www.semanticscholar.org/paper/5800cfdc0b5c3211c3a86e8c47a97df99476de09">3: Progress in Regression : Why Natural Language Data Calls For Mixed-Effects Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.719436</td>
<td>0.107748</td>
<td><a href="https://www.semanticscholar.org/paper/0c8322e921d5da17bb20e494da32e5c78d48f7fb">0: An examination of age-related stereotypes and the linguistic intergroup bias using two measures</a></td>
</tr>
<tr>
<td>0</td>
<td>0.719291</td>
<td>0.048518</td>
<td><a href="https://www.semanticscholar.org/paper/e83a937384b865468594758f88247d9c336f93a4">1: Gender representation in linguistic example sentences</a></td>
</tr>
<tr>
<td>0</td>
<td>0.719096</td>
<td>0.490834</td>
<td><a href="https://www.semanticscholar.org/paper/bcb9830e54f132d4e3f5bcf080add08de22c9a8d">2: Language use shapes cultural norms: Large scale evidence from gender</a></td>
</tr>
<tr>
<td>0</td>
<td>0.708781</td>
<td>0.307740</td>
<td><a href="https://www.semanticscholar.org/paper/34117906199461434af69909c27b91f8ade18141">12: Social B(eye)as: Human and Machine Descriptions of People Images</a></td>
</tr>
<tr>
<td>0</td>
<td>0.750378</td>
<td>0.997974</td>
<td><a href="https://www.semanticscholar.org/paper/4f76d26369ee573cb03db4b9f6e4ab5d61806095">19: HONEST: Measuring Hurtful Sentence Completion in Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.764207</td>
<td>0.997805</td>
<td><a href="https://www.semanticscholar.org/paper/f72983cef733670d6915e37383257f548b5a3365">22: UNQOVERing Stereotypical Biases via Underspecified Questions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.804668</td>
<td>0.997397</td>
<td><a href="https://www.semanticscholar.org/paper/babeda48b10a4d638252118f2238d05a06f4ec55">141: StereoSet: Measuring stereotypical bias in pretrained language models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.753039</td>
<td>0.997170</td>
<td><a href="https://www.semanticscholar.org/paper/de6807676d8171472ed6cf421c4e4ed3cbb47699">2: An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.801021</td>
<td>0.996472</td>
<td><a href="https://www.semanticscholar.org/paper/8fa0de4920c8edcb1fea698ff3463a347771d889">17: Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.653859</td>
<td>0.995925</td>
<td><a href="https://www.semanticscholar.org/paper/d9424371662717c8981eef3d501d7ce59c66ce77">2: On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.658071</td>
<td>0.995285</td>
<td><a href="https://www.semanticscholar.org/paper/4888102774ad93140391f3a26af0f54cfba5ec34">33: Human vs. Muppet: A Conservative Estimate of Human Performance on the GLUE Benchmark</a></td>
</tr>
<tr>
<td>0</td>
<td>0.738799</td>
<td>0.995230</td>
<td><a href="https://www.semanticscholar.org/paper/4d96bfa3a4c283d6fcd71122897c2cb8ee1c886d">21: “You Are Grounded!”: Latent Name Artifacts in Pre-trained Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.672216</td>
<td>0.995140</td>
<td><a href="https://www.semanticscholar.org/paper/bead3588b51321727cfa71cec6543ec1cd65a42f">15: Gendered Ambiguous Pronoun (GAP) Shared Task at the Gender Bias in NLP Workshop 2019</a></td>
</tr>
</table></html>
