<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/b56e2e7b93be127c953b6ad18230d5905051d23b">118: BLiMP: A Benchmark of Linguistic Minimal Pairs for English</a></td>
</tr>
<tr>
<td>0</td>
<td>0.810450</td>
<td>0.445026</td>
<td><a href="https://www.semanticscholar.org/paper/0dd3fef8441ae15dc6bfdb9d9b3c05d3cf5755c6">12: Developing an interlingual translation lexicon using WordNets and Grammatical Framework</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809884</td>
<td>0.529334</td>
<td><a href="https://www.semanticscholar.org/paper/01e53c7bf8a1f82f3ff76c8f02e0b87bebc9de94">4: Sentence Realization with Unlexicalized Tree Linearization Grammars</a></td>
</tr>
<tr>
<td>0</td>
<td>0.808457</td>
<td>0.228515</td>
<td><a href="https://www.semanticscholar.org/paper/5cadc631fd9eb40bbdb3b4f7e34882622ab6a54b">19: An Architecture For A Universal Lexicon: A Case Study on Shared Syntactic Information in Japanese, Hindi, Bengali, Greek, and English</a></td>
</tr>
<tr>
<td>0</td>
<td>0.807655</td>
<td>0.502159</td>
<td><a href="https://www.semanticscholar.org/paper/aa2b0d4c29fa1b9dd3676d21aeb731f8066c07f4">1: Automatic Construction of Morphologically Motivated Translation Models for Highly Inflected, Low-Resource Languages</a></td>
</tr>
<tr>
<td>0</td>
<td>0.803593</td>
<td>0.276767</td>
<td><a href="https://www.semanticscholar.org/paper/c3f5cc5f00a8aef8eff1aeb0964dd9f5f25f3843">12: Entry Generation by Analogy – Encoding New Words for Morphological Lexicons</a></td>
</tr>
<tr>
<td>0</td>
<td>0.799915</td>
<td>0.166416</td>
<td><a href="https://www.semanticscholar.org/paper/89b7e66c2842416ec5265f7761f0a08d20951137">2: Formatting Lexical Entries : Interface Optionality and Zero</a></td>
</tr>
<tr>
<td>0</td>
<td>0.799347</td>
<td>-0.010142</td>
<td><a href="https://www.semanticscholar.org/paper/1093c5cbb8833ecd59d630003622333d1f3ab19b">0: Word Order in English and Albanian Declarative Sentences</a></td>
</tr>
<tr>
<td>0</td>
<td>0.794070</td>
<td>0.519526</td>
<td><a href="https://www.semanticscholar.org/paper/e9d8e72da81ef9c13da8ac969c4c753331918a5d">0: Evaluating the suitability of human-oriented text simplification for machine translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793165</td>
<td>0.688138</td>
<td><a href="https://www.semanticscholar.org/paper/c9e5456e328be30de60a87b742e733b5dca004b9">31: HUME: Human UCCA-Based Evaluation of Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.756572</td>
<td>0.992715</td>
<td><a href="https://www.semanticscholar.org/paper/8c25f38044b69f54233803c280677c3f8d547e9f">0: Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.771413</td>
<td>0.992004</td>
<td><a href="https://www.semanticscholar.org/paper/ab8ea4bf7b58edea54edbd179c48bbc24b8aeadf">0: Assessing the Syntactic Capabilities of Transformer-based Multilingual Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.696878</td>
<td>0.991788</td>
<td><a href="https://www.semanticscholar.org/paper/d93a5d3116ffba494c3cff322f300fc56d1081d4">1: Look at that! BERT can be easily distracted from paying attention to morphosyntax</a></td>
</tr>
<tr>
<td>0</td>
<td>0.804978</td>
<td>0.991481</td>
<td><a href="https://www.semanticscholar.org/paper/3cd331c997e90f737810aad6fcce4d993315189f">67: Investigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs</a></td>
</tr>
<tr>
<td>0</td>
<td>0.807793</td>
<td>0.991350</td>
<td><a href="https://www.semanticscholar.org/paper/2e3a7760c543a181b47245ffece91bff027c43c9">27: UnNatural Language Inference</a></td>
</tr>
<tr>
<td>0</td>
<td>0.758473</td>
<td>0.991016</td>
<td><a href="https://www.semanticscholar.org/paper/7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8">4: AND does not mean OR: Using Formal Languages to Study Language Models’ Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.794432</td>
<td>0.989435</td>
<td><a href="https://www.semanticscholar.org/paper/2bbf022d9a61f124a01da306a7ddd49d1b93abae">0: When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it</a></td>
</tr>
<tr>
<td>0</td>
<td>0.828764</td>
<td>0.989101</td>
<td><a href="https://www.semanticscholar.org/paper/d21830c081094fbf1b2eaf8513cca0459069cd35">5: CLiMP: A Benchmark for Chinese Language Model Evaluation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.768734</td>
<td>0.988883</td>
<td><a href="https://www.semanticscholar.org/paper/3962f108081b22c7e54b413f47ba6f2c16f2cc05">6: Frequency Effects on Syntactic Rule Learning in Transformers</a></td>
</tr>
</table></html>
