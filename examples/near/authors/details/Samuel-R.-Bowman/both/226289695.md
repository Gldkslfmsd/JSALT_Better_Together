<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/31392ad8722d9c66181b621936e2013199e02edc">31: When Do You Need Billions of Words of Pretraining Data?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.816760</td>
<td>0.745354</td>
<td><a href="https://www.semanticscholar.org/paper/fb29af99e4ef690bcde788442b087fbac087f533">0: LANGUAGE MODELING TEACHES YOU MORE SYNTAX</a></td>
</tr>
<tr>
<td>0</td>
<td>0.804468</td>
<td>0.947458</td>
<td><a href="https://www.semanticscholar.org/paper/0b46915313f95a0112564fdfd2baaa2bcf36565a">27: Syntactic Structure Distillation Pretraining for Bidirectional Encoders</a></td>
</tr>
<tr>
<td>0</td>
<td>0.802251</td>
<td>0.979552</td>
<td><a href="https://www.semanticscholar.org/paper/acac699d02a972f58b091bfbef7518f0e61c8225">1: Frustratingly Simple Pretraining Alternatives to Masked Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.797078</td>
<td>0.906021</td>
<td><a href="https://www.semanticscholar.org/paper/010cb53244852d12acc313e7b7803547d02912aa">0: Enhancing Transformers with Gradient Boosted Decision Trees for NLI Fine-Tuning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793395</td>
<td>0.503826</td>
<td><a href="https://www.semanticscholar.org/paper/569b731b2cf30c63d1919a38c875e95e79e278ab">7: Learning with imperfect supervision for language understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.789428</td>
<td>0.647750</td>
<td><a href="https://www.semanticscholar.org/paper/15be357a094d22dd7895961cc2fab83f577a9602">25: The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.783052</td>
<td>0.549144</td>
<td><a href="https://www.semanticscholar.org/paper/a2f7b6508158985c88019c6544b70c52a8fa784a">9: Tabula Nearly Rasa: Probing the Linguistic Knowledge of Character-level Neural Language Models Trained on Unsegmented Text</a></td>
</tr>
<tr>
<td>0</td>
<td>0.782365</td>
<td>0.648844</td>
<td><a href="https://www.semanticscholar.org/paper/2b5d6aff7c19d879c12e9757ba8a65967d27da93">0: Predictive Representation Learning for Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.777673</td>
<td>0.993405</td>
<td><a href="https://www.semanticscholar.org/paper/529edafa160a77901bec123cf8858e6c08f6cd06">14: When does pretraining help?: assessing self-supervised learning for law and the CaseHOLD dataset of 53,000+ legal holdings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792142</td>
<td>0.998242</td>
<td><a href="https://www.semanticscholar.org/paper/673e970fd835c7dd1bb1e071c5a37e9df99b7c8e">43: Intermediate-Task Transfer Learning with Pretrained Models for Natural Language Understanding: When and Why Does It Work?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.765419</td>
<td>0.997545</td>
<td><a href="https://www.semanticscholar.org/paper/1cde1aa4f7bcebc47b35518cec452893ea6b824c">2: Investigating Transfer Learning in Multilingual Pre-trained Language Models through Chinese Natural Language Inference</a></td>
</tr>
<tr>
<td>0</td>
<td>0.614849</td>
<td>0.997332</td>
<td><a href="https://www.semanticscholar.org/paper/07c1c2429b63fefdae41eb546c31b40de2a880f7">26: INFOTABS: Inference on Tables as Semi-structured Data</a></td>
</tr>
<tr>
<td>0</td>
<td>0.782511</td>
<td>0.997249</td>
<td><a href="https://www.semanticscholar.org/paper/0672f88d5dc762002b515ca4a0a9f101017fea35">20: Probing Across Time: What Does RoBERTa Know and When?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.743170</td>
<td>0.996732</td>
<td><a href="https://www.semanticscholar.org/paper/616610e0b0a31ab4bac1c64fd0b65c2572185522">12: BERTnesia: Investigating the capture and forgetting of knowledge in BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.757904</td>
<td>0.996689</td>
<td><a href="https://www.semanticscholar.org/paper/79e5447a394ad178aa0fdaef1d2f1ecf9bf07e88">49: Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge</a></td>
</tr>
<tr>
<td>0</td>
<td>0.719561</td>
<td>0.996517</td>
<td><a href="https://www.semanticscholar.org/paper/45436d6f52832154bf7a90ae02ce8bc302802b4d">16: How Additional Knowledge can Improve Natural Language Commonsense Question Answering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.754774</td>
<td>0.996502</td>
<td><a href="https://www.semanticscholar.org/paper/b769b629c8de35b16735214251d6b4e99cb55762">22: Generating Datasets with Pretrained Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.734811</td>
<td>0.996434</td>
<td><a href="https://www.semanticscholar.org/paper/17d4681b29b79c4ee5029ae39acabfdf9946bd77">4: FiD-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation</a></td>
</tr>
</table></html>
