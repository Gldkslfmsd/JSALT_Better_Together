<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/06a1bf4a7333bbc78dbd7470666b33bd9e26882b">70: Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.884886</td>
<td>0.985285</td>
<td><a href="https://www.semanticscholar.org/paper/256623ff025f36d343588bcd0b966c1fd26afcf8">28: Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.874396</td>
<td>0.830141</td>
<td><a href="https://www.semanticscholar.org/paper/fb29af99e4ef690bcde788442b087fbac087f533">0: LANGUAGE MODELING TEACHES YOU MORE SYNTAX</a></td>
</tr>
<tr>
<td>0</td>
<td>0.819948</td>
<td>-0.022175</td>
<td>NA:59315798</td>
</tr>
<tr>
<td>0</td>
<td>0.810143</td>
<td>0.977690</td>
<td><a href="https://www.semanticscholar.org/paper/e816f788767eec6a8ef0ea9eddd0e902435d4271">720: Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809716</td>
<td>0.957724</td>
<td><a href="https://www.semanticscholar.org/paper/65650c58498c34a019bdbd422c3737a512cc4b28">79: Do Massively Pretrained Language Models Make Better Storytellers?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.804135</td>
<td>0.965880</td>
<td><a href="https://www.semanticscholar.org/paper/209f9bde2dee7cf1677801586562ffe56d435d38">96: Learning How to Ask: Querying LMs with Mixtures of Soft Prompts</a></td>
</tr>
<tr>
<td>0</td>
<td>0.803913</td>
<td>0.921591</td>
<td><a href="https://www.semanticscholar.org/paper/1b553c34270543a36ca12784821a7817b36e66ad">19: Grammatical Analysis of Pretrained Sentence Encoders with Acceptability Judgments</a></td>
</tr>
<tr>
<td>0</td>
<td>0.798957</td>
<td>0.978136</td>
<td><a href="https://www.semanticscholar.org/paper/016760dc4a05489ddf5dbb48aecbb49e214e1b71">56: Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793088</td>
<td>0.844851</td>
<td><a href="https://www.semanticscholar.org/paper/2368f712f12ab91218e3ddbaaa662830832e4e15">10: Pretrained Language Models for Document-Level Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.691817</td>
<td>0.997601</td>
<td><a href="https://www.semanticscholar.org/paper/cf8c493079702ec420ab4fc9c0fabb56b2a16c84">271: SciTaiL: A Textual Entailment Dataset from Science Question Answering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.767620</td>
<td>0.997111</td>
<td><a href="https://www.semanticscholar.org/paper/8e00d81ff7b1656c621f64fe72fff2356bacb29f">20: Transfer Fine-Tuning: A BERT Case Study</a></td>
</tr>
<tr>
<td>0</td>
<td>0.704624</td>
<td>0.996757</td>
<td><a href="https://www.semanticscholar.org/paper/1cb3f6d545b68db3e7fc6055dcf44099c3ac4672">46: Structured Prediction as Translation between Augmented Natural Languages</a></td>
</tr>
<tr>
<td>0</td>
<td>0.767844</td>
<td>0.996686</td>
<td><a href="https://www.semanticscholar.org/paper/c846cbb24866af99a8d02d4c73aa4d7dd1831538">46: XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741105</td>
<td>0.996207</td>
<td><a href="https://www.semanticscholar.org/paper/50170a78f22e03b08e62a20e6cf0e36133a8bdad">9: A logical-based corpus for cross-lingual evaluation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.717669</td>
<td>0.996115</td>
<td><a href="https://www.semanticscholar.org/paper/fa025e5d117929361bcf798437957762eb5bb6d4">324: Zero-Shot Relation Extraction via Reading Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.708256</td>
<td>0.996026</td>
<td><a href="https://www.semanticscholar.org/paper/e092ecf56fcca38d0cd6fe9e1e6b11c380f6c286">30: A Survey on Contextual Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.679612</td>
<td>0.995560</td>
<td><a href="https://www.semanticscholar.org/paper/104715e1097b7ebee436058bfd9f45540f269845">1191: Reading Wikipedia to Answer Open-Domain Questions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.802271</td>
<td>0.995480</td>
<td><a href="https://www.semanticscholar.org/paper/1321419b4e093ebd5064cd9c44b61c0d8b6c361d">62: Probing What Different NLP Tasks Teach Machines about Function Word Comprehension</a></td>
</tr>
</table></html>
