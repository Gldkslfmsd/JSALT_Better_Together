<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/9e594ae4ae9c38b6495810a8872f513ae19be29c">85: Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?</a></td>
</tr>
<tr>
<td>1</td>
<td>0.988400</td>
<td>0.999062</td>
<td><a href="https://www.semanticscholar.org/paper/673e970fd835c7dd1bb1e071c5a37e9df99b7c8e">43: Intermediate-Task Transfer Learning with Pretrained Models for Natural Language Understanding: When and Why Does It Work?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.837257</td>
<td>0.996614</td>
<td><a href="https://www.semanticscholar.org/paper/5e0cffc51e8b64a8f11326f955fa4b4f1803e3be">157: oLMpics-On What Language Model Pre-training Captures</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800667</td>
<td>0.985634</td>
<td><a href="https://www.semanticscholar.org/paper/40b4d98588719407fb72a014ab79e4145695654b">0: Quantifying Adaptability in Pre-trained Language Models with 500 Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800621</td>
<td>0.980852</td>
<td><a href="https://www.semanticscholar.org/paper/a53b834aaa696fc719b4f5641e9c5a76c940be0f">0: Task-Adaptive Pretraining , Domain Sampling , and Data Augmentation Improve Generalized Question Answering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.797384</td>
<td>0.992792</td>
<td><a href="https://www.semanticscholar.org/paper/87e02a265606f31e65986f3c1c448a3e3a3a066e">19: Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.788995</td>
<td>-0.007509</td>
<td>NA:198971011</td>
</tr>
<tr>
<td>0</td>
<td>0.788068</td>
<td>0.928044</td>
<td><a href="https://www.semanticscholar.org/paper/7d070735d5aeaa511e372e510b69537aa62067a6">2: Multi-Task Learning using Dynamic Task Weighting for Conversational Question Answering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.784769</td>
<td>0.963260</td>
<td><a href="https://www.semanticscholar.org/paper/5166b4268a5f7dab3feb65e9ab006bebfa8e5001">0: Towards a Robust Question Answering System through Domain-adaptive Pretraining and Data Augmentation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.778236</td>
<td>0.970367</td>
<td><a href="https://www.semanticscholar.org/paper/2d82ee05b132d4681c3bd517afc17d608fe6e525">3: Simple Local Attentions Remain Competitive for Long-Context Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.719380</td>
<td>0.998466</td>
<td><a href="https://www.semanticscholar.org/paper/636904d91d9dd1a641a595d9578ba7640f35aa74">120: MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.735337</td>
<td>0.998436</td>
<td><a href="https://www.semanticscholar.org/paper/18318b10e7c2dd4ad292208f4399eb1d4dca5768">106: CLUE: A Chinese Language Understanding Evaluation Benchmark</a></td>
</tr>
<tr>
<td>0</td>
<td>0.717086</td>
<td>0.997739</td>
<td><a href="https://www.semanticscholar.org/paper/00b30ed463625da04166eb78ca617539b41a9846">50: jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.725564</td>
<td>0.997696</td>
<td><a href="https://www.semanticscholar.org/paper/708dcd8456426cd609c89a86344e0007c04c80bf">38: X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.735670</td>
<td>0.997597</td>
<td><a href="https://www.semanticscholar.org/paper/b769b629c8de35b16735214251d6b4e99cb55762">22: Generating Datasets with Pretrained Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.728163</td>
<td>0.997594</td>
<td><a href="https://www.semanticscholar.org/paper/14489ec7893e373a0dcc9555c52b99b2b3a429f6">100: Are All Languages Created Equal in Multilingual BERT?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.746893</td>
<td>0.997508</td>
<td><a href="https://www.semanticscholar.org/paper/616610e0b0a31ab4bac1c64fd0b65c2572185522">12: BERTnesia: Investigating the capture and forgetting of knowledge in BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.762531</td>
<td>0.997414</td>
<td><a href="https://www.semanticscholar.org/paper/bc87279d4b32a425377ff18ab63f7ecf95ff228c">35: Rethinking embedding coupling in pre-trained language models</a></td>
</tr>
</table></html>
