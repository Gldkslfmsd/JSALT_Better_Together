<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/efef34c1caef102ad5cc052642d75beaaf5adcaf">82: Deep RNNs Encode Soft Hierarchical Syntax</a></td>
</tr>
<tr>
<td>0</td>
<td>0.825583</td>
<td>0.796890</td>
<td><a href="https://www.semanticscholar.org/paper/607e528b054d28e9d963dcda245161f7547f63ce">21: Two Local Models for Neural Constituent Parsing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.823986</td>
<td>-0.022394</td>
<td><a href="https://www.semanticscholar.org/paper/1a34bab2c022a5fe381785d70edc6e1e175c4624">0: DIBERT: Dependency Injected Bidirectional Encoder Representations from Transformers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.821382</td>
<td>0.985659</td>
<td><a href="https://www.semanticscholar.org/paper/060ff1aad5619a7d6d6cdfaf8be5da29bff3808c">274: Linguistically-Informed Self-Attention for Semantic Role Labeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.820681</td>
<td>0.806152</td>
<td><a href="https://www.semanticscholar.org/paper/a1f88548fa9225847c49f9994a9a1decb1fc46b7">1: Compositional Neural Machine Translation by Removing the Lexicon from Syntax</a></td>
</tr>
<tr>
<td>0</td>
<td>0.820587</td>
<td>0.613874</td>
<td><a href="https://www.semanticscholar.org/paper/1453892cf162d193e58912caf67e191485a65b03">8: Temporal hierarchies in multilayer gated recurrent neural networks for language models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811643</td>
<td>0.933618</td>
<td><a href="https://www.semanticscholar.org/paper/8f1cb428c2db0583386b260d0b1f77191f6916a1">2: Discovering Useful Sentence Representations from Large Pretrained Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.810924</td>
<td>0.945833</td>
<td><a href="https://www.semanticscholar.org/paper/d2ad6ae5f57844c4474cd3f318c3cdc469994fae">45: Structural Embedding of Syntactic Trees for Machine Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.810429</td>
<td>0.752229</td>
<td><a href="https://www.semanticscholar.org/paper/6ef294bd047c4f92a643c00a772508b1f5f38da4">0: Neural Networks for Natural Language Inference</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809881</td>
<td>0.529872</td>
<td>NA:159034485</td>
</tr>
<tr>
<td>0</td>
<td>0.678216</td>
<td>0.995310</td>
<td><a href="https://www.semanticscholar.org/paper/2af67c1063172c924a977d97d4b848651cc1617e">22: A Survey on Machine Reading Comprehension Systems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.586602</td>
<td>0.994108</td>
<td><a href="https://www.semanticscholar.org/paper/81b58944372ea10436ff7252b115e21e893d11c6">2: Enhanced Speaker-aware Multi-party Multi-turn Dialogue Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.691348</td>
<td>0.992829</td>
<td><a href="https://www.semanticscholar.org/paper/8490431f3a76fbd165d108eba938ead212a2a639">171: Stochastic Answer Networks for Machine Reading Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.808183</td>
<td>0.992447</td>
<td><a href="https://www.semanticscholar.org/paper/26f7305e4cf293b3daa672f0f75c1b0bac1e873a">102: Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis</a></td>
</tr>
<tr>
<td>0</td>
<td>0.719675</td>
<td>0.992048</td>
<td><a href="https://www.semanticscholar.org/paper/bb429a17280c2df86ac34789df880a4f728009ae">15: Probing for Referential Information in Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.708969</td>
<td>0.992030</td>
<td><a href="https://www.semanticscholar.org/paper/ad6c25a46a083e02dbfcdd4b6341945d517b5e31">0: A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.732586</td>
<td>0.991938</td>
<td><a href="https://www.semanticscholar.org/paper/c644956d5cfdb7ad7ea24a420608b9b58c148e3d">133: Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog</a></td>
</tr>
<tr>
<td>0</td>
<td>0.712080</td>
<td>0.991794</td>
<td><a href="https://www.semanticscholar.org/paper/26b47e35fe6e4260fdf7b7cc98f279a73c277494">140: Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering</a></td>
</tr>
<tr>
<td>0</td>
<td>-1.000000</td>
<td>0.991747</td>
<td><a href="https://www.semanticscholar.org/paper/ba6f9c49ead39548c0840bede896465cf5790637">0: Autoregressive Pre-training Model-Assisted Low-Resource Neural Machine Translation</a></td>
</tr>
</table></html>
