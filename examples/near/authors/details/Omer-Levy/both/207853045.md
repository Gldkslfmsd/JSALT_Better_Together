<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/3ff8d265f4351e4b1fdac5b586466bee0b5d6fff">37: Improving Transformer Models by Reordering their Sublayers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.807409</td>
<td>0.956136</td>
<td><a href="https://www.semanticscholar.org/paper/9b39de93ab9b5c74ed726d1ee8e31a927e4f6292">3: AutoTrans: Automating Transformer Design via Reinforced Architecture Search</a></td>
</tr>
<tr>
<td>0</td>
<td>0.787494</td>
<td>0.980374</td>
<td><a href="https://www.semanticscholar.org/paper/54bc3e055d05e44c010febc669e8dea394643efc">3: Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size</a></td>
</tr>
<tr>
<td>0</td>
<td>0.772969</td>
<td>0.958223</td>
<td><a href="https://www.semanticscholar.org/paper/3544650f12a05cf4ed3bf2f7e22fc5c02fcabf50">70: Pretrained Transformers as Universal Computation Engines</a></td>
</tr>
<tr>
<td>0</td>
<td>0.767787</td>
<td>0.977791</td>
<td><a href="https://www.semanticscholar.org/paper/fb486b63058925d762317992efa65e3cd6f188de">2: Leveraging redundancy in attention with Reuse Transformers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.758941</td>
<td>0.838958</td>
<td><a href="https://www.semanticscholar.org/paper/109b5c0bade0dc285153d4c7f90d42a8af06126b">5: On Biasing Transformer Attention Towards Monotonicity</a></td>
</tr>
<tr>
<td>0</td>
<td>0.756747</td>
<td>0.843562</td>
<td><a href="https://www.semanticscholar.org/paper/6a250b904965732840a75b6a13e35ac15f5cce4d">0: Compositional Generalization and Decomposition in Neural Program Synthesis</a></td>
</tr>
<tr>
<td>0</td>
<td>0.748541</td>
<td>0.877628</td>
<td><a href="https://www.semanticscholar.org/paper/5127dd9446a61e08aa1d68420ac8e4bc3f243b83">6: An Evaluation of Language-Agnostic Inner-Attention-Based Representations in Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.740064</td>
<td>-0.023142</td>
<td><a href="https://www.semanticscholar.org/paper/9d0ba4abf8e334df031b629fc4553bef75b30df4">0: Accelerating Attention through Gradient-Based Learned Runtime Pruning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.732805</td>
<td>0.853621</td>
<td><a href="https://www.semanticscholar.org/paper/fcc612d5e461c849b2beb6f46020a873612d5287">0: A text autoencoder from transformer for fast encoding language representation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.654620</td>
<td>0.994209</td>
<td><a href="https://www.semanticscholar.org/paper/79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb">41: Do Transformer Modifications Transfer Across Implementations and Applications?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.739504</td>
<td>0.994199</td>
<td><a href="https://www.semanticscholar.org/paper/af679d69fcc1d0fcf0f039aba937853bcb50a8de">18: Luna: Linear Unified Nested Attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792531</td>
<td>0.993992</td>
<td><a href="https://www.semanticscholar.org/paper/054e307c1edf4b28137ffcbce980fe81f0647d20">20: Finetuning Pretrained Transformers into RNNs</a></td>
</tr>
<tr>
<td>0</td>
<td>0.786879</td>
<td>0.993662</td>
<td><a href="https://www.semanticscholar.org/paper/7c5c149699a0ba54b52cd5b9e291077f4a1f9d13">135: Synthesizer: Rethinking Self-Attention in Transformer Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.680248</td>
<td>0.993575</td>
<td><a href="https://www.semanticscholar.org/paper/1554887c6bd76c443a477b27dbcab35877787b27">10: LightSeq: A High Performance Inference Library for Transformers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.661507</td>
<td>0.993371</td>
<td><a href="https://www.semanticscholar.org/paper/34a4e6818d680875ff0bef9a76de0376118446d1">100: Sparse Sinkhorn Attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.762913</td>
<td>0.993113</td>
<td><a href="https://www.semanticscholar.org/paper/cf4a4f76017a299b7baa9faf055d7e1d9b76453b">1: Adaptive Multi-Resolution Attention with Linear Complexity</a></td>
</tr>
<tr>
<td>0</td>
<td>0.761392</td>
<td>0.992923</td>
<td><a href="https://www.semanticscholar.org/paper/1566d3de5321a27483f8dd26a2294634d43492e5">0: Linearizing Transformer with Key-Value Memory Bank</a></td>
</tr>
<tr>
<td>0</td>
<td>0.701390</td>
<td>0.992891</td>
<td><a href="https://www.semanticscholar.org/paper/3cadeded3b1d4ec5f3146064c596c95a822e7e0f">3: Adaptive Adapters: An Efficient Way to Incorporate BERT Into Neural Machine Translation</a></td>
</tr>
</table></html>
