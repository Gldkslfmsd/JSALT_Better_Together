<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/f4c018bcc8ea707b83247866bdc8ccb87cd9f5da">1561: Neural Word Embedding as Implicit Matrix Factorization</a></td>
</tr>
<tr>
<td>1</td>
<td>0.890203</td>
<td>0.989903</td>
<td><a href="https://www.semanticscholar.org/paper/cc14a56eb0361261f9294646a727dc853813c532">106: Word Embedding Revisited: A New Representation Learning and Explicit Matrix Factorization Perspective</a></td>
</tr>
<tr>
<td>0</td>
<td>0.839232</td>
<td>0.945539</td>
<td><a href="https://www.semanticscholar.org/paper/9b235727491a6057ced4248eb1f9ec0750232c44">2: Continuous Word Embedding Fusion via Spectral Decomposition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.834865</td>
<td>0.864514</td>
<td><a href="https://www.semanticscholar.org/paper/04a22a9eb76b399a78467728fb1c9cf4507a7031">3: Word Embedding With Zipfâ€™s Context</a></td>
</tr>
<tr>
<td>0</td>
<td>0.830688</td>
<td>0.923494</td>
<td><a href="https://www.semanticscholar.org/paper/1f50db5786913b43f9668f997fc4c97d9cd18730">9: Spectral Word Embedding with Negative Sampling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.817626</td>
<td>0.895877</td>
<td><a href="https://www.semanticscholar.org/paper/f59cb86bf5c9221301841130f02ff473b561bfaa">0: Unsupervised Learning of Multi-Sense Embedding with Matrix Factorization and Sparse Soft Clustering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.812409</td>
<td>0.956467</td>
<td><a href="https://www.semanticscholar.org/paper/c8f09a71fa6b2a4f5ae86296e5c22ae75414a916">114: Symmetric Pattern Based Word Embeddings for Improved Word Similarity Prediction</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811846</td>
<td>0.878289</td>
<td><a href="https://www.semanticscholar.org/paper/3f7b6fe8a850c0632c6b97b0190d26f5650e6097">1: Word2rate: training and evaluating multiple word embeddings as statistical transitions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811833</td>
<td>0.977212</td>
<td><a href="https://www.semanticscholar.org/paper/3290c8aa90285e95e302419474ba7ef96944feac">11: PSDVec: a Toolbox for Incremental and Scalable Word Embedding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.803641</td>
<td>0.004160</td>
<td><a href="https://www.semanticscholar.org/paper/a43295286e4cba02465dd198107963d8eada0124">0: Complex-Valued Vectors for Word Representation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.675465</td>
<td>0.990824</td>
<td><a href="https://www.semanticscholar.org/paper/c485fa1e053fe65621bb76bf0ab1789472e21427">25: Incremental Skip-gram Model with Negative Sampling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.639630</td>
<td>0.990279</td>
<td><a href="https://www.semanticscholar.org/paper/2ff5b6adcb22a02606afc6a77a5cc6937478750e">7: Efficient and accurate Word2Vec implementations in GPU and shared-memory multicore architectures</a></td>
</tr>
<tr>
<td>0</td>
<td>0.775301</td>
<td>0.989361</td>
<td><a href="https://www.semanticscholar.org/paper/720ad49010638bcf93de3a58689de84a047d3040">17: What the Vec? Towards Probabilistically Grounded Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.726388</td>
<td>0.989090</td>
<td><a href="https://www.semanticscholar.org/paper/c5dba6ade9795f6ba42a011b16929bcc34d4ca58">79: Hierarchical Neural Language Models for Joint Representation of Streaming Documents and their Content</a></td>
</tr>
<tr>
<td>0</td>
<td>0.750565</td>
<td>0.989018</td>
<td><a href="https://www.semanticscholar.org/paper/2012f32199adc88747d5a1b47c7b4ba1cb3cb995">1184: word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method</a></td>
</tr>
<tr>
<td>0</td>
<td>0.510141</td>
<td>0.987643</td>
<td><a href="https://www.semanticscholar.org/paper/193e0149fd1133b75c8ca4d58dcebb03046f1345">1: Generating Distributed Representation of User Movement for Extracting Detour Spots</a></td>
</tr>
<tr>
<td>0</td>
<td>0.779051</td>
<td>0.987609</td>
<td><a href="https://www.semanticscholar.org/paper/80daf3097356bfbae455a9b86a5e0f0fb0479ae3">53: Random Walks on Context Spaces: Towards an Explanation of the Mysteries of Semantic Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.694847</td>
<td>0.986629</td>
<td><a href="https://www.semanticscholar.org/paper/3b5093c165a6f7118c294fcf611b3a206ac5107d">16: word2vec Skip-Gram with Negative Sampling is a Weighted Logistic PCA</a></td>
</tr>
</table></html>
