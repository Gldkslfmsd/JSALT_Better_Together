<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/4a54d58a4b20e4f3af25cea3c188a12082a95e02">32: Transformer Feed-Forward Layers Are Key-Value Memories</a></td>
</tr>
<tr>
<td>0</td>
<td>0.774106</td>
<td>0.865495</td>
<td><a href="https://www.semanticscholar.org/paper/8890eeda67d02117a589b0ba41c69419c40c7d5e">6: Accessing Higher-level Representations in Sequential Transformers with Feedback Memory</a></td>
</tr>
<tr>
<td>0</td>
<td>0.750123</td>
<td>0.612779</td>
<td><a href="https://www.semanticscholar.org/paper/80d15b5e95b27e224838914ad06a81b0567e6e0f">9: Temporal Convolutional Attention-based Network For Sequence Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.723600</td>
<td>0.936033</td>
<td><a href="https://www.semanticscholar.org/paper/dc52b09089704ebd6f471177474bc29741c50023">25: Fast Transformer Decoding: One Write-Head is All You Need</a></td>
</tr>
<tr>
<td>0</td>
<td>0.713676</td>
<td>0.606633</td>
<td><a href="https://www.semanticscholar.org/paper/d918d3193981bd2c2b21e68ef076b0d98c829dc7">2: Sketch based Memory for Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.712116</td>
<td>0.967544</td>
<td><a href="https://www.semanticscholar.org/paper/2e14e84ccec924ed770b58108ad1d9de6f0ca295">37: BP-Transformer: Modelling Long-Range Context via Binary Partitioning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.711778</td>
<td>0.977542</td>
<td><a href="https://www.semanticscholar.org/paper/0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe">23: Hash Layers For Large Sparse Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.708681</td>
<td>0.436806</td>
<td><a href="https://www.semanticscholar.org/paper/474848cede7a23c30c3444282360a78a7ef37b28">8: Sequence to Sequence Learning in Neural Network</a></td>
</tr>
<tr>
<td>0</td>
<td>0.706172</td>
<td>0.662439</td>
<td><a href="https://www.semanticscholar.org/paper/1da75743ad30c9c9b23a534888be2dc99e52d087">2: Select, Extract and Generate: Neural Keyphrase Generation with Syntactic Guidance</a></td>
</tr>
<tr>
<td>0</td>
<td>0.704246</td>
<td>0.233966</td>
<td><a href="https://www.semanticscholar.org/paper/c1374877ae1877a16ff70a43e51b81bf5485a4fd">1: An Input Residual Connection for Simplifying Gated Recurrent Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.714297</td>
<td>0.995905</td>
<td><a href="https://www.semanticscholar.org/paper/9d7fbdb2e9817a6396992a1c92f75206689852d9">109: On Identifiability in Transformers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.813131</td>
<td>0.994762</td>
<td><a href="https://www.semanticscholar.org/paper/b832638a7c8f8e9b51b2762e36fbd29733af63b2">2: Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space</a></td>
</tr>
<tr>
<td>0</td>
<td>0.716483</td>
<td>0.994644</td>
<td><a href="https://www.semanticscholar.org/paper/044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3">458: Big Bird: Transformers for Longer Sequences</a></td>
</tr>
<tr>
<td>0</td>
<td>0.675375</td>
<td>0.994572</td>
<td><a href="https://www.semanticscholar.org/paper/71b6394ad5654f5cd0fba763768ba4e523f7bbca">800: Longformer: The Long-Document Transformer</a></td>
</tr>
<tr>
<td>0</td>
<td>0.625602</td>
<td>0.994316</td>
<td><a href="https://www.semanticscholar.org/paper/d05141dc0900140f7146bb71e1f7402cf896ea87">0: A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.638510</td>
<td>0.993826</td>
<td><a href="https://www.semanticscholar.org/paper/cc50f846ed7222698d130cddbc58ed4d547914ed">38: CPM: A Large-scale Generative Chinese Pre-trained Language Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.627783</td>
<td>0.993365</td>
<td><a href="https://www.semanticscholar.org/paper/d78aed1dac6656affa4a04cbf225ced11a83d103">285: Revealing the Dark Secrets of BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.669713</td>
<td>0.993290</td>
<td><a href="https://www.semanticscholar.org/paper/07a64686ce8e43ac475a8d820a8a9f1d87989583">427: Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned</a></td>
</tr>
<tr>
<td>0</td>
<td>0.715235</td>
<td>0.992975</td>
<td><a href="https://www.semanticscholar.org/paper/7dd61ef46abaaf8ed8d1d69f61956307b05cb9fb">1: Measuring the Mixing of Contextual Information in the Transformer</a></td>
</tr>
</table></html>
