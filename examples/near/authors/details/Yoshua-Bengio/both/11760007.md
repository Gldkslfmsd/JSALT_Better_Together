<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/878ba5458e9e51f0b341fd9117fa0b43ef4096d3">890: End-to-end attention-based large vocabulary speech recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.882227</td>
<td>0.983780</td>
<td><a href="https://www.semanticscholar.org/paper/77854e1a86835065b77b7b15ffabb34f3853f4a2">109: On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.863687</td>
<td>-0.030595</td>
<td>NA:1840795</td>
</tr>
<tr>
<td>0</td>
<td>0.863687</td>
<td>0.966860</td>
<td><a href="https://www.semanticscholar.org/paper/70fb722da267b117d3aa2b5aa56da81c702bce1a">74: A study of the recurrent neural network encoder-decoder for large vocabulary speech recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.860236</td>
<td>0.881564</td>
<td><a href="https://www.semanticscholar.org/paper/b2703d6bcc09671a028de0aabecd37e1db1c0406">29: Towards end-to-end speech recognition for Chinese Mandarin using long short-term memory recurrent neural networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.858985</td>
<td>-0.030595</td>
<td><a href="https://www.semanticscholar.org/paper/73d3bb81acb8ca87c4d458faed89b521c27b734a">0: End-to-end neural network modeling for Japanese speech recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.851147</td>
<td>0.754592</td>
<td><a href="https://www.semanticscholar.org/paper/20f3d1f45974d232e39a80201b0b82043b966eae">0: Recurrent neural network language models for automatic speech recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.843189</td>
<td>0.971704</td>
<td><a href="https://www.semanticscholar.org/paper/dc4518521ce9fd0226e52005a5aedd106e721ae1">41: Acoustic-to-Word Attention-Based Model Complemented with Character-Level CTC-Based Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.841104</td>
<td>0.928935</td>
<td><a href="https://www.semanticscholar.org/paper/612a42e2fa4c33b609aade451528d3c11989f88a">71: Self-attention Networks for Connectionist Temporal Classification in Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.840487</td>
<td>0.970756</td>
<td><a href="https://www.semanticscholar.org/paper/29ca46ff635e7967c37f94072edd778575e9a3a5">24: End-to-end Contextual Speech Recognition Using Class Language Models and a Token Passing Decoder</a></td>
</tr>
<tr>
<td>0</td>
<td>0.850635</td>
<td>0.996580</td>
<td><a href="https://www.semanticscholar.org/paper/47d2dc34e1d02a8109f5c04bb6939725de23716d">370: End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results</a></td>
</tr>
<tr>
<td>0</td>
<td>0.823127</td>
<td>0.995493</td>
<td><a href="https://www.semanticscholar.org/paper/dc555e8156c956f823587ebbff018863e6d2a95e">358: Listen, Attend and Spell</a></td>
</tr>
<tr>
<td>0</td>
<td>0.833575</td>
<td>0.991480</td>
<td><a href="https://www.semanticscholar.org/paper/1f2c92c53cc5ad80bc929ff3b0ad746da0bb5f30">123: Direct Acoustics-to-Word Models for English Conversational Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.824424</td>
<td>0.991337</td>
<td><a href="https://www.semanticscholar.org/paper/b1cb867270f87f96397cb5f0d76cbb58cdf2c2f2">267: Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.802187</td>
<td>0.991098</td>
<td><a href="https://www.semanticscholar.org/paper/39a8ce943d0b4171e7fa6e4cab2d80cdb23cdbf1">47: On Online Attention-Based Speech Recognition and Joint Mandarin Character-Pinyin Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.839788</td>
<td>0.990607</td>
<td><a href="https://www.semanticscholar.org/paper/cf0e9724e51b420bc51a1d0625410c86d36641db">103: Building Competitive Direct Acoustics-to-Word Models for English Conversational Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.795572</td>
<td>0.989153</td>
<td><a href="https://www.semanticscholar.org/paper/495364aefa4dfef25e534582c4310ed4785c7dfd">14: A comprehensive analysis on attention models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.790223</td>
<td>0.988916</td>
<td><a href="https://www.semanticscholar.org/paper/608811ac14dcec1b8f860b30e4dbd5b60628598c">73: Advancing Acoustic-to-Word CTC Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.817440</td>
<td>0.988673</td>
<td><a href="https://www.semanticscholar.org/paper/4af08d465168c9b5fffe8cf1de6ee649ca4a8ac9">14: Joint Grapheme and Phoneme Embeddings for Contextual End-to-End ASR</a></td>
</tr>
</table></html>
