<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd">7157: Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.774310</td>
<td>0.209767</td>
<td><a href="https://www.semanticscholar.org/paper/41ec7222ad2583ae88ee2f91f4e7c2e98432765f">0: OOSTING SALIENCY PREDICTION WITH FEATURE MAPS TRAINED ON I MAGE N ET</a></td>
</tr>
<tr>
<td>0</td>
<td>0.774023</td>
<td>0.080862</td>
<td><a href="https://www.semanticscholar.org/paper/f0d75c37e233875f2de4e5a071f6660be766c6d0">2: Remember What You have drawn: Semantic Image Manipulation with Memory</a></td>
</tr>
<tr>
<td>0</td>
<td>0.768617</td>
<td>-0.031106</td>
<td><a href="https://www.semanticscholar.org/paper/634659fd051d8d9cf6b84d6b7171388608b48227">0: Unsupervised Learning for Object Representations by Watching and Moving</a></td>
</tr>
<tr>
<td>0</td>
<td>0.767525</td>
<td>0.243734</td>
<td><a href="https://www.semanticscholar.org/paper/0026d112cf8f3b98e45455d967de9ca3c33d22f6">12: Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.767218</td>
<td>0.099239</td>
<td><a href="https://www.semanticscholar.org/paper/a10d6877c90de39c42a143af60c0bf5e588be763">238: Deep Visual Analogy-Making</a></td>
</tr>
<tr>
<td>0</td>
<td>0.767144</td>
<td>0.068356</td>
<td><a href="https://www.semanticscholar.org/paper/dc55fa2fa5adfe2847117db1c4364781597fd815">6: Meaning maps and saliency models based on deep convolutional neural networks are insensitive to image meaning when predicting human fixations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763137</td>
<td>0.085981</td>
<td><a href="https://www.semanticscholar.org/paper/ff2bb372b6a419d274284379f2ce309cd4f6d425">9: Meaning maps and saliency models based on deep convolutional neural networks are insensitive to image meaning when predicting human fixations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.755703</td>
<td>0.019503</td>
<td><a href="https://www.semanticscholar.org/paper/768d483ed5879d94a413bd95b275438b8b19fb0d">3: Selective Attention in the Learning of Invariant Representation of Objects</a></td>
</tr>
<tr>
<td>0</td>
<td>0.748714</td>
<td>0.609011</td>
<td><a href="https://www.semanticscholar.org/paper/b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1">83: Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.825623</td>
<td>0.979714</td>
<td><a href="https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0">4457: Show and tell: A neural image caption generator</a></td>
</tr>
<tr>
<td>0</td>
<td>0.618824</td>
<td>0.974748</td>
<td><a href="https://www.semanticscholar.org/paper/076b02b481a41f1e07b8a2bdbe0ac8d946f9872e">9: Learning Long- and Short-Term User Literal-Preference with Multimodal Hierarchical Transformer Network for Personalized Image Caption</a></td>
</tr>
<tr>
<td>0</td>
<td>0.728589</td>
<td>0.973117</td>
<td><a href="https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88">2464: Deep Visual-Semantic Alignments for Generating Image Descriptions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.653100</td>
<td>0.972939</td>
<td><a href="https://www.semanticscholar.org/paper/a1af6068ea47b37648ffe0075242c92a39dfabf8">0: Component based comparative analysis of each module in image captioning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.679323</td>
<td>0.971192</td>
<td><a href="https://www.semanticscholar.org/paper/08903ceeee6420992d30ff3f3b8b4830118af4d9">245: Attention-Based Multimodal Fusion for Video Description</a></td>
</tr>
<tr>
<td>0</td>
<td>0.556981</td>
<td>0.968690</td>
<td><a href="https://www.semanticscholar.org/paper/441dc546f4658852779319b50bdee739d78485df">3: Unifying Relational Sentence Generation and Retrieval for Medical Image Report Composition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.446435</td>
<td>0.968479</td>
<td><a href="https://www.semanticscholar.org/paper/0da353e79f666a3ae7dd0a5d28c75b852a7f60bf">344: SHOW</a></td>
</tr>
<tr>
<td>0</td>
<td>0.824737</td>
<td>0.968153</td>
<td><a href="https://www.semanticscholar.org/paper/62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e">615: Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge</a></td>
</tr>
<tr>
<td>0</td>
<td>0.699537</td>
<td>0.966790</td>
<td><a href="https://www.semanticscholar.org/paper/b196bc11ad516c8e6ff96f83acfc443fd7161730">217: ABC-CNN: An Attention Based Convolutional Neural Network for Visual Question Answering</a></td>
</tr>
</table></html>
