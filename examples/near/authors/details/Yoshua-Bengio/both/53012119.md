<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/715a73290f260cf2196307e59fe0b6776841f170">305: On the Spectral Bias of Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.884635</td>
<td>0.886912</td>
<td><a href="https://www.semanticscholar.org/paper/cdea7d036d2c4abbf54130a80946391e43849800">50: On the Spectral Bias of Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.846910</td>
<td>0.618706</td>
<td><a href="https://www.semanticscholar.org/paper/b492410f0da1e30d99c62b3464b92c4a3693262a">0: A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.842561</td>
<td>0.794653</td>
<td><a href="https://www.semanticscholar.org/paper/092c88f430124dca071846dde56f3f90f91cb90c">23: The Surprising Simplicity of the Early-Time Learning Dynamics of Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.835196</td>
<td>0.782920</td>
<td><a href="https://www.semanticscholar.org/paper/e837dfa120e8ce3cd587bde7b0787ef43fa7832d">286: Sensitivity and Generalization in Neural Networks: an Empirical Study</a></td>
</tr>
<tr>
<td>0</td>
<td>0.832543</td>
<td>0.682180</td>
<td><a href="https://www.semanticscholar.org/paper/ed0c33b4aa79a793bd580ba46522433c51b2c482">49: Efficient Approximation of Deep ReLU Networks for Functions on Low Dimensional Manifolds</a></td>
</tr>
<tr>
<td>0</td>
<td>0.819072</td>
<td>0.789136</td>
<td><a href="https://www.semanticscholar.org/paper/4cbc6a3d08aedd590a23797686ce3a2f42aed7b4">0: Convergence Analysis of Over-parameterized Deep Linear Networks, and the Principal Components Bias</a></td>
</tr>
<tr>
<td>0</td>
<td>0.814832</td>
<td>0.568162</td>
<td><a href="https://www.semanticscholar.org/paper/52ce57ccd25f30b9ff3b63eba8a78a430dff7a27">30: Learning Compact Neural Networks with Regularization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.812332</td>
<td>0.730593</td>
<td><a href="https://www.semanticscholar.org/paper/a25bb56506fd1772e17d5b57a75ec838dafb6757">58: Modern Neural Networks Generalize on Small Data Sets</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811131</td>
<td>0.586583</td>
<td><a href="https://www.semanticscholar.org/paper/2a8f0a9706370d8f0c52c3438ca4e78615deddb2">51: On the Complexity of Learning Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.805853</td>
<td>0.958837</td>
<td><a href="https://www.semanticscholar.org/paper/dc71376a5531f72a3e84865f57d9b61e3354e73c">121: Training behavior of deep neural network in frequency domain</a></td>
</tr>
<tr>
<td>0</td>
<td>0.731927</td>
<td>0.950557</td>
<td><a href="https://www.semanticscholar.org/paper/8c7490f2af6f6ba9f361c2dcba4c4e717d99392e">0: Frequency Principle in deep learning: an overview</a></td>
</tr>
<tr>
<td>0</td>
<td>0.742567</td>
<td>0.944564</td>
<td><a href="https://www.semanticscholar.org/paper/645b8d4f63b1297a67b38562392daa4f1be3108c">0: An Upper Limit of Decaying Rate with Respect to Frequency in Deep Neural Network</a></td>
</tr>
<tr>
<td>0</td>
<td>0.752581</td>
<td>0.937258</td>
<td><a href="https://www.semanticscholar.org/paper/82485be41a5492088db6772e4f04a0a640782e02">7: On Infinite-Width Hypernetworks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.697543</td>
<td>0.936262</td>
<td><a href="https://www.semanticscholar.org/paper/5f1e85540af411bdaa183557d5d1272a13f982f5">25: Principled Weight Initialization for Hypernetworks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.827830</td>
<td>0.934281</td>
<td><a href="https://www.semanticscholar.org/paper/77a8f258958d90e43eb19fa7af971cdc65a32310">41: Frequency Bias in Neural Networks for Input of Non-Uniform Density</a></td>
</tr>
<tr>
<td>0</td>
<td>0.799993</td>
<td>0.933075</td>
<td><a href="https://www.semanticscholar.org/paper/d34b3bb6d5b611e6117e7e25f0b5419d3a99fdf1">127: Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.711775</td>
<td>0.928711</td>
<td><a href="https://www.semanticscholar.org/paper/2a3c60fbff134e293f934b44ec5055de20538e9c">0: Relaxing Equivariance Constraints with Non-stationary Continuous Filters</a></td>
</tr>
<tr>
<td>0</td>
<td>0.678965</td>
<td>0.927493</td>
<td><a href="https://www.semanticscholar.org/paper/9e4b3bce118e16a94f1cea5baf1d29ff27d46485">6: JFB: Jacobian-Free Backpropagation for Implicit Networks</a></td>
</tr>
</table></html>
