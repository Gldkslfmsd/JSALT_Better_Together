<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/135112c7ba1762d65f39b1a61777f26ae4dfd8ad">247: Is Attention Interpretable?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.958478</td>
<td>0.893116</td>
<td>NA:186201552</td>
</tr>
<tr>
<td>0</td>
<td>0.798827</td>
<td>0.758847</td>
<td><a href="https://www.semanticscholar.org/paper/470d7291b5b1cc98b97438bbebeafcd71a416095">0: Integrating Global Attention for Pairwise Text Comparison</a></td>
</tr>
<tr>
<td>0</td>
<td>0.797022</td>
<td>0.772643</td>
<td><a href="https://www.semanticscholar.org/paper/c6e6f4d0710e248d516cb5bebb6720ac2526900e">0: Siamese Network with Soft Attention for Semantic Text Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.791639</td>
<td>0.595946</td>
<td><a href="https://www.semanticscholar.org/paper/d6f7f15d1519b507f716d67fd64c899c7da5708f">0: Guided Attention for Neural Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.788331</td>
<td>0.751077</td>
<td><a href="https://www.semanticscholar.org/paper/2b5d6aff7c19d879c12e9757ba8a65967d27da93">0: Predictive Representation Learning for Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.778505</td>
<td>0.879928</td>
<td><a href="https://www.semanticscholar.org/paper/b63011d1480597f0bd87a6c6a6f2e8151bc1b202">0: Pre-trained language models evaluating themselves - A comparative study</a></td>
</tr>
<tr>
<td>0</td>
<td>0.770461</td>
<td>0.508576</td>
<td><a href="https://www.semanticscholar.org/paper/daa51400cb081bcd9c036c45061a73a85fd6f610">1: Leveraging contextual embeddings and self-attention neural networks with bi-attention for sentiment analysis</a></td>
</tr>
<tr>
<td>0</td>
<td>0.769188</td>
<td>0.864385</td>
<td><a href="https://www.semanticscholar.org/paper/5bc1a2a177d0881456e4f1077c91e29339883df5">3: NLITrans at SemEval-2018 Task 12: Transfer of Semantic Knowledge for Argument Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.767918</td>
<td>0.921055</td>
<td><a href="https://www.semanticscholar.org/paper/966a38882be844dbf7e8b15478e1bdf3c75ef8a6">4: A Closer Look at How Fine-tuning Changes BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.879232</td>
<td>0.998277</td>
<td><a href="https://www.semanticscholar.org/paper/1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f">603: Attention is not Explanation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.729161</td>
<td>0.995679</td>
<td><a href="https://www.semanticscholar.org/paper/cf2fcb73e2effff29ceb5a5b89bbca34d2d27c1a">103: Learning to Deceive with Attention-Based Explanations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.784172</td>
<td>0.995205</td>
<td><a href="https://www.semanticscholar.org/paper/ce177672b00ddf46e4906157a7e997ca9338b8b9">407: Attention is not not Explanation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.810737</td>
<td>0.992874</td>
<td><a href="https://www.semanticscholar.org/paper/508884a136a461869be128027950d2aa1778518c">58: The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.750774</td>
<td>0.989119</td>
<td><a href="https://www.semanticscholar.org/paper/e969778bced13a339f3d0465cea4e10c489ee1cc">0: Is Attention Explanation? An Introduction to the Debate</a></td>
</tr>
<tr>
<td>0</td>
<td>0.791968</td>
<td>0.987899</td>
<td><a href="https://www.semanticscholar.org/paper/f447c73de2f5cee843ad14d70e1d373294611934">67: Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference</a></td>
</tr>
<tr>
<td>0</td>
<td>0.746675</td>
<td>0.987592</td>
<td><a href="https://www.semanticscholar.org/paper/d5784fd3ac7e06ec030abb8f7787faa9279c1a50">4: Interpreting Deep Learning Models in Natural Language Processing: A Review</a></td>
</tr>
<tr>
<td>0</td>
<td>0.713294</td>
<td>0.987489</td>
<td><a href="https://www.semanticscholar.org/paper/1a066314e819d34aa4a836ef59a730ff1a11a1b0">1: An Empirical Study on Explanations in Out-of-Domain Settings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.717635</td>
<td>0.986686</td>
<td><a href="https://www.semanticscholar.org/paper/8c5465eb110d0cab951ca6858a0d51ae759d2f9c">112: Interpretable Neural Predictions with Differentiable Binary Variables</a></td>
</tr>
</table></html>
