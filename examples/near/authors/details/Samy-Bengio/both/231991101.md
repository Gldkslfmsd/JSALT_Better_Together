<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c">247: Understanding deep learning (still) requires rethinking generalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.978980</td>
<td>0.985574</td>
<td><a href="https://www.semanticscholar.org/paper/54ddb00fa691728944fd8becea90a373d21597cf">3442: Understanding deep learning requires rethinking generalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.870497</td>
<td>0.358282</td>
<td><a href="https://www.semanticscholar.org/paper/9aaef293143bfcf3ad73a1dc328308eb03e87fc3">1: Performance of Deep Learning Algorithms vs. Shallow Models, in Extreme Conditions - Some Empirical Studies</a></td>
</tr>
<tr>
<td>0</td>
<td>0.846531</td>
<td>0.979415</td>
<td><a href="https://www.semanticscholar.org/paper/2b627185499791048681e8d24190c31dea928f16">149: Uniform convergence may be unable to explain generalization in deep learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.833394</td>
<td>0.921355</td>
<td><a href="https://www.semanticscholar.org/paper/adc9ccd21791d7e44aedeb677484b8e9e245f1dc">17: Understanding Generalization in Deep Learning via Tensor Methods</a></td>
</tr>
<tr>
<td>0</td>
<td>0.825629</td>
<td>0.686417</td>
<td><a href="https://www.semanticscholar.org/paper/2adb616a77fe28b49be2a2d66cccf2d7400e4a04">322: Data-Driven Sparse Structure Selection for Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.824405</td>
<td>0.918255</td>
<td><a href="https://www.semanticscholar.org/paper/8501e330d78391f4e690886a8eb8fac867704ea6">508: Train longer, generalize better: closing the generalization gap in large batch training of neural networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.822645</td>
<td>0.883923</td>
<td><a href="https://www.semanticscholar.org/paper/f70dca374752cdd5d4279a7d03ba027559201e7f">8: Towards an Understanding of Benign Overfitting in Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.820840</td>
<td>0.828171</td>
<td><a href="https://www.semanticscholar.org/paper/521ebc310afd88a2672f0af5f77dd4e6ec5c994f">262: Understanding Batch Normalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.813917</td>
<td>0.161061</td>
<td>NA:139084956</td>
</tr>
<tr>
<td>0</td>
<td>0.789567</td>
<td>0.990127</td>
<td><a href="https://www.semanticscholar.org/paper/29090beb90c184a9aaf7aa610bfed5ee1631d2f2">171: Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.658344</td>
<td>0.987608</td>
<td><a href="https://www.semanticscholar.org/paper/4ab10d6b989cc38b95a36e4fa346b80b7d993e56">29: Understanding Generalization through Visualizations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.820943</td>
<td>0.985170</td>
<td><a href="https://www.semanticscholar.org/paper/209e496d30b405f34c59e613a35955d16e5b0d10">12: Understanding Generalization of Deep Neural Networks Trained with Noisy Labels</a></td>
</tr>
<tr>
<td>0</td>
<td>0.685474</td>
<td>0.984752</td>
<td><a href="https://www.semanticscholar.org/paper/d53fb3feeeab07a0d70bf466dd473ec6052ecc07">739: Exploring Generalization in Deep Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811447</td>
<td>0.984097</td>
<td><a href="https://www.semanticscholar.org/paper/1b4c4f16a798dacbafb30ddc4511e03f98f59fd7">61: Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee</a></td>
</tr>
<tr>
<td>0</td>
<td>0.729690</td>
<td>0.982735</td>
<td><a href="https://www.semanticscholar.org/paper/ea415809bf87ef4b99966c6c50de6cb996a02a97">363: Deep double descent: where bigger models and more data hurt</a></td>
</tr>
<tr>
<td>0</td>
<td>0.804466</td>
<td>0.981767</td>
<td><a href="https://www.semanticscholar.org/paper/4e431827bf1ed1d8c435c01e75b12c79ba968721">106: SGD on Neural Networks Learns Functions of Increasing Complexity</a></td>
</tr>
<tr>
<td>0</td>
<td>0.760434</td>
<td>0.981708</td>
<td><a href="https://www.semanticscholar.org/paper/a9022d8ffb5e417458fba9a280f90c1b08cb6c73">437: Stronger generalization bounds for deep nets via a compression approach</a></td>
</tr>
<tr>
<td>0</td>
<td>0.482574</td>
<td>0.981322</td>
<td><a href="https://www.semanticscholar.org/paper/4fc3ee440c2b0f66255a9e6966cee871ee0cc6da">397: A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks</a></td>
</tr>
</table></html>
