<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/df137487e20ba7c6e1e2b9a1e749f2a578b5ad99">1390: Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.819229</td>
<td>0.834670</td>
<td><a href="https://www.semanticscholar.org/paper/463b8cc69b689643b16fd7677b228030a7f6dd9b">0: k-Neighbor Based Curriculum Sampling for Sequence Prediction</a></td>
</tr>
<tr>
<td>0</td>
<td>0.817269</td>
<td>0.778996</td>
<td><a href="https://www.semanticscholar.org/paper/9b9ec89ce8daab58b0d114b5d880845d713b9241">75: Tuning Recurrent Neural Networks with Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.815778</td>
<td>0.913783</td>
<td><a href="https://www.semanticscholar.org/paper/eee77aae5e2bae2a7645669a2653a77b522831e1">2: Deep successor feature learning for text generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.808121</td>
<td>0.844653</td>
<td><a href="https://www.semanticscholar.org/paper/0b8cb0843d915633ebde939bec1dfd7901dc31ac">0: Recurrent Nested Model for Sequence Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800705</td>
<td>0.739012</td>
<td><a href="https://www.semanticscholar.org/paper/ec34e40cfff80f3496bbec82658ba391fc6ded68">5: Learning to skip state updates in recurrent neural networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.798635</td>
<td>0.892177</td>
<td><a href="https://www.semanticscholar.org/paper/b7b97fff93bcd32aa2d1c9bc1acc3827bb3d4347">5: Sequence-to-Sequence Learning with Latent Neural Grammars</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793296</td>
<td>0.803574</td>
<td><a href="https://www.semanticscholar.org/paper/110116481848d8d2e371652a11874635024426f0">7: Recognizing Long Grammatical Sequences Using Recurrent Networks Augmented With An External Differentiable Stack</a></td>
</tr>
<tr>
<td>0</td>
<td>0.785458</td>
<td>0.453530</td>
<td><a href="https://www.semanticscholar.org/paper/891942849f4e94178aa780a7049ba036c56e67ad">0: Hierarchical Conflict Propagation: Sequence Learning in a Recurrent Deep Neural Network</a></td>
</tr>
<tr>
<td>0</td>
<td>0.783352</td>
<td>0.913564</td>
<td><a href="https://www.semanticscholar.org/paper/b2c61b87f82b888d86fb10c7947c5b380c3cbf06">22: Connecting the Dots Between MLE and RL for Sequence Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.677524</td>
<td>0.973205</td>
<td><a href="https://www.semanticscholar.org/paper/4216ee11823aba41ad4c2adbe50f765e86a8a04b">27: Learning Hard Alignments with Variational Inference</a></td>
</tr>
<tr>
<td>0</td>
<td>-1.000000</td>
<td>0.971065</td>
<td><a href="https://www.semanticscholar.org/paper/086cef88cbac0b733e1d0b7d4756600df953851f">27: Posterior Attention Models for Sequence to Sequence Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.819664</td>
<td>0.968566</td>
<td><a href="https://www.semanticscholar.org/paper/db38edba294b7d2fd8ca3aad65721bd9dce32619">390: Professor Forcing: A New Algorithm for Training Recurrent Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.689553</td>
<td>0.968530</td>
<td><a href="https://www.semanticscholar.org/paper/8b84405fb6e75d41ae35337b86916ca059201824">85: Latent Alignment and Variational Attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.836205</td>
<td>0.967522</td>
<td><a href="https://www.semanticscholar.org/paper/35c1668dc64d24a28c6041978e5fcca754eb2f4b">1199: Sequence Level Training with Recurrent Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.694190</td>
<td>0.965500</td>
<td><a href="https://www.semanticscholar.org/paper/27a591ca871b22dfd6dd0c7d59fed69cbe6d96da">0: 2 Background : Latent Alignment and Neural Attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.777840</td>
<td>0.962487</td>
<td><a href="https://www.semanticscholar.org/paper/115a3312a9273ffb85b7c8f72951bd63627f6d7e">60: Variational Recurrent Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.764081</td>
<td>0.962206</td>
<td><a href="https://www.semanticscholar.org/paper/b564a25159090c6f82cda494008081055f8917f2">74: Variational Attention for Sequence-to-Sequence Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.735880</td>
<td>0.961161</td>
<td><a href="https://www.semanticscholar.org/paper/609e0f0e60ddfe83fdc71bf5397205323888289d">871: A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues</a></td>
</tr>
</table></html>
