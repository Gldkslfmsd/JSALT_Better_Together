<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b">3254: Adversarial examples in the physical world</a></td>
</tr>
<tr>
<td>0</td>
<td>0.867899</td>
<td>0.996441</td>
<td><a href="https://www.semanticscholar.org/paper/36a3eed52ff0a694aa73ce6a0d592cb440ed3d31">235: Robust Physical-World Attacks on Machine Learning Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.852287</td>
<td>0.993315</td>
<td><a href="https://www.semanticscholar.org/paper/efdb28e0107a3c6142ff5b01e4fc5a974b5ea762">16: Detecting Adversarial Examples Using Data Manifolds</a></td>
</tr>
<tr>
<td>0</td>
<td>0.851802</td>
<td>0.777525</td>
<td><a href="https://www.semanticscholar.org/paper/19396bf48e0ce2b1bf495542ef725529e3b159e0">7: Adversarial learning: the impact of statistical sample selection techniques on neural ensembles</a></td>
</tr>
<tr>
<td>0</td>
<td>0.849178</td>
<td>0.980954</td>
<td><a href="https://www.semanticscholar.org/paper/a5caa8642f198bcb107092c18af9e884d263308f">3: Detecting Adversarial Examples Based on Steganalysis</a></td>
</tr>
<tr>
<td>1</td>
<td>0.849164</td>
<td>0.999242</td>
<td><a href="https://www.semanticscholar.org/paper/531e3a7b7768f199fdd401b266504db245ca039a">685: On Detecting Adversarial Perturbations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.848853</td>
<td>0.909141</td>
<td><a href="https://www.semanticscholar.org/paper/3da2e9dd965992f83a1ef7354c0f46f5abbdfb5e">24: Sparse Feature Attacks in Adversarial Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.845896</td>
<td>0.965690</td>
<td><a href="https://www.semanticscholar.org/paper/241729e58700a73238d4fe5c9cc5cc5a9cc7d06d">14: Adversarial Machine Learning in Network Intrusion Detection Systems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.843646</td>
<td>0.993713</td>
<td><a href="https://www.semanticscholar.org/paper/c8b29239540dd6d49b7d5a533844894dc2cf981e">0: Generating Adversarial Inputs Using A Black-box Differential Technique</a></td>
</tr>
<tr>
<td>0</td>
<td>0.838943</td>
<td>0.991645</td>
<td><a href="https://www.semanticscholar.org/paper/9108ac69fe4ceb756e54ec490ed9abb86e40f8ba">22: Multi-Targeted Adversarial Example in Evasion Attack on Deep Neural Network</a></td>
</tr>
<tr>
<td>0</td>
<td>0.642731</td>
<td>0.999727</td>
<td><a href="https://www.semanticscholar.org/paper/52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35">2913: DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800755</td>
<td>0.999630</td>
<td><a href="https://www.semanticscholar.org/paper/df40ce107a71b770c9d0354b78fdd8989da80d2f">4538: Towards Evaluating the Robustness of Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.849613</td>
<td>0.999620</td>
<td><a href="https://www.semanticscholar.org/paper/e2a85a6766b982ff7c8980e57ca6342d22493827">1852: Adversarial Machine Learning at Scale</a></td>
</tr>
<tr>
<td>0</td>
<td>0.810283</td>
<td>0.999195</td>
<td><a href="https://www.semanticscholar.org/paper/6adf016e7531c91100d3cf4a74f5d4c87b26b528">2077: Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.748195</td>
<td>0.999107</td>
<td><a href="https://www.semanticscholar.org/paper/899005eb65650c91240a6624c8b4dc14f757afad">271: SafetyNet: Detecting and Rejecting Adversarial Examples Robustly</a></td>
</tr>
<tr>
<td>0</td>
<td>0.760301</td>
<td>0.999035</td>
<td><a href="https://www.semanticscholar.org/paper/9fec45e1ff97ffb0e0cf9f039e39b46043430301">1054: Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.822396</td>
<td>0.998968</td>
<td><a href="https://www.semanticscholar.org/paper/405b6ff2ea2ec9a7c7d6b18ac951dc778892ffcf">594: Detecting Adversarial Samples from Artifacts</a></td>
</tr>
<tr>
<td>0</td>
<td>0.818544</td>
<td>0.998927</td>
<td><a href="https://www.semanticscholar.org/paper/63a010c69f00e65c946a68b546bbd42cbed03564">769: MagNet: A Two-Pronged Defense against Adversarial Examples</a></td>
</tr>
</table></html>
