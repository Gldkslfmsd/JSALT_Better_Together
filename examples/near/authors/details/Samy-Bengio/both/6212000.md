<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/54ddb00fa691728944fd8becea90a373d21597cf">3442: Understanding deep learning requires rethinking generalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.978980</td>
<td>0.985574</td>
<td><a href="https://www.semanticscholar.org/paper/2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c">247: Understanding deep learning (still) requires rethinking generalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.813462</td>
<td>0.958478</td>
<td><a href="https://www.semanticscholar.org/paper/9f9fc406c76255fec51a6196ce167c0ff1d1efc0">528: Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent</a></td>
</tr>
<tr>
<td>0</td>
<td>0.813247</td>
<td>0.690055</td>
<td><a href="https://www.semanticscholar.org/paper/2adb616a77fe28b49be2a2d66cccf2d7400e4a04">322: Data-Driven Sparse Structure Selection for Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.805828</td>
<td>0.958329</td>
<td><a href="https://www.semanticscholar.org/paper/5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf">821: A Closer Look at Memorization in Deep Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.805539</td>
<td>0.878678</td>
<td><a href="https://www.semanticscholar.org/paper/bb6c9ca521212e5bc5fd62ef4a5bd60a34768275">6: A Spectral Approach to Generalization and Optimization in Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.802495</td>
<td>0.926227</td>
<td><a href="https://www.semanticscholar.org/paper/be0f6dd323880f6130b78f9ad6239edc0d597f9f">29: On the Benefits of Invariance in Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.801228</td>
<td>0.124496</td>
<td><a href="https://www.semanticscholar.org/paper/c004d0ca3105032be918b5895258daf3c4ae9030">0: An Optimal Transport Analysis on Generalization in Deep Learning.</a></td>
</tr>
<tr>
<td>0</td>
<td>0.799485</td>
<td>0.820290</td>
<td><a href="https://www.semanticscholar.org/paper/b87a3a9a6c31982eb56a8161f75ae79537ac8566">11: The Gaussian equivalence of generative models for learning with shallow neural networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.798548</td>
<td>0.907926</td>
<td><a href="https://www.semanticscholar.org/paper/b85fc5a54f59478cefd36ba9e9e8fdde59e5d415">32: The Implicit Bias of Depth: How Incremental Learning Drives Generalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.690313</td>
<td>0.990482</td>
<td><a href="https://www.semanticscholar.org/paper/d53fb3feeeab07a0d70bf466dd473ec6052ecc07">739: Exploring Generalization in Deep Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.767533</td>
<td>0.985786</td>
<td><a href="https://www.semanticscholar.org/paper/29090beb90c184a9aaf7aa610bfed5ee1631d2f2">171: Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.839881</td>
<td>0.983119</td>
<td><a href="https://www.semanticscholar.org/paper/0ebb83e5c28719c7b5cb5bc482e62f835fb0d116">63: Deep Nets Don't Learn via Memorization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.678887</td>
<td>0.981950</td>
<td><a href="https://www.semanticscholar.org/paper/4ab10d6b989cc38b95a36e4fa346b80b7d993e56">29: Understanding Generalization through Visualizations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741032</td>
<td>0.981947</td>
<td><a href="https://www.semanticscholar.org/paper/a9022d8ffb5e417458fba9a280f90c1b08cb6c73">437: Stronger generalization bounds for deep nets via a compression approach</a></td>
</tr>
<tr>
<td>0</td>
<td>0.825508</td>
<td>0.981806</td>
<td><a href="https://www.semanticscholar.org/paper/e837dfa120e8ce3cd587bde7b0787ef43fa7832d">286: Sensitivity and Generalization in Neural Networks: an Empirical Study</a></td>
</tr>
<tr>
<td>0</td>
<td>0.739464</td>
<td>0.980789</td>
<td><a href="https://www.semanticscholar.org/paper/06ee9741730e4a2c7c8cdf643f5f34bc497a0b7c">369: Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.498691</td>
<td>0.980420</td>
<td><a href="https://www.semanticscholar.org/paper/4fc3ee440c2b0f66255a9e6966cee871ee0cc6da">397: A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.807524</td>
<td>0.979111</td>
<td><a href="https://www.semanticscholar.org/paper/4e431827bf1ed1d8c435c01e75b12c79ba968721">106: SGD on Neural Networks Learns Functions of Increasing Complexity</a></td>
</tr>
</table></html>
