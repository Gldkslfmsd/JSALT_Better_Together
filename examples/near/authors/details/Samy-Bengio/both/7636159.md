<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/58123025178256279bb060ca5da971b62bc329ee">449: Sharp Minima Can Generalize For Deep Nets</a></td>
</tr>
<tr>
<td>0</td>
<td>0.825420</td>
<td>0.946677</td>
<td><a href="https://www.semanticscholar.org/paper/e4aa5c644ea23a3927694114e4be1f9d4c3f5719">0: Overparameterized Nonlinear Optimization with Applications to Neural Nets</a></td>
</tr>
<tr>
<td>0</td>
<td>0.824139</td>
<td>0.900268</td>
<td><a href="https://www.semanticscholar.org/paper/033f7570be9877c5a4bcbb71f6aec8f95cee3608">271: To understand deep learning we need to understand kernel learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.821572</td>
<td>0.062064</td>
<td><a href="https://www.semanticscholar.org/paper/37c87a80d02e9058399f3cf03cd7309a3c4f01a6">0: Guaranteeing Generalization via Measures of Information</a></td>
</tr>
<tr>
<td>0</td>
<td>0.817298</td>
<td>0.955974</td>
<td><a href="https://www.semanticscholar.org/paper/84f72096b6d432b9d1bfe4c9bf7c69bfced92bcf">1: On the Convergence of Deep Networks with Sample Quadratic Overparameterization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811659</td>
<td>0.439777</td>
<td><a href="https://www.semanticscholar.org/paper/ce73e21ad34832bd17648cae483b960fc8c47644">46: Connections with Robust PCA and the Role of Emergent Sparsity in Variational Autoencoder Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809318</td>
<td>0.936103</td>
<td><a href="https://www.semanticscholar.org/paper/f077c4f5a6e5d6457ce42bc0c7bd087dfa5f18a0">0: On Measuring Excess Capacity in Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.804858</td>
<td>0.957005</td>
<td><a href="https://www.semanticscholar.org/paper/2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c">247: Understanding deep learning (still) requires rethinking generalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.802990</td>
<td>0.984794</td>
<td><a href="https://www.semanticscholar.org/paper/4eec1e327baa011b736c8b933fbe83c172ac38d6">3: A Reparameterization-Invariant Flatness Measure for Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.801920</td>
<td>0.972982</td>
<td><a href="https://www.semanticscholar.org/paper/67ee4b4272053c30a86a2b8515f6dfe38ccd9e66">0: VESTIGATION OF DEEP LEARNING THEORY</a></td>
</tr>
<tr>
<td>0</td>
<td>0.740863</td>
<td>0.996028</td>
<td><a href="https://www.semanticscholar.org/paper/b6583fe9c9dc52bb129aff4cefc60519349f3b4c">484: Entropy-SGD: Biasing Gradient Descent Into Wide Valleys</a></td>
</tr>
<tr>
<td>0</td>
<td>0.829475</td>
<td>0.995551</td>
<td><a href="https://www.semanticscholar.org/paper/104ef009c1b2fc3b8be95dbb6bb1456631c61f94">117: Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes</a></td>
</tr>
<tr>
<td>0</td>
<td>0.660139</td>
<td>0.989550</td>
<td><a href="https://www.semanticscholar.org/paper/2c90d366126a3ccd3c43e47891730650003059da">205: Essentially No Barriers in Neural Network Energy Landscape</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793406</td>
<td>0.989444</td>
<td><a href="https://www.semanticscholar.org/paper/ae4b0b63ff26e52792be7f60bda3ed5db83c1577">253: A Bayesian Perspective on Generalization and Stochastic Gradient Descent</a></td>
</tr>
<tr>
<td>0</td>
<td>0.687529</td>
<td>0.987008</td>
<td><a href="https://www.semanticscholar.org/paper/f0c8ac4ac7720f2844651e5cb7ecb31b3ea199b6">1: Weight Expansion: A New Perspective on Dropout and Generalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.696831</td>
<td>0.986928</td>
<td><a href="https://www.semanticscholar.org/paper/ecd88d2ef770348e16b807ad5430314dcf219098">144: Fisher-Rao Metric, Geometry, and Complexity of Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.660899</td>
<td>0.986847</td>
<td><a href="https://www.semanticscholar.org/paper/f9e6af73d33e7aac3f349bef927fcd666e8e00db">284: Three Factors Influencing Minima in SGD</a></td>
</tr>
<tr>
<td>0</td>
<td>0.709167</td>
<td>0.986836</td>
<td><a href="https://www.semanticscholar.org/paper/d53fb3feeeab07a0d70bf466dd473ec6052ecc07">739: Exploring Generalization in Deep Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.626376</td>
<td>0.986816</td>
<td><a href="https://www.semanticscholar.org/paper/e1a49946720cb8fdbd45bd94ccbfb85b06093cfb">3: Cockpit: A Practical Debugging Tool for Training Deep Neural Networks</a></td>
</tr>
</table></html>
