<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/2990cf242558ede739d6a26a2f8b098f94390323">60: Morphological Smoothing and Extrapolation of Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.853784</td>
<td>0.970894</td>
<td><a href="https://www.semanticscholar.org/paper/bc52cf9a232f230ff1de2a0ce3441753e917c5ea">7: Word Embeddings for Morphologically Rich Languages</a></td>
</tr>
<tr>
<td>0</td>
<td>0.846114</td>
<td>0.912273</td>
<td><a href="https://www.semanticscholar.org/paper/20b47056a608cb676b87de55cbc0b7ee5755551b">9: Detection of Multiword Expressions for Hindi Language using Word Embeddings and WordNet-based Features</a></td>
</tr>
<tr>
<td>0</td>
<td>0.843560</td>
<td>0.949518</td>
<td><a href="https://www.semanticscholar.org/paper/e96b7d6384b41d758ac1ee2761221aa3e9106bf2">1: A Survey On Neural Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.841507</td>
<td>0.765724</td>
<td><a href="https://www.semanticscholar.org/paper/8c3cf30db12d17638b01e0e464e09d6b58a88187">8: Variable length word encodings for neural translation models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.840334</td>
<td>0.926159</td>
<td><a href="https://www.semanticscholar.org/paper/3ede26443dbc943356d88a1a3ff1de297eacc64a">1: Morphological Skip-Gram: Replacing FastText characters n-gram with morphological knowledge</a></td>
</tr>
<tr>
<td>0</td>
<td>0.839746</td>
<td>0.958178</td>
<td><a href="https://www.semanticscholar.org/paper/3ffbbfa569d5125b85b0ccb7625e77e556dfb493">13: Learning Context-Specific Word/Character Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.834582</td>
<td>0.960275</td>
<td><a href="https://www.semanticscholar.org/paper/a28a6e5f623995864396a47160d183cc547294f0">0: Comparison of Turkish Word Representations Trained on Different Morphological Forms</a></td>
</tr>
<tr>
<td>0</td>
<td>0.834312</td>
<td>0.852239</td>
<td><a href="https://www.semanticscholar.org/paper/c187174cae32d272edcf566b400e091e44ccf59e">2: Exploring word embeddings and phonological similarity for the unsupervised correction of language learner errors</a></td>
</tr>
<tr>
<td>0</td>
<td>0.833346</td>
<td>0.821063</td>
<td><a href="https://www.semanticscholar.org/paper/86c5d27c1783af857509589f7b7b3c0b93d2ca52">1: Word embeddings and semantic shifts in historical Spanish: Methodological considerations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.796446</td>
<td>0.993883</td>
<td><a href="https://www.semanticscholar.org/paper/ec27e759576f14addeace610304365d96737f644">86: Morphological Word-Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763072</td>
<td>0.993733</td>
<td><a href="https://www.semanticscholar.org/paper/4ad638ae669a28c24c18412c14bff57e284663dd">12: Characters or Morphemes: How to Represent Words?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.821906</td>
<td>0.992920</td>
<td><a href="https://www.semanticscholar.org/paper/d7613c4ded0893ad57497b7f743b47ee553a382a">70: A Joint Model for Word Embedding and Word Morphology</a></td>
</tr>
<tr>
<td>0</td>
<td>0.827703</td>
<td>0.992844</td>
<td><a href="https://www.semanticscholar.org/paper/35254a19bac666c291646e3640014bf0bb2ce11b">59: Embedding Word Similarity with Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.765198</td>
<td>0.991224</td>
<td><a href="https://www.semanticscholar.org/paper/9dd5fe071ba12e3d7d1e678b9ea74a43f298b7a7">0: Improving Chinese Segmentation-free Word Embedding With Unsupervised Association Measure</a></td>
</tr>
<tr>
<td>0</td>
<td>0.855814</td>
<td>0.990334</td>
<td><a href="https://www.semanticscholar.org/paper/f690c45d7cb80ce46a438c4dd1f9b50e1a45a84e">38: Morphological Priors for Probabilistic Neural Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.753378</td>
<td>0.990085</td>
<td><a href="https://www.semanticscholar.org/paper/39ef3906b13ac2758ebc0d8f75e738d4f6314b39">113: Unsupervised Morphology Induction Using Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.806261</td>
<td>0.989809</td>
<td><a href="https://www.semanticscholar.org/paper/8ce4f05303f0850e58748af2c6757eb12a147f2c">17: Learning to Embed Words in Context for Syntactic Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763116</td>
<td>0.989653</td>
<td><a href="https://www.semanticscholar.org/paper/1347bd4f826f72ff561b70e665477edadb2a72be">124: Not All Contexts Are Created Equal: Better Word Representations with Variable Attention</a></td>
</tr>
</table></html>
