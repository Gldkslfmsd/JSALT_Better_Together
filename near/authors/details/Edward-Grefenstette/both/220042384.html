<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/3228f6fc7902b23c26378e59f8c8820412de7f42">40: The NetHack Learning Environment</a></td>
</tr>
<tr>
<td>0</td>
<td>0.840283</td>
<td>0.965939</td>
<td><a href="https://www.semanticscholar.org/paper/88c38f203bbf0f881bcc42a8d1a4235b4ba7cb86">4: Escape Room: A Configurable Testbed for Hierarchical Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.832559</td>
<td>0.982476</td>
<td><a href="https://www.semanticscholar.org/paper/bc18f1ec5209f8ebacb91eff42927a33664bc1a2">0: Influence-based Reinforcement Learning for Intrinsically-motivated Agents</a></td>
</tr>
<tr>
<td>0</td>
<td>0.828612</td>
<td>-0.013340</td>
<td><a href="https://www.semanticscholar.org/paper/f68a9216f574cbb03d0ce87d001577bc4bf7dd7c">0: Gradient-Free Deep Q-Networks Reinforcement learning: Benchmark and Evaluation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.827684</td>
<td>0.935407</td>
<td><a href="https://www.semanticscholar.org/paper/d88af85e84914d18f86f88afcf7b7d0d80f0b0c3">18: ORL: Reinforcement Learning Benchmarks for Online Stochastic Optimization Problems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826842</td>
<td>0.933208</td>
<td><a href="https://www.semanticscholar.org/paper/b2435b1eaaaa74a8365bfb01537595070df2b476">2: Increasing sample efficiency in deep reinforcement learning using generative environment modelling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826633</td>
<td>0.987221</td>
<td><a href="https://www.semanticscholar.org/paper/a5885d3438789e64986ddd1fd896dda5992f81f6">0: Towards a Distributed Framework for Multi-Agent Reinforcement Learning Research</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826554</td>
<td>0.948728</td>
<td><a href="https://www.semanticscholar.org/paper/4d91b03f638a510945efb82168e8445070c9b96e">2: Modeling Decisions in Games Using Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826052</td>
<td>0.898856</td>
<td><a href="https://www.semanticscholar.org/paper/26b6477ef683ef55c453d65fbcbf4e51e0d77b07">12: Learning to Play: Reinforcement Learning and Games</a></td>
</tr>
<tr>
<td>0</td>
<td>0.824711</td>
<td>-0.013340</td>
<td><a href="https://www.semanticscholar.org/paper/53f090f1a908fe1949f7c4957db3acc033c7f9f9">0: AlphaZero-Inspired General Board Game Learning and Playing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.841828</td>
<td>0.997752</td>
<td><a href="https://www.semanticscholar.org/paper/43ea4f5d999d35d4fc6c544eedbc100d8c3a5e00">15: MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research</a></td>
</tr>
<tr>
<td>0</td>
<td>0.657594</td>
<td>0.995313</td>
<td><a href="https://www.semanticscholar.org/paper/f40e5ab8605665f7a28d65edf0ea0f732bcb11f9">5: MazeExplorer: A Customisable 3D Benchmark for Assessing Generalisation in Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.806647</td>
<td>0.994787</td>
<td><a href="https://www.semanticscholar.org/paper/8e128a1b2efb0ddf688902ade4405d22d5b61eec">9: Benchmarking the Spectrum of Agent Capabilities</a></td>
</tr>
<tr>
<td>0</td>
<td>0.775495</td>
<td>0.994091</td>
<td><a href="https://www.semanticscholar.org/paper/eb44e9824a927bad07f48dbd41c516626513156a">9: Griddly: A platform for AI research in games</a></td>
</tr>
<tr>
<td>0</td>
<td>0.786800</td>
<td>0.994063</td>
<td><a href="https://www.semanticscholar.org/paper/54631c2af9d2b6f896f4469a9391440eb57f7c1b">0: Inventory Management with Attention-Based Meta Actions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.735148</td>
<td>0.993737</td>
<td><a href="https://www.semanticscholar.org/paper/60a5be13ba59e21e4b8f335b3bcc341d93d8deea">53: Automatic Curriculum Learning For Deep RL: A Short Survey</a></td>
</tr>
<tr>
<td>0</td>
<td>0.709836</td>
<td>0.993311</td>
<td><a href="https://www.semanticscholar.org/paper/2d0e625fca0efc079bb523baf1ff78c46ff4916e">21: Rotation, Translation, and Cropping for Zero-Shot Generalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.673643</td>
<td>0.993010</td>
<td><a href="https://www.semanticscholar.org/paper/8112e04e71b83f996f6098bc6ae2b4856128fbce">1: Automated curriculum generation for Policy Gradients from Demonstrations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.772128</td>
<td>0.992977</td>
<td><a href="https://www.semanticscholar.org/paper/33a2c0f3b9a0adc452a13d53f950c0c3c4abb11b">296: Emergent Tool Use From Multi-Agent Autocurricula</a></td>
</tr>
</table></html>
