<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/300529850ca8e8ad6633a2b566206bf7f2a38fd9">1: Evolving Curricula with Regret-Based Environment Design</a></td>
</tr>
<tr>
<td>0</td>
<td>0.797061</td>
<td>0.982209</td>
<td><a href="https://www.semanticscholar.org/paper/e102cd42c402026a3862d2e60a75eed7c78860a2">91: Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey</a></td>
</tr>
<tr>
<td>0</td>
<td>0.771449</td>
<td>0.980505</td>
<td><a href="https://www.semanticscholar.org/paper/9b4d48eca82e8a3907b0e941a7ebd6976c95d6ae">37: Backplay: "Man muss immer umkehren"</a></td>
</tr>
<tr>
<td>0</td>
<td>0.761105</td>
<td>0.971861</td>
<td><a href="https://www.semanticscholar.org/paper/2e0cd27cd60acbbf2bc5f62d773df9c0eacacd6b">14: Self-Paced Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.760680</td>
<td>0.952069</td>
<td><a href="https://www.semanticscholar.org/paper/8ad0ad9736dc2ed7b8c59929c1f58409d487aeaf">3: Learning Context-Sensitive Strategies in Space Fortress</a></td>
</tr>
<tr>
<td>0</td>
<td>0.757029</td>
<td>0.923015</td>
<td><a href="https://www.semanticscholar.org/paper/eb7a64195ef4a268f79fa6740f128387f2696c65">30: Teaching AI Agents Ethical Values Using Reinforcement Learning and Policy Orchestration</a></td>
</tr>
<tr>
<td>0</td>
<td>0.755426</td>
<td>0.986227</td>
<td><a href="https://www.semanticscholar.org/paper/e5c3b520d7415e1268cf6c6e2f7e618248e11662">0: DCUR: Data Curriculum for Teaching via Samples with Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741692</td>
<td>0.922702</td>
<td><a href="https://www.semanticscholar.org/paper/d44f002fcd9c547d5efe7af15fd137f32629f838">0: Curriculum Development for Transfer Learning in Dynamic Multiagent Settings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741087</td>
<td>0.968409</td>
<td><a href="https://www.semanticscholar.org/paper/e245ef09fe9e4f56d0fa7f75257298588f4d0392">40: Agent-Agnostic Human-in-the-Loop Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.738745</td>
<td>0.604195</td>
<td><a href="https://www.semanticscholar.org/paper/d42ebc6717cb33237a7c684e57734dacd8a7bed5">0: Reinforcement Learning for a Simple Racing Game</a></td>
</tr>
<tr>
<td>0</td>
<td>0.730141</td>
<td>0.995866</td>
<td><a href="https://www.semanticscholar.org/paper/60fc8a885083f74c7a0aea829c81a92f2107e4d1">122: Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.776552</td>
<td>0.995071</td>
<td><a href="https://www.semanticscholar.org/paper/36910e1b83109be5346c18cb57ba222c8dd45c26">12: Adversarial Reinforcement Learning for Procedural Content Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.620749</td>
<td>0.994649</td>
<td><a href="https://www.semanticscholar.org/paper/a7b3b89ef299261e172e8f47a263dee05511e7b4">73: Stochastic Grounded Action Transformation for Robot Learning in Simulation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.682240</td>
<td>0.994524</td>
<td><a href="https://www.semanticscholar.org/paper/a6dd4e15b4e6b09d0c0584d64db2f335fad033f9">0: Active Reinforcement Learning over MDPs</a></td>
</tr>
<tr>
<td>0</td>
<td>0.765629</td>
<td>0.994465</td>
<td><a href="https://www.semanticscholar.org/paper/549bfdfd9fa718331076810f0d5817adcd79fe69">0: AutoDIME: Automatic Design of Interesting Multi-Agent Environments</a></td>
</tr>
<tr>
<td>0</td>
<td>0.762707</td>
<td>0.993849</td>
<td><a href="https://www.semanticscholar.org/paper/fdb8ccebef9f544f44cd9b88085d8ec5458b38ab">8: Replay-Guided Adversarial Environment Design</a></td>
</tr>
<tr>
<td>0</td>
<td>0.730565</td>
<td>0.993606</td>
<td><a href="https://www.semanticscholar.org/paper/ff3f23189c3a7fed677a868ddfc22a02bb6e4ef8">21: Procedural Level Generation Improves Generality of Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.769901</td>
<td>0.993054</td>
<td><a href="https://www.semanticscholar.org/paper/e94241c59b9caa3efe6950731a164fa787b4d737">5: Adversarial Environment Generation for Learning to Navigate the Web</a></td>
</tr>
<tr>
<td>0</td>
<td>0.616271</td>
<td>0.992648</td>
<td><a href="https://www.semanticscholar.org/paper/6509c28c459e07021a2505258715442b2ff0d72b">8: Reinforcement Learning Generalization with Surprise Minimization</a></td>
</tr>
</table></html>
