<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/b65faba7088864e134e7eb3b68c8e2f18cc5b4f6">165: Situation Recognition: Visual Semantic Role Labeling for Image Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.782113</td>
<td>0.859084</td>
<td><a href="https://www.semanticscholar.org/paper/54a42098b34c2602305b03fe07b7db82b789f5db">1: Integrating Image Captioning with Rule-based Entity Masking</a></td>
</tr>
<tr>
<td>0</td>
<td>0.778784</td>
<td>0.578617</td>
<td><a href="https://www.semanticscholar.org/paper/bd739f8f9ca6d02251ae01b866f83e03cdfc9935">8: Scene Text Recognition and Retrieval for Large Lexicons</a></td>
</tr>
<tr>
<td>0</td>
<td>0.766152</td>
<td>0.210694</td>
<td><a href="https://www.semanticscholar.org/paper/0875b99f11e4b1e1f8bbba5d2b1e23d8865b4a7c">2: Capturing Text Semantics for Concept Detection in News Video</a></td>
</tr>
<tr>
<td>0</td>
<td>0.760407</td>
<td>0.924202</td>
<td><a href="https://www.semanticscholar.org/paper/21fa67345e49642b8ebb22a59c4b2799a56e996f">88: Dense Captioning with Joint Inference and Visual Context</a></td>
</tr>
<tr>
<td>0</td>
<td>0.758160</td>
<td>0.859014</td>
<td><a href="https://www.semanticscholar.org/paper/a31c2b3a88719419cf679778846edbe3be9e81b3">1: What we see in a photograph: content selection for image captioning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.753737</td>
<td>0.856556</td>
<td><a href="https://www.semanticscholar.org/paper/3c8cf97f00cd8b4303eccc4134fa79b15cc3d564">9: Data-driven image captioning via salient region discovery</a></td>
</tr>
<tr>
<td>0</td>
<td>0.748154</td>
<td>0.729075</td>
<td><a href="https://www.semanticscholar.org/paper/16c6469bd85da1c982d190e24200f41b76c288a8">0: Caption-Based Region Extraction in Images</a></td>
</tr>
<tr>
<td>0</td>
<td>0.745026</td>
<td>0.557681</td>
<td><a href="https://www.semanticscholar.org/paper/3419ccd5c94d301ee08d716d037f0c3c6a62e78e">873: The Role of Context for Object Detection and Semantic Segmentation in the Wild</a></td>
</tr>
<tr>
<td>0</td>
<td>0.742274</td>
<td>0.911806</td>
<td><a href="https://www.semanticscholar.org/paper/2b9b7b5fb3f4450b6e3a267c4077bca4bf5997c8">2: Whoâ€™s Waldo? Linking People Across Text and Images</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793837</td>
<td>0.978085</td>
<td><a href="https://www.semanticscholar.org/paper/fc261c0efb5f9ce82581932d1440630b861fb85f">22: Grounded Situation Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.747090</td>
<td>0.975164</td>
<td><a href="https://www.semanticscholar.org/paper/3bbf2a343eda14e6cbf36ba2ae42663c95de37c7">29: Commonly Uncommon: Semantic Sparsity in Situation Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.738281</td>
<td>0.969161</td>
<td><a href="https://www.semanticscholar.org/paper/021b08b823700f8053afc54356e8d0ce57a3df71">28: Unsupervised Textual Grounding: Linking Words to Image Concepts</a></td>
</tr>
<tr>
<td>0</td>
<td>0.716607</td>
<td>0.966885</td>
<td><a href="https://www.semanticscholar.org/paper/8842a793e10d35f917345f7290fc671edd8cc7bf">24: Not All Frames Are Equal: Weakly-Supervised Video Grounding With Contextual Similarity and Visual Clustering Losses</a></td>
</tr>
<tr>
<td>0</td>
<td>0.620437</td>
<td>0.965543</td>
<td><a href="https://www.semanticscholar.org/paper/3dd4f855d0c794060a8a2fabe0639f4a4e792b45">74: Structured Matching for Phrase Localization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.732481</td>
<td>0.964518</td>
<td><a href="https://www.semanticscholar.org/paper/95f0125e6dda6c0028e09e814a7aaae5ef4922a4">18: Solving VIsual Madlibs with Multiple Cues</a></td>
</tr>
<tr>
<td>0</td>
<td>0.665507</td>
<td>0.963618</td>
<td><a href="https://www.semanticscholar.org/paper/936227f7483938097cc1cdd3032016df54dbd5b6">55: Learning to generalize to new compositions in image understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.751007</td>
<td>0.963586</td>
<td><a href="https://www.semanticscholar.org/paper/caffa07ead18aae78bf654bc57023eef58e74faf">0: Learning to detect visual relations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.697864</td>
<td>0.963354</td>
<td><a href="https://www.semanticscholar.org/paper/aeac614f10cb2a5dc000fdee30d857bbe5456ce5">65: Finding "It": Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos</a></td>
</tr>
</table></html>
