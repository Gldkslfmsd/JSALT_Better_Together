<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/ac11062f1f368d97f4c826c317bf50dcc13fdb59">257: Dissecting Contextual Word Embeddings: Architecture and Representation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.867804</td>
<td>0.991875</td>
<td><a href="https://www.semanticscholar.org/paper/f6fbb6809374ca57205bd2cf1421d4f4fa04f975">429: Linguistic Knowledge and Transferability of Contextual Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.865878</td>
<td>0.783071</td>
<td><a href="https://www.semanticscholar.org/paper/dac731a9bf707b35ed351950740654184db83938">0: Beyond Context: A New Perspective for Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.863580</td>
<td>0.986819</td>
<td><a href="https://www.semanticscholar.org/paper/3febb2bed8865945e7fddc99efd791887bb7e14f">7841: Deep Contextualized Word Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.863148</td>
<td>0.715103</td>
<td><a href="https://www.semanticscholar.org/paper/2cd823959202e91f9ffe8ba236d818c6baab57b1">12: An Unsupervised Character-Aware Neural Approach to Word and Context Representation Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.862428</td>
<td>0.903117</td>
<td><a href="https://www.semanticscholar.org/paper/5a0dfe2f44aa91478fb84864c565e9902a9344da">1: Analysing Word Representation from the Input and Output Embeddings in Neural Network Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.858981</td>
<td>0.964248</td>
<td><a href="https://www.semanticscholar.org/paper/3404d26159ba2a73526e7eac61a018a999d007fe">5: Pre-trained Contextualized Character Embeddings Lead to Major Improvements in Time Normalization: a Detailed Analysis</a></td>
</tr>
<tr>
<td>0</td>
<td>0.857122</td>
<td>0.765582</td>
<td><a href="https://www.semanticscholar.org/paper/7bd55627c21f507446b2e46dd6bda8944da8afb1">0: Contextualised Word Embeddings Based on Transfer Learning to Dialogue Response Generation: a Proposal and Comparisons</a></td>
</tr>
<tr>
<td>0</td>
<td>0.850363</td>
<td>0.939179</td>
<td><a href="https://www.semanticscholar.org/paper/35badb4a18900e649f18f58dc810b85937dfed98">0: LEARN FROM CONTEXT ? P ROBING FOR SENTENCE STRUCTURE IN CONTEXTUALIZED WORD REPRESENTATIONS</a></td>
</tr>
<tr>
<td>0</td>
<td>0.848243</td>
<td>0.966334</td>
<td><a href="https://www.semanticscholar.org/paper/fff9d7e9cc71d6ad7217dad9e5f4994c1c155e9f">0: Analyzing the Variation Property of Contextualized Word Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.727087</td>
<td>0.997713</td>
<td><a href="https://www.semanticscholar.org/paper/93b4cc549a1bc4bc112189da36c318193d05d806">880: AllenNLP: A Deep Semantic Natural Language Processing Platform</a></td>
</tr>
<tr>
<td>0</td>
<td>0.764532</td>
<td>0.996697</td>
<td><a href="https://www.semanticscholar.org/paper/c41516420ddbd0f29e010ca259a74c1fc2da0466">534: What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties</a></td>
</tr>
<tr>
<td>0</td>
<td>0.759490</td>
<td>0.996673</td>
<td><a href="https://www.semanticscholar.org/paper/f04df4e20a18358ea2f689b4c129781628ef7fc1">2477: A large annotated corpus for learning natural language inference</a></td>
</tr>
<tr>
<td>0</td>
<td>0.771407</td>
<td>0.995562</td>
<td><a href="https://www.semanticscholar.org/paper/9f1c5777a193b2c3bb2b25e248a156348e5ba56d">151: Cloze-driven Pretraining of Self-attention Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.665467</td>
<td>0.995052</td>
<td><a href="https://www.semanticscholar.org/paper/549c1a581b61f9ea47afc6f6871845392eaebbc4">89: LCQMC:A Large-scale Chinese Question Matching Corpus</a></td>
</tr>
<tr>
<td>0</td>
<td>0.694581</td>
<td>0.994732</td>
<td><a href="https://www.semanticscholar.org/paper/a0cabbd2572118bfaa1f0372a02514c96fcc99db">59: Humor Detection: A Transformer Gets the Last Laugh</a></td>
</tr>
<tr>
<td>0</td>
<td>0.750649</td>
<td>0.994574</td>
<td><a href="https://www.semanticscholar.org/paper/fdc0d45e08319a3078775f92048543d16f4f6e8a">25: Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages</a></td>
</tr>
<tr>
<td>0</td>
<td>0.776141</td>
<td>0.993990</td>
<td><a href="https://www.semanticscholar.org/paper/473921de1b52f98f34f37afd507e57366ff7d1ca">50: CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters</a></td>
</tr>
<tr>
<td>0</td>
<td>0.567248</td>
<td>0.993820</td>
<td><a href="https://www.semanticscholar.org/paper/435553998fbef790b5bed3491a8f634d9ec5cfa2">144: Efficient Natural Language Response Suggestion for Smart Reply</a></td>
</tr>
</table></html>
