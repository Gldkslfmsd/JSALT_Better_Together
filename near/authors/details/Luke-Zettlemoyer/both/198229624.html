<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/81f5810fbbab9b7203b9556f4ce3c741875407bc">854: SpanBERT: Improving Pre-training by Representing and Predicting Spans</a></td>
</tr>
<tr>
<td>0</td>
<td>0.761219</td>
<td>0.936231</td>
<td><a href="https://www.semanticscholar.org/paper/8617b501fedf65efaf82c3f911fe490407ba3650">7: BERT for Question Answering on SQuAD 2 . 0</a></td>
</tr>
<tr>
<td>0</td>
<td>0.755770</td>
<td>0.989269</td>
<td><a href="https://www.semanticscholar.org/paper/a54b56af24bb4873ed0163b77df63b92bd018ddc">1992: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a></td>
</tr>
<tr>
<td>0</td>
<td>0.745791</td>
<td>0.963612</td>
<td><a href="https://www.semanticscholar.org/paper/76778bddbeba9b7f854db2e988bfe76d43ccd691">1: Exploring Span Representations in Neural Coreference Resolution</a></td>
</tr>
<tr>
<td>0</td>
<td>0.744091</td>
<td>0.978844</td>
<td><a href="https://www.semanticscholar.org/paper/f89e2f4fb44e30b1adb08d153bf22b063597f896">1: Current Limitations of Language Models: What You Need is Retrieval</a></td>
</tr>
<tr>
<td>0</td>
<td>0.732018</td>
<td>0.969819</td>
<td><a href="https://www.semanticscholar.org/paper/0822f8d7e6a72a65e65f147d3a8d8fccd485da40">30: Shortformer: Better Language Modeling using Shorter Inputs</a></td>
</tr>
<tr>
<td>0</td>
<td>0.729610</td>
<td>0.991248</td>
<td><a href="https://www.semanticscholar.org/paper/7b99c51d562e33309a46601c846abbe72a65c6a4">22: What to Pre-Train on? Efficient Intermediate Task Selection</a></td>
</tr>
<tr>
<td>0</td>
<td>0.725815</td>
<td>0.773422</td>
<td><a href="https://www.semanticscholar.org/paper/f9ee2f592b9a43fe1ef41f773d329ac50631afe7">1: PRE-TRAINED LANGUAGE MODEL FINE-TUNING</a></td>
</tr>
<tr>
<td>0</td>
<td>0.723471</td>
<td>0.970469</td>
<td><a href="https://www.semanticscholar.org/paper/ff800ed01cbef800c79706c69a728b30a6e7fb24">2: Revisiting Memory-Efficient Incremental Coreference Resolution</a></td>
</tr>
<tr>
<td>0</td>
<td>0.721875</td>
<td>0.895528</td>
<td><a href="https://www.semanticscholar.org/paper/cefc6e55d53a4c46c9be4074e5803bf5e3b3d8c2">4: DirectQE: Direct Pretraining for Machine Translation Quality Estimation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.701913</td>
<td>0.999467</td>
<td><a href="https://www.semanticscholar.org/paper/80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef">325: ERNIE 2.0: A Continual Pre-training Framework for Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.620545</td>
<td>0.999139</td>
<td><a href="https://www.semanticscholar.org/paper/031e4e43aaffd7a479738dcea69a2d5be7957aa3">381: ERNIE: Enhanced Representation through Knowledge Integration</a></td>
</tr>
<tr>
<td>0</td>
<td>0.690902</td>
<td>0.998587</td>
<td><a href="https://www.semanticscholar.org/paper/6afe0fb12ceacadbbfed7202d430770a3f344731">138: Revisiting Pre-Trained Models for Chinese Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.748058</td>
<td>0.998140</td>
<td><a href="https://www.semanticscholar.org/paper/d0086b86103a620a86bc918746df0aa642e2a8a3">668: Language Models as Knowledge Bases?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.701491</td>
<td>0.998040</td>
<td><a href="https://www.semanticscholar.org/paper/2fe7dba5a58aee5156594b4d78634ecd6c7dcabd">275: End-to-End Open-Domain Question Answering with BERTserini</a></td>
</tr>
<tr>
<td>0</td>
<td>0.670825</td>
<td>0.997918</td>
<td><a href="https://www.semanticscholar.org/paper/bbde0f942a2541437e3edd55941ddafba6c5adb0">190: Transformers : State-ofthe-art Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.747754</td>
<td>0.997440</td>
<td><a href="https://www.semanticscholar.org/paper/2fa3f7ce620a1c7155daef6620dd6bb0e01934f3">375: Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.654408</td>
<td>0.997399</td>
<td><a href="https://www.semanticscholar.org/paper/b496b11fb2091678cc2d299cc778046d9a64b0a4">95: A BERT Baseline for the Natural Questions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.750323</td>
<td>0.997244</td>
<td><a href="https://www.semanticscholar.org/paper/b47381e04739ea3f392ba6c8faaf64105493c196">258: Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks</a></td>
</tr>
</table></html>
