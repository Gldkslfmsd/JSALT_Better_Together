<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/60ed82ca3ec8fbfef4d52e98e49ab687ce501a0c">146: Sparse Networks from Scratch: Faster Training without Losing Performance</a></td>
</tr>
<tr>
<td>0</td>
<td>0.783740</td>
<td>0.119504</td>
<td><a href="https://www.semanticscholar.org/paper/e6f9e565a59dfee29beacb59c4844bced9b12373">20: Building recurrent networks by unfolding iterative thresholding for sequential sparse recovery</a></td>
</tr>
<tr>
<td>0</td>
<td>0.774924</td>
<td>0.556409</td>
<td><a href="https://www.semanticscholar.org/paper/865031747807d12f0a10d6db1f7cab40e9549f8e">27: Training Neural Networks for and by Interpolation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.768053</td>
<td>0.231065</td>
<td><a href="https://www.semanticscholar.org/paper/82616585490603906c23fddf6ceebadf01179223">5: Stabilized Sparse Online Learning for Sparse Data</a></td>
</tr>
<tr>
<td>0</td>
<td>0.767215</td>
<td>0.916559</td>
<td><a href="https://www.semanticscholar.org/paper/248d22bd9e69caedaef5f276f6633dad75e46028">1: Activation Density driven Energy-Efficient Pruning in Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.765103</td>
<td>0.780242</td>
<td><a href="https://www.semanticscholar.org/paper/224d05a85191bee2cda1369368abf90938489644">0: Accelerating DNN Training Through Selective Localized Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.749582</td>
<td>0.047832</td>
<td><a href="https://www.semanticscholar.org/paper/2e2f71b454ac3290dd1d29130f743e79bedd4c32">2: Exact gradient updates in time independent of output size for the spherical loss family</a></td>
</tr>
<tr>
<td>0</td>
<td>0.746982</td>
<td>0.719041</td>
<td><a href="https://www.semanticscholar.org/paper/0c334b610b9ce71a0e3d41ed697604412e5c2aef">16: Momentum Residual Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741767</td>
<td>0.888862</td>
<td><a href="https://www.semanticscholar.org/paper/2e10560579f2bdeae0143141f26bd9f0a195b4b7">719: Mixed Precision Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741391</td>
<td>0.069022</td>
<td><a href="https://www.semanticscholar.org/paper/a4b3e2e095ed6e776e4ea4b81bac75885976218a">10: Online Sparse Passive Aggressive Learning with Kernels</a></td>
</tr>
<tr>
<td>0</td>
<td>0.777215</td>
<td>0.998623</td>
<td><a href="https://www.semanticscholar.org/paper/2e3002f131e1815bda7a10303eff97f79dea01ec">171: Rigging the Lottery: Making All Tickets Winners</a></td>
</tr>
<tr>
<td>0</td>
<td>0.777254</td>
<td>0.995618</td>
<td><a href="https://www.semanticscholar.org/paper/26384278cf5d575fc32cb92c303fb648fa0d5217">342: The State of Sparsity in Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.774231</td>
<td>0.995556</td>
<td><a href="https://www.semanticscholar.org/paper/229a4d27d04bd3901ef0ca41942eb0cdd4f28eed">20: Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.586458</td>
<td>0.995475</td>
<td><a href="https://www.semanticscholar.org/paper/03e0cbeb4604262446a97cb381874c7de1cffea2">70: The Lottery Ticket Hypothesis at Scale</a></td>
</tr>
<tr>
<td>0</td>
<td>0.635545</td>
<td>0.995408</td>
<td><a href="https://www.semanticscholar.org/paper/075da5ebbb890924267b4b163292ad21d0b100a0">118: Stabilizing the Lottery Ticket Hypothesis</a></td>
</tr>
<tr>
<td>0</td>
<td>0.541427</td>
<td>0.994585</td>
<td><a href="https://www.semanticscholar.org/paper/f7c410ab241bc972cda2f47993124ea8483003b6">188: Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask</a></td>
</tr>
<tr>
<td>0</td>
<td>0.789313</td>
<td>0.994582</td>
<td><a href="https://www.semanticscholar.org/paper/c703e42ac401ad734f440d56f6e19e6b2af86a60">136: Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.659487</td>
<td>0.994368</td>
<td><a href="https://www.semanticscholar.org/paper/a85ba5bb3e97c999f5f6dbc78f277b107af1dba2">15: Sparse Training via Boosting Pruning Plasticity with Neuroregeneration</a></td>
</tr>
<tr>
<td>0</td>
<td>0.725391</td>
<td>0.993197</td>
<td><a href="https://www.semanticscholar.org/paper/917b18b8dad23284c0a42f665f2ba1984fa360de">26: Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win</a></td>
</tr>
</table></html>
