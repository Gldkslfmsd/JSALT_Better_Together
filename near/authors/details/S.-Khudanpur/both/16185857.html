<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/efe03a2940e09547bb15035d35e7e07ed59848bf">215: JHU-ISI Gesture and Skill Assessment Working Set ( JIGSAWS ) : A Surgical Activity Dataset for Human Motion Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.732675</td>
<td>0.340855</td>
<td><a href="https://www.semanticscholar.org/paper/6c678a9efa572030fca3b05399073aa1e6266371">6: Gaze, visual, myoelectric, and inertial data of grasps for intelligent prosthetics</a></td>
</tr>
<tr>
<td>0</td>
<td>0.727168</td>
<td>0.338229</td>
<td><a href="https://www.semanticscholar.org/paper/900cba7cbae0150af5eeb17e6478d962e230929f">4: Gaze, Visual, Myoelectric, and Inertial Data of Grasps for Intelligent Prosthetics</a></td>
</tr>
<tr>
<td>0</td>
<td>0.718312</td>
<td>0.457486</td>
<td><a href="https://www.semanticscholar.org/paper/d9eebbbd543c511e9c5ab018416fba2c03091baf">134: Learning silhouette features for control of human motion</a></td>
</tr>
<tr>
<td>0</td>
<td>0.707635</td>
<td>0.072224</td>
<td><a href="https://www.semanticscholar.org/paper/47650b280f6e44034a0731bd730b49362fc4f074">15: Dynamic Monitoring Reveals Motor Task Characteristics in Prehistoric Technical Gestures</a></td>
</tr>
<tr>
<td>0</td>
<td>0.688282</td>
<td>0.379140</td>
<td><a href="https://www.semanticscholar.org/paper/ae19befbb48d93a7f8a05366d0ab940effd7dd97">4: Symbolic-Based Recognition of Contact States for Learning Assembly Skills</a></td>
</tr>
<tr>
<td>0</td>
<td>0.687300</td>
<td>0.508125</td>
<td><a href="https://www.semanticscholar.org/paper/a2c0c600e9a45bc6bad08ec881798bf40606959c">3: Comparison of Trajectories and Quaternions of Folk Dance Movements Using Dynamic Time Warping</a></td>
</tr>
<tr>
<td>0</td>
<td>0.686639</td>
<td>0.460989</td>
<td><a href="https://www.semanticscholar.org/paper/3fd89cf73d1705befe7a08dcc5df2abc6271d496">298: Enabling always-available input with muscle-computer interfaces</a></td>
</tr>
<tr>
<td>0</td>
<td>0.686057</td>
<td>0.497661</td>
<td><a href="https://www.semanticscholar.org/paper/e794305a3ec6ee95e1b3a2786947a26c2abea87c">0: Gaining a Sense of Touch. Physical Parameters Estimation using a Soft Gripper and Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.685529</td>
<td>0.306239</td>
<td><a href="https://www.semanticscholar.org/paper/5dac1b214711e09c87813ab25f9ef709a183b664">20: Learning task-specific models for reach to grasp movements: Towards EMG-based teleoperation of robotic arm-hand systems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.738516</td>
<td>0.958999</td>
<td><a href="https://www.semanticscholar.org/paper/9bd76023b5dae31eb29eb34b414d6fe8799dfcc3">11: Gesture Recognition in Robotic Surgery: A Review</a></td>
</tr>
<tr>
<td>0</td>
<td>0.760307</td>
<td>0.952980</td>
<td><a href="https://www.semanticscholar.org/paper/fe458179bd2f6d4d6ffe913a4cc2de3f23a8894e">15: Multi-Task Recurrent Neural Network for Surgical Gesture Recognition and Progress Prediction</a></td>
</tr>
<tr>
<td>0</td>
<td>0.743158</td>
<td>0.950865</td>
<td><a href="https://www.semanticscholar.org/paper/f233aa950840c1e2027ef1557b4e44c00299fc42">19: Temporal clustering of surgical activities in robot-assisted surgery</a></td>
</tr>
<tr>
<td>0</td>
<td>0.756567</td>
<td>0.946378</td>
<td><a href="https://www.semanticscholar.org/paper/c706e2b90ddb38138385a32ff758c854a34bb60c">14: Temporal Segmentation of Surgical Sub-tasks through Deep Learning with Multiple Data Sources</a></td>
</tr>
<tr>
<td>0</td>
<td>0.787858</td>
<td>0.946023</td>
<td><a href="https://www.semanticscholar.org/paper/88d7c3b8cc0d7aafc071f70309135d5925792981">64: Automated surgical skill assessment in RMIS training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.758909</td>
<td>0.935093</td>
<td><a href="https://www.semanticscholar.org/paper/924c6b9f1de65404aa9ced89fe3f3007374d582b">63: Video and accelerometer-based motion analysis for automated surgical skills assessment</a></td>
</tr>
<tr>
<td>0</td>
<td>0.785753</td>
<td>0.934247</td>
<td><a href="https://www.semanticscholar.org/paper/a4044c27188bd7279e406e0508f4a9548ba909c3">136: A Dataset and Benchmarks for Segmentation and Recognition of Gestures in Robotic Surgery</a></td>
</tr>
<tr>
<td>0</td>
<td>0.714195</td>
<td>0.930034</td>
<td><a href="https://www.semanticscholar.org/paper/4ab382d0a97b4a193b844d982218d23ed91e640f">93: Surgical Gesture Segmentation and Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.705058</td>
<td>0.929810</td>
<td><a href="https://www.semanticscholar.org/paper/96256c3374dd81548f7c906508130612fd4373bf">0: Clearness of operating field: a surrogate for surgical skills on in vivo clinical data</a></td>
</tr>
</table></html>
