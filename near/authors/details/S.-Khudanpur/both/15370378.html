<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/4030a62e75313110dc4a4c78483f4459dc4526bc">130: Parallel training of Deep Neural Networks with Natural Gradient and Parameter Averaging</a></td>
</tr>
<tr>
<td>1</td>
<td>0.966447</td>
<td>0.987320</td>
<td><a href="https://www.semanticscholar.org/paper/4e423c1e7e4d32b62ec2f952174c12ffeb916752">157: Parallel training of DNNs with Natural Gradient and Parameter Averaging</a></td>
</tr>
<tr>
<td>0</td>
<td>0.797277</td>
<td>0.385436</td>
<td><a href="https://www.semanticscholar.org/paper/ec64238ab45836fd4e34040326d3ef008e07dcd6">0: Winning Solution on LPIRC-ll Competition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792258</td>
<td>0.049714</td>
<td><a href="https://www.semanticscholar.org/paper/0d44f827ff819f5c56e578a2a1da88b2e1b238ad">0: WindTunnel</a></td>
</tr>
<tr>
<td>0</td>
<td>0.788476</td>
<td>0.664626</td>
<td><a href="https://www.semanticscholar.org/paper/7143230a68aecbce640e53b6cde171699a1e4270">26: A Hitchhiker's Guide On Distributed Training of Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.786110</td>
<td>0.576829</td>
<td><a href="https://www.semanticscholar.org/paper/d02bdac2e1abafcb0116862eb358da72a189fdfa">55: Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.785178</td>
<td>0.654073</td>
<td><a href="https://www.semanticscholar.org/paper/822e7515152e74626265c26b7aaccd2c654b5eba">57: Learning Gradient Descent: Better Generalization and Longer Horizons</a></td>
</tr>
<tr>
<td>0</td>
<td>0.780407</td>
<td>0.558738</td>
<td><a href="https://www.semanticscholar.org/paper/f55cc9f9aa01dfec4b8e8ef3c62990383b813999">0: A Reduced Memory Marquardt Algorlthm for Training Feedforward Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.778757</td>
<td>0.546892</td>
<td><a href="https://www.semanticscholar.org/paper/ce465c6830cb28a1f57cad2b9ee5f4084ab1f9c3">12: Principled Deep Neural Network Training through Linear Programming</a></td>
</tr>
<tr>
<td>0</td>
<td>0.776652</td>
<td>0.615935</td>
<td><a href="https://www.semanticscholar.org/paper/f4e5578dfe6483fd221b7118817297f7b70d3a27">138: Distributed Deep Learning Using Synchronous Stochastic Gradient Descent</a></td>
</tr>
<tr>
<td>0</td>
<td>0.646009</td>
<td>0.981918</td>
<td><a href="https://www.semanticscholar.org/paper/dfc1f71fd503d665ccb8481783f34b941fb02b2e">4: Distilling Knowledge Using Parallel Data for Far-field Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.641235</td>
<td>0.980018</td>
<td><a href="https://www.semanticscholar.org/paper/5824acc64a872cd07ce2c72a64b0836a9a8cba9c">0: Distilling Knowledge for Distant Speech Recognition via Parallel Data</a></td>
</tr>
<tr>
<td>0</td>
<td>0.673093</td>
<td>0.977133</td>
<td><a href="https://www.semanticscholar.org/paper/2904a5c43940a2d1da84d4c4a387cb17de987ffb">8: Generalized Distillation Framework for Speaker Normalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.515658</td>
<td>0.973624</td>
<td><a href="https://www.semanticscholar.org/paper/335f8860e2697e4c9aecc760978bf2139117f13c">51: An end-to-end approach to language identification in short utterances using convolutional neural networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.752954</td>
<td>0.973535</td>
<td><a href="https://www.semanticscholar.org/paper/1a6f5bc2a081b0a8eb6d7941aa54299a0e5aefb9">2: A Distributed Optimisation Framework Combining Natural Gradient with Hessian-Free for Discriminative Sequence Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.748173</td>
<td>0.972607</td>
<td><a href="https://www.semanticscholar.org/paper/e7508315922b3f53c92d8cdf4119f052c55cef12">8: Bayesian and Gaussian Process Neural Networks for Large Vocabulary Continuous Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.690249</td>
<td>0.971784</td>
<td><a href="https://www.semanticscholar.org/paper/c809fdaba2f15528b524733f8366fc2c19d06c49">13: A study of rank-constrained multilingual DNNS for low-resource ASR</a></td>
</tr>
<tr>
<td>0</td>
<td>0.705246</td>
<td>0.970789</td>
<td><a href="https://www.semanticscholar.org/paper/6928f47ec12dd406112b48967edd64a08e031df2">7: Cross-Entropy Training of DNN Ensemble Acoustic Models for Low-Resource ASR</a></td>
</tr>
</table></html>
