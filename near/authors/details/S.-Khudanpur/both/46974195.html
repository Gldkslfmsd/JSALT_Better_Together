<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/a6e4beb28b345fce7470da122b4e45e2cd0dcd12">95: A Time-Restricted Self-Attention Layer for ASR</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826943</td>
<td>0.890725</td>
<td><a href="https://www.semanticscholar.org/paper/48461e4cd0206e9cfb77f0663a0d2a4eae506864">11: Windowed Attention Mechanisms for Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.812817</td>
<td>0.975870</td>
<td><a href="https://www.semanticscholar.org/paper/b72c5236dacf2b958ebcf427d17a100bc54af504">23: Transformer ASR with Contextual Block Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800825</td>
<td>0.964419</td>
<td><a href="https://www.semanticscholar.org/paper/a88c5474d96a6629bd1f36c3c6c06242034cb0a4">9: How Much Self-Attention Do We Need∆í Trading Attention for Feed-Forward Layers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.797990</td>
<td>0.938309</td>
<td><a href="https://www.semanticscholar.org/paper/a00dc9cea49d5c2bd49558390bc4f755ae0b9d19">1: Generalizing RNN-Transducer to Out-Domain Audio via Sparse Self-Attention Layers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.794670</td>
<td>0.985580</td>
<td><a href="https://www.semanticscholar.org/paper/35de6075abf76e6e83101d2dbbe8270ee79a8dcd">15: Streaming Chunk-Aware Multihead Attention for Online End-to-End Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793136</td>
<td>0.974384</td>
<td><a href="https://www.semanticscholar.org/paper/7e464656d5fbc1fd347b2bf3bf86457aeb4fb4d4">0: Mutually-Constrained Monotonic Multihead Attention for Online ASR</a></td>
</tr>
<tr>
<td>0</td>
<td>0.780770</td>
<td>0.929586</td>
<td><a href="https://www.semanticscholar.org/paper/b837a453d4046100c84b1232f705c1b08903400c">6: Multilingual Speech Recognition with Self-Attention Structured Parameterization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.777836</td>
<td>0.899345</td>
<td><a href="https://www.semanticscholar.org/paper/3cefc68260c429b5914b1b3529dce8a229aecec3">29: Neural Machine Translation With GRU-Gated Attention Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.775009</td>
<td>0.937745</td>
<td><a href="https://www.semanticscholar.org/paper/4d68c1b4167f858979c6a8e8b9ad0b484cd48c63">1: Cross Aggregation of Multi-head Attention for Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.744890</td>
<td>0.994835</td>
<td><a href="https://www.semanticscholar.org/paper/abd91aca4d78799492256b406f5abc199d3802e4">45: Multilingual End-to-End Speech Recognition with A Single Transformer on Low-Resource Languages</a></td>
</tr>
<tr>
<td>0</td>
<td>0.784010</td>
<td>0.994626</td>
<td><a href="https://www.semanticscholar.org/paper/53dcd6068586d50169877d145df550ff3f568221">94: Syllable-Based Sequence-to-Sequence Speech Recognition with the Transformer in Mandarin Chinese</a></td>
</tr>
<tr>
<td>0</td>
<td>0.825822</td>
<td>0.992972</td>
<td><a href="https://www.semanticscholar.org/paper/e54f6751476ba07928b3f78c097150aa610acffc">52: Self-attention Aligner: A Latency-control End-to-end Model for ASR Using Self-attention Network and Chunk-hopping</a></td>
</tr>
<tr>
<td>0</td>
<td>0.776791</td>
<td>0.992528</td>
<td><a href="https://www.semanticscholar.org/paper/bc1ab519c225b08332f243269ad6d99284bbf1bf">81: A Comparison of Transformer and LSTM Encoder Decoder Models for ASR</a></td>
</tr>
<tr>
<td>0</td>
<td>0.778070</td>
<td>0.991267</td>
<td><a href="https://www.semanticscholar.org/paper/768e5f9b019c27babbfaf817a5bb20316b9df113">26: Streaming Transformer-based Acoustic Models Using Self-attention with Augmented Memory</a></td>
</tr>
<tr>
<td>0</td>
<td>0.737650</td>
<td>0.990912</td>
<td><a href="https://www.semanticscholar.org/paper/aa50768a67b9f3f578ab5eb300a248ebc33b7da6">44: The Speechtransformer for Large-scale Mandarin Chinese Speech Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.796762</td>
<td>0.990781</td>
<td><a href="https://www.semanticscholar.org/paper/c52ac453e154953abdb06fc041023e327ea609a4">91: Self-Attentional Acoustic Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793997</td>
<td>0.990765</td>
<td><a href="https://www.semanticscholar.org/paper/45c0e9d42624d02752b2c5adce6032db1deb2940">91: Streaming Automatic Speech Recognition with the Transformer Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.785116</td>
<td>0.990636</td>
<td><a href="https://www.semanticscholar.org/paper/8513f5c709d3a744e68116faf3f51026f9a4fee9">17: Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition</a></td>
</tr>
</table></html>
