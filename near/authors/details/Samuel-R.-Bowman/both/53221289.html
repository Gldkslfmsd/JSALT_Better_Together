<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/b47381e04739ea3f392ba6c8faaf64105493c196">258: Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.840445</td>
<td>0.981833</td>
<td><a href="https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe">5985: Language Models are Unsupervised Multitask Learners</a></td>
</tr>
<tr>
<td>0</td>
<td>0.838287</td>
<td>0.937963</td>
<td><a href="https://www.semanticscholar.org/paper/2607dce6dcb9043ca9cae67e25e6a24411f08c0b">4: BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.838244</td>
<td>0.967759</td>
<td><a href="https://www.semanticscholar.org/paper/ea0cf15285a37a506e594f2fb2a4e9928ea3a352">7: Style Attuned Pre-training and Parameter Efficient Fine-tuning for Spoken Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.836082</td>
<td>0.908306</td>
<td><a href="https://www.semanticscholar.org/paper/9dc68f8362df16b2b7f17556a7728fe727055c6a">16: AmericasNLI: Evaluating Zero-shot Natural Language Understanding of Pretrained Multilingual Models in Truly Low-resource Languages</a></td>
</tr>
<tr>
<td>0</td>
<td>0.832341</td>
<td>0.805368</td>
<td><a href="https://www.semanticscholar.org/paper/2368f712f12ab91218e3ddbaaa662830832e4e15">10: Pretrained Language Models for Document-Level Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.827255</td>
<td>0.572247</td>
<td><a href="https://www.semanticscholar.org/paper/1fd8b00bc89870a762d2f66788f422481ca5c44b">4: Training ASR Models By Generation of Contextual Information</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826837</td>
<td>0.955700</td>
<td><a href="https://www.semanticscholar.org/paper/f4061bd225b3be5b3f5b18eb1a229ce991efefeb">519: PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826364</td>
<td>0.654467</td>
<td><a href="https://www.semanticscholar.org/paper/ab207f929d0539e7189e36ad1a4ad4bb9586c5ee">1: Self-Training for Compositional Neural NLG in Task-Oriented Dialogue</a></td>
</tr>
<tr>
<td>0</td>
<td>0.825753</td>
<td>0.576572</td>
<td><a href="https://www.semanticscholar.org/paper/fb3a6098945bcb8b48b1f8a21c9a0edcb38defe0">2: Towards better translation performance on spoken language</a></td>
</tr>
<tr>
<td>0</td>
<td>0.760669</td>
<td>0.998666</td>
<td><a href="https://www.semanticscholar.org/paper/e816f788767eec6a8ef0ea9eddd0e902435d4271">720: Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.723482</td>
<td>0.997914</td>
<td><a href="https://www.semanticscholar.org/paper/4f03e69963b9649950ba29ae864a0de8c14f1f86">158: K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</a></td>
</tr>
<tr>
<td>0</td>
<td>0.765471</td>
<td>0.997645</td>
<td><a href="https://www.semanticscholar.org/paper/fa7e728c4c612025b9fb7601af65c4a8f5a2b33e">0: Clinical Prompt Learning with Frozen Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.790996</td>
<td>0.997499</td>
<td><a href="https://www.semanticscholar.org/paper/a49e9a8d29b5838ba392d5d33fb9694f4667c59e">20: BERTese: Learning to Speak to BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.727986</td>
<td>0.997288</td>
<td><a href="https://www.semanticscholar.org/paper/d9f6ada77448664b71128bb19df15765336974a6">797: SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.750323</td>
<td>0.997244</td>
<td><a href="https://www.semanticscholar.org/paper/81f5810fbbab9b7203b9556f4ce3c741875407bc">854: SpanBERT: Improving Pre-training by Representing and Predicting Spans</a></td>
</tr>
<tr>
<td>0</td>
<td>0.785416</td>
<td>0.997242</td>
<td><a href="https://www.semanticscholar.org/paper/756810258e3419af76aff38c895c20343b0602d0">1134: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792781</td>
<td>0.997051</td>
<td><a href="https://www.semanticscholar.org/paper/18318b10e7c2dd4ad292208f4399eb1d4dca5768">106: CLUE: A Chinese Language Understanding Evaluation Benchmark</a></td>
</tr>
<tr>
<td>0</td>
<td>0.612658</td>
<td>0.997049</td>
<td><a href="https://www.semanticscholar.org/paper/81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85">248: How Can We Know What Language Models Know?</a></td>
</tr>
</table></html>
