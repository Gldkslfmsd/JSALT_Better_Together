<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/93b8da28d006415866bf48f9a6e06b5242129195">2544: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.934123</td>
<td>-0.000370</td>
<td>NA:13749111</td>
</tr>
<tr>
<td>0</td>
<td>0.846945</td>
<td>0.920233</td>
<td><a href="https://www.semanticscholar.org/paper/a201c722c7de07b7354dda9cdabf9baf7e6e2ec0">0: Open-Ended Generative Commonsense Question Answering with Knowledge Graph-enhanced Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.839853</td>
<td>0.820132</td>
<td><a href="https://www.semanticscholar.org/paper/6a33e798c64d0cbf9015d5ad444c7f2cef3bce81">16: Reading Twice for Natural Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.839172</td>
<td>0.795878</td>
<td><a href="https://www.semanticscholar.org/paper/3fdd1f6ea4e309a12b7a5e75a661246d1698522e">1: Adaptive Cross-Lingual Question Generation with Minimal Resources</a></td>
</tr>
<tr>
<td>0</td>
<td>0.838476</td>
<td>0.790902</td>
<td><a href="https://www.semanticscholar.org/paper/5127dd9446a61e08aa1d68420ac8e4bc3f243b83">6: An Evaluation of Language-Agnostic Inner-Attention-Based Representations in Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.830971</td>
<td>0.860377</td>
<td><a href="https://www.semanticscholar.org/paper/f3280d95a5a05fb2ae2154532cd3c32d5d757c02">0: IMPLI: Investigating NLI Models’ Performance on Figurative Language</a></td>
</tr>
<tr>
<td>0</td>
<td>0.828814</td>
<td>0.651622</td>
<td><a href="https://www.semanticscholar.org/paper/f07646d92d6bb17ce3b0cee7e353e00e3a033680">1: Mutlitask Learning for Cross-Lingual Transfer of Semantic Dependencies</a></td>
</tr>
<tr>
<td>0</td>
<td>0.825145</td>
<td>0.896270</td>
<td><a href="https://www.semanticscholar.org/paper/1bd29e8578f972ab2edbb32ed29418a145fe627f">1: BEAMetrics: A Benchmark for Language Generation Evaluation Evaluation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.821908</td>
<td>0.949058</td>
<td><a href="https://www.semanticscholar.org/paper/3b7deb3ff648776e11f55a235360bd40a6c15ffb">0: CoDA21: Evaluating Language Understanding Capabilities of NLP Models With Context-Definition Alignment</a></td>
</tr>
<tr>
<td>0</td>
<td>0.750909</td>
<td>0.998588</td>
<td><a href="https://www.semanticscholar.org/paper/7a064df1aeada7e69e5173f7d4c8606f4470365b">2612: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.588174</td>
<td>0.998452</td>
<td><a href="https://www.semanticscholar.org/paper/d78aed1dac6656affa4a04cbf225ced11a83d103">285: Revealing the Dark Secrets of BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.723697</td>
<td>0.997821</td>
<td><a href="https://www.semanticscholar.org/paper/756810258e3419af76aff38c895c20343b0602d0">1134: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a></td>
</tr>
<tr>
<td>0</td>
<td>0.795461</td>
<td>0.997679</td>
<td><a href="https://www.semanticscholar.org/paper/d9f6ada77448664b71128bb19df15765336974a6">797: SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.753207</td>
<td>0.997622</td>
<td><a href="https://www.semanticscholar.org/paper/a54b56af24bb4873ed0163b77df63b92bd018ddc">1992: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a></td>
</tr>
<tr>
<td>0</td>
<td>0.682536</td>
<td>0.997470</td>
<td><a href="https://www.semanticscholar.org/paper/2f9d4887d0022400fc40c774c4c78350c3bc5390">75: Small and Practical BERT Models for Sequence Labeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.638939</td>
<td>0.996224</td>
<td><a href="https://www.semanticscholar.org/paper/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de">6919: RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></td>
</tr>
<tr>
<td>0</td>
<td>0.782118</td>
<td>0.996170</td>
<td><a href="https://www.semanticscholar.org/paper/e816f788767eec6a8ef0ea9eddd0e902435d4271">720: Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.778813</td>
<td>0.995855</td>
<td><a href="https://www.semanticscholar.org/paper/b47381e04739ea3f392ba6c8faaf64105493c196">258: Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks</a></td>
</tr>
</table></html>
