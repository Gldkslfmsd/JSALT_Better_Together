<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/673e970fd835c7dd1bb1e071c5a37e9df99b7c8e">43: Intermediate-Task Transfer Learning with Pretrained Models for Natural Language Understanding: When and Why Does It Work?</a></td>
</tr>
<tr>
<td>1</td>
<td>0.988400</td>
<td>0.999062</td>
<td><a href="https://www.semanticscholar.org/paper/9e594ae4ae9c38b6495810a8872f513ae19be29c">85: Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.840851</td>
<td>-0.008605</td>
<td><a href="https://www.semanticscholar.org/paper/9784bf6b4a0b16aff1817c760f48b16ff1e97bea">0: When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.832128</td>
<td>0.973609</td>
<td><a href="https://www.semanticscholar.org/paper/0f1107494c77a6aa559c52f8b37aede31398e334">32: Investigating Transferability in Pretrained Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.815187</td>
<td>0.974537</td>
<td><a href="https://www.semanticscholar.org/paper/2f291b0b59483e9c3c4a3391f34e6b29aff848a1">0: DeepStruct: Pretraining of Language Models for Structure Prediction</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811416</td>
<td>0.991273</td>
<td><a href="https://www.semanticscholar.org/paper/d6599d4dfaeb78bea1f975db683aa653e26b3987">28: Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800887</td>
<td>0.955251</td>
<td><a href="https://www.semanticscholar.org/paper/7e4d5dad222877e0b526de63b37e548cd4b2c0ae">2: A Thorough Evaluation of Task-Specific Pretraining for Summarization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800772</td>
<td>0.986951</td>
<td><a href="https://www.semanticscholar.org/paper/40b4d98588719407fb72a014ab79e4145695654b">0: Quantifying Adaptability in Pre-trained Language Models with 500 Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.799235</td>
<td>0.995667</td>
<td><a href="https://www.semanticscholar.org/paper/789b5441743c2e38cf4c38749ed820c0671d81b1">60: Muppet: Massive Multi-task Representations with Pre-Finetuning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.797337</td>
<td>0.991020</td>
<td><a href="https://www.semanticscholar.org/paper/246fd63312287e3b629d6baa49c8842123d5a7a9">2: REPT: Bridging Language Models and Machine Reading Comprehension via Retrieval-Based Pre-training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792142</td>
<td>0.998242</td>
<td><a href="https://www.semanticscholar.org/paper/31392ad8722d9c66181b621936e2013199e02edc">31: When Do You Need Billions of Words of Pretraining Data?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.718372</td>
<td>0.997992</td>
<td><a href="https://www.semanticscholar.org/paper/636904d91d9dd1a641a595d9578ba7640f35aa74">120: MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.709079</td>
<td>0.997932</td>
<td><a href="https://www.semanticscholar.org/paper/5679431425a81c07bcafa521e5609cc05b3ec5dc">7: Neural Retrieval for Question Answering with Cross-Attention Supervised Data Augmentation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.822084</td>
<td>0.997834</td>
<td><a href="https://www.semanticscholar.org/paper/7b99c51d562e33309a46601c846abbe72a65c6a4">22: What to Pre-Train on? Efficient Intermediate Task Selection</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741167</td>
<td>0.997724</td>
<td><a href="https://www.semanticscholar.org/paper/00b30ed463625da04166eb78ca617539b41a9846">50: jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.586184</td>
<td>0.997204</td>
<td><a href="https://www.semanticscholar.org/paper/07c1c2429b63fefdae41eb546c31b40de2a880f7">26: INFOTABS: Inference on Tables as Semi-structured Data</a></td>
</tr>
<tr>
<td>0</td>
<td>0.747048</td>
<td>0.997135</td>
<td><a href="https://www.semanticscholar.org/paper/45436d6f52832154bf7a90ae02ce8bc302802b4d">16: How Additional Knowledge can Improve Natural Language Commonsense Question Answering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.729651</td>
<td>0.997037</td>
<td><a href="https://www.semanticscholar.org/paper/14489ec7893e373a0dcc9555c52b99b2b3a429f6">100: Are All Languages Created Equal in Multilingual BERT?</a></td>
</tr>
</table></html>
