<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/0e012c2bd18236445cfbc6e3e409eb02df4691fe">23: Can neural networks acquire a structural bias from raw linguistic data?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.821434</td>
<td>0.811717</td>
<td><a href="https://www.semanticscholar.org/paper/c5cd436252a3c395f25f13810d0a93f0c803f222">3: Structure Here, Bias There: Hierarchical Generalization by Jointly Learning Syntactic Transformations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.816728</td>
<td>0.342618</td>
<td><a href="https://www.semanticscholar.org/paper/26431edcecf7d6e37bda6368a5235d5579164d76">177: Learning biases predict a word order universal</a></td>
</tr>
<tr>
<td>0</td>
<td>0.803774</td>
<td>0.844981</td>
<td><a href="https://www.semanticscholar.org/paper/dd9dc03c2f72724f91bc84d92e4e82160aac4523">52: Revisiting the poverty of the stimulus: hierarchical generalization without a hierarchical bias in recurrent neural networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792594</td>
<td>0.780954</td>
<td><a href="https://www.semanticscholar.org/paper/4bc2bb6584774b0d8ad0b4f5215dc2075487c192">64: A Benchmark for Systematic Generalization in Grounded Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.784282</td>
<td>0.737951</td>
<td><a href="https://www.semanticscholar.org/paper/1d3b07adeb810f5f60dff8ac82734189dc8eb5d8">6: Neural Language Models Capture Some, But Not All Agreement Attraction Effects</a></td>
</tr>
<tr>
<td>0</td>
<td>0.777258</td>
<td>0.645793</td>
<td><a href="https://www.semanticscholar.org/paper/2b060b89324c376892a096c84fd14664f7b71710">1: Understanding Robust Generalization in Learning Regular Languages</a></td>
</tr>
<tr>
<td>0</td>
<td>0.776711</td>
<td>0.312910</td>
<td><a href="https://www.semanticscholar.org/paper/a937460575fe056a53ba4d582c94816077be33e0">0: Language Learning in the Full Or, Why the Stimulus Might Not Be so Poor after All</a></td>
</tr>
<tr>
<td>0</td>
<td>0.771687</td>
<td>0.324141</td>
<td><a href="https://www.semanticscholar.org/paper/6bd728f719965b39fca681e2f60a0d1ba6ba6733">6: Pronouns in competition: Predicting acquisition delays cross-linguistically</a></td>
</tr>
<tr>
<td>0</td>
<td>0.771162</td>
<td>0.774185</td>
<td><a href="https://www.semanticscholar.org/paper/61c49e6399e76a96ebc82dbe486131db6c07b772">2: Relational reasoning and generalization using non-symbolic neural networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.740064</td>
<td>0.986484</td>
<td><a href="https://www.semanticscholar.org/paper/8c25f38044b69f54233803c280677c3f8d547e9f">0: Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.744858</td>
<td>0.986336</td>
<td><a href="https://www.semanticscholar.org/paper/4548c8e706599f71fdcaa1bb7b278ef07e6e5d69">11: Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.706957</td>
<td>0.985712</td>
<td><a href="https://www.semanticscholar.org/paper/0f4f27bb267b238d6044375863335db7fe69d661">0: This is a BERT. Now there are several of them. Can they generalize to novel words?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.781002</td>
<td>0.985568</td>
<td><a href="https://www.semanticscholar.org/paper/a418c0daa98a3639e1b1bd682c68644250259944">1: Transformers in the loop: Polarity in neural models of language</a></td>
</tr>
<tr>
<td>0</td>
<td>0.779110</td>
<td>0.983823</td>
<td><a href="https://www.semanticscholar.org/paper/c8ff35e58c7d8db1024cd898cdf422452a11aefd">1: Uncovering Constraint-Based Behavior in Neural Models via Targeted Fine-Tuning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.769525</td>
<td>0.983339</td>
<td><a href="https://www.semanticscholar.org/paper/1b553c34270543a36ca12784821a7817b36e66ad">19: Grammatical Analysis of Pretrained Sentence Encoders with Acceptability Judgments</a></td>
</tr>
<tr>
<td>0</td>
<td>0.753700</td>
<td>0.982375</td>
<td><a href="https://www.semanticscholar.org/paper/8c5488623b9a012909f3bbdda508b538a7bb544c">1: Schr\"odinger's Tree -- On Syntax and Neural Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.798189</td>
<td>0.982139</td>
<td><a href="https://www.semanticscholar.org/paper/38a91371928f8ec3ad0f0d3f71af06e1efc0ef9d">0: Do pretrained transformers infer telicity like humans?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.762217</td>
<td>0.981725</td>
<td><a href="https://www.semanticscholar.org/paper/b56e2e7b93be127c953b6ad18230d5905051d23b">118: BLiMP: A Benchmark of Linguistic Minimal Pairs for English</a></td>
</tr>
</table></html>
