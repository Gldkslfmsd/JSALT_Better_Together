<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/2fe7dba5a58aee5156594b4d78634ecd6c7dcabd">275: End-to-End Open-Domain Question Answering with BERTserini</a></td>
</tr>
<tr>
<td>0</td>
<td>0.827049</td>
<td>0.933135</td>
<td><a href="https://www.semanticscholar.org/paper/ddd2c4a0f68a41fb4d8faedc3e1bc4ab965c5483">1: Revisiting the Open-Domain Question Answering Pipeline</a></td>
</tr>
<tr>
<td>0</td>
<td>0.801480</td>
<td>0.848694</td>
<td><a href="https://www.semanticscholar.org/paper/e9b73b3b5bfd09dd77db332d80ea3e5e0d1d8f84">0: The Role of Data in Bert Question-Answering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.799160</td>
<td>0.726861</td>
<td><a href="https://www.semanticscholar.org/paper/66b796ac43dd1dc59970d2f93ee05492125f9eb9">0: Question Answering as an Automatic Summarization Evaluation Metric</a></td>
</tr>
<tr>
<td>0</td>
<td>0.797392</td>
<td>0.571278</td>
<td><a href="https://www.semanticscholar.org/paper/adfd824da32849941e87ece541eeeed2c0459705">0: SQuAD Reading Comprehension with Coattention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.794030</td>
<td>0.960293</td>
<td><a href="https://www.semanticscholar.org/paper/f7df82c5417b9ec7582def05b79ca080a07c4f3b">164: R3: Reinforced Ranker-Reader for Open-Domain Question Answering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.791883</td>
<td>0.861263</td>
<td><a href="https://www.semanticscholar.org/paper/a1e79bc3717486b311488bc67b319b3f6a44da14">53: Self-Training for Jointly Learning to Ask and Answer Questions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.788826</td>
<td>0.886634</td>
<td><a href="https://www.semanticscholar.org/paper/3ad4ab5c4f0c7bd537739fc19f9b7b54dc594fdf">2: Transfer learning for question answering on SQuAD</a></td>
</tr>
<tr>
<td>0</td>
<td>0.787631</td>
<td>0.456028</td>
<td><a href="https://www.semanticscholar.org/paper/b0069132b52e2e07b000a3959671aebd3256296f">3: Evaluation of baseline information retrieval for Polish open-domain Question Answering system</a></td>
</tr>
<tr>
<td>0</td>
<td>0.786746</td>
<td>0.949596</td>
<td><a href="https://www.semanticscholar.org/paper/79cb9bca730a4c5bbe73d97c1f40da1d0debe568">0: Long Context Question Answering via Supervised Contrastive Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.595167</td>
<td>0.998224</td>
<td><a href="https://www.semanticscholar.org/paper/80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef">325: ERNIE 2.0: A Continual Pre-training Framework for Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.701491</td>
<td>0.998040</td>
<td><a href="https://www.semanticscholar.org/paper/81f5810fbbab9b7203b9556f4ce3c741875407bc">854: SpanBERT: Improving Pre-training by Representing and Predicting Spans</a></td>
</tr>
<tr>
<td>0</td>
<td>0.738552</td>
<td>0.997533</td>
<td><a href="https://www.semanticscholar.org/paper/d0086b86103a620a86bc918746df0aa642e2a8a3">668: Language Models as Knowledge Bases?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.605063</td>
<td>0.997496</td>
<td><a href="https://www.semanticscholar.org/paper/14489ec7893e373a0dcc9555c52b99b2b3a429f6">100: Are All Languages Created Equal in Multilingual BERT?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.737519</td>
<td>0.997458</td>
<td><a href="https://www.semanticscholar.org/paper/5679431425a81c07bcafa521e5609cc05b3ec5dc">7: Neural Retrieval for Question Answering with Cross-Attention Supervised Data Augmentation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.572853</td>
<td>0.997430</td>
<td><a href="https://www.semanticscholar.org/paper/031e4e43aaffd7a479738dcea69a2d5be7957aa3">381: ERNIE: Enhanced Representation through Knowledge Integration</a></td>
</tr>
<tr>
<td>0</td>
<td>0.630968</td>
<td>0.997150</td>
<td><a href="https://www.semanticscholar.org/paper/636904d91d9dd1a641a595d9578ba7640f35aa74">120: MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.710280</td>
<td>0.997095</td>
<td><a href="https://www.semanticscholar.org/paper/65be695739d0fa35212e49ccccd129535e6d9e15">40: Understanding tables with intermediate pre-training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.729123</td>
<td>0.997021</td>
<td><a href="https://www.semanticscholar.org/paper/c7fc1cac162c0e2a934704184c7554fd6b6253f0">104: Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model</a></td>
</tr>
</table></html>
