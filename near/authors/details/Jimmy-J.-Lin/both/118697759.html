<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/1a9954d86466a7e4de6f98ddee452ceb50e15d86">160: DocBERT: BERT for Document Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.803441</td>
<td>0.817862</td>
<td><a href="https://www.semanticscholar.org/paper/af5eb1b8b89888a7f65040dd31356d071e8dcf03">0: Construction of document feature vectors using BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.761142</td>
<td>0.168718</td>
<td><a href="https://www.semanticscholar.org/paper/1e94dc17cc27c33102d94bde62bb5c81b405f27f">0: BdLAN:BERTdoc Label Attention Networks for Multi-label text classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.761130</td>
<td>0.966478</td>
<td><a href="https://www.semanticscholar.org/paper/97906df07855b029b7aae7c2a1c6c5e8df1d531c">653: BERT Rediscovers the Classical NLP Pipeline</a></td>
</tr>
<tr>
<td>0</td>
<td>0.759908</td>
<td>0.962037</td>
<td><a href="https://www.semanticscholar.org/paper/7dcb6a4b7b1a1d2317c3058f529c5dbcceebbcae">0: Easy-to-use Combination of POS and BERT Model for Domain-Specific and Misspelled Terms</a></td>
</tr>
<tr>
<td>0</td>
<td>0.750141</td>
<td>0.732504</td>
<td><a href="https://www.semanticscholar.org/paper/1ef3827636f2beaaac2e3a1712555a281a6c053b">2: BE-BLC: BERT-ELMO-Based Deep Neural Network Architecture for English Named Entity Recognition Task</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741851</td>
<td>0.961868</td>
<td><a href="https://www.semanticscholar.org/paper/665b38fe83b1bc155de3319a72ca4e6a6696f9f2">83: Taming Pretrained Transformers for Extreme Multi-label Text Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741784</td>
<td>0.941864</td>
<td><a href="https://www.semanticscholar.org/paper/6482dbb26e6441c0cfdaa43c52b868751d16d10b">4: Contextual BERT: Conditioning the Language Model Using a Global State</a></td>
</tr>
<tr>
<td>0</td>
<td>0.740605</td>
<td>0.866825</td>
<td><a href="https://www.semanticscholar.org/paper/29be3eec992d4ce79ea732af510573fb4f436d52">2: To Share or not to Share: Predicting Sets of Sources for Model Transfer Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.740338</td>
<td>0.953465</td>
<td><a href="https://www.semanticscholar.org/paper/c2dfc728abf4127fdb24195143ce3d2059418f0e">16: Establishing Baselines for Text Classification in Low-Resource Languages</a></td>
</tr>
<tr>
<td>0</td>
<td>0.797123</td>
<td>0.993213</td>
<td><a href="https://www.semanticscholar.org/paper/a022bda79947d1f656a1164003c1b3ae9a843df9">601: How to Fine-Tune BERT for Text Classification?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.676596</td>
<td>0.992924</td>
<td><a href="https://www.semanticscholar.org/paper/3febb2bed8865945e7fddc99efd791887bb7e14f">7841: Deep Contextualized Word Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.744123</td>
<td>0.992670</td>
<td><a href="https://www.semanticscholar.org/paper/1e077413b25c4d34945cc2707e17e46ed4fe784a">2192: Universal Language Model Fine-tuning for Text Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.645741</td>
<td>0.992269</td>
<td><a href="https://www.semanticscholar.org/paper/bd2fbe5f6c7f26851c004e659a1e38a8f5005d80">68: Fine-grained Sentiment Classification using BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.685743</td>
<td>0.990281</td>
<td><a href="https://www.semanticscholar.org/paper/4f2841cf0ed8edd5f9d2b7f76b95ec2a8674afb1">101: Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.725920</td>
<td>0.989952</td>
<td><a href="https://www.semanticscholar.org/paper/3a7bbc46795929f0eace82b64c44c92a48682fb5">336: FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP</a></td>
</tr>
<tr>
<td>0</td>
<td>0.606145</td>
<td>0.989810</td>
<td><a href="https://www.semanticscholar.org/paper/1e43c7084bdcb6b3102afaf301cce10faead2702">1923: BioBERT: a pre-trained biomedical language representation model for biomedical text mining</a></td>
</tr>
<tr>
<td>0</td>
<td>0.655567</td>
<td>0.989454</td>
<td><a href="https://www.semanticscholar.org/paper/a4bc4b98a917174ac2ab14bd5e66d64306079ab5">286: BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis</a></td>
</tr>
<tr>
<td>0</td>
<td>0.644887</td>
<td>0.989377</td>
<td><a href="https://www.semanticscholar.org/paper/02c408e66604094d67cfad5c79c2eecc6879c4fd">12: Understanding BERT performance in propaganda analysis</a></td>
</tr>
</table></html>
