<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/90a1491ac32e732c93773354e4e665794ed4d490">97: DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference</a></td>
</tr>
<tr>
<td>0</td>
<td>0.848048</td>
<td>0.995519</td>
<td><a href="https://www.semanticscholar.org/paper/5d34881ff68bd203ff790187e7e5c9e034389cfa">112: FastBERT: a Self-distilling BERT with Adaptive Inference Time</a></td>
</tr>
<tr>
<td>1</td>
<td>0.805949</td>
<td>0.996631</td>
<td><a href="https://www.semanticscholar.org/paper/217913c84a4bdbe5cee3630d70480fda8d44bfb0">2: Magic Pyramid: Accelerating Inference with Early Exiting and Token Pruning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800522</td>
<td>0.983865</td>
<td><a href="https://www.semanticscholar.org/paper/cbde5598c1a78285adfcfd77fb3636f5498987a0">2: EBERT: Efficient BERT Inference with Dynamic Structured Pruning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800316</td>
<td>0.992141</td>
<td><a href="https://www.semanticscholar.org/paper/4510d9ad22f474c30c530ae7f886ec4d42402d68">15: PoWER-BERT: Accelerating BERT inference for Classification Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.786495</td>
<td>0.985312</td>
<td><a href="https://www.semanticscholar.org/paper/e71fff3eb700250dc4f6e2c17626e4743bba300d">0: Can depth-adaptive BERT perform better on binary classification tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.783241</td>
<td>0.981912</td>
<td><a href="https://www.semanticscholar.org/paper/91e10d89fe5a9c8c43e9a7262bf4756838fa4dd4">1: SDSK2BERT: Explore the Specific Depth with Specific Knowledge to Compress BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.780510</td>
<td>0.963613</td>
<td><a href="https://www.semanticscholar.org/paper/47354f49a4768719add414ea853977cb868faf25">2: AutoBERT-Zero: Evolving BERT Backbone from Scratch</a></td>
</tr>
<tr>
<td>0</td>
<td>0.776394</td>
<td>0.824952</td>
<td><a href="https://www.semanticscholar.org/paper/30a9b6da70349f1825ea53dd03d52a465b6d44b7">0: Teaching BERT to Wait: Balancing Accuracy and Latency for Streaming Disfluency Detection</a></td>
</tr>
<tr>
<td>0</td>
<td>0.764731</td>
<td>0.958220</td>
<td><a href="https://www.semanticscholar.org/paper/5de7c0f63b95c1ea8b3eae36717e4fa18dc65f1b">0: Distilling Task-Specific Knowledge from BERT via Adversarial Belief Matching</a></td>
</tr>
<tr>
<td>0</td>
<td>0.742356</td>
<td>0.998093</td>
<td><a href="https://www.semanticscholar.org/paper/2573af4e13d9a5dddb257d22cd38a600528d9a8b">284: MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices</a></td>
</tr>
<tr>
<td>0</td>
<td>0.655098</td>
<td>0.997901</td>
<td><a href="https://www.semanticscholar.org/paper/2e27f119e6fcc5477248eb0f4a6abe8d7cf4f6e7">94: BERT-of-Theseus: Compressing BERT by Progressive Module Replacing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.773255</td>
<td>0.997632</td>
<td><a href="https://www.semanticscholar.org/paper/ce106590145e89ea4b621c99665862967ccf5dac">167: Q8BERT: Quantized 8Bit BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792349</td>
<td>0.997376</td>
<td><a href="https://www.semanticscholar.org/paper/76ef68c7c2410b503e5f1d43ca0c3d6764f72de1">3: ROSITA: Refined BERT cOmpreSsion with InTegrAted techniques</a></td>
</tr>
<tr>
<td>0</td>
<td>0.784327</td>
<td>0.996965</td>
<td><a href="https://www.semanticscholar.org/paper/1c332cfa211400fc6f56983fb01a6692046116dd">89: DynaBERT: Dynamic BERT with Adaptive Width and Depth</a></td>
</tr>
<tr>
<td>0</td>
<td>0.730688</td>
<td>0.996944</td>
<td><a href="https://www.semanticscholar.org/paper/738215a396f6eee1709c6b521a6199769f0ce674">58: Compressing Large-Scale Transformer-Based Models: A Case Study on BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.698388</td>
<td>0.996074</td>
<td><a href="https://www.semanticscholar.org/paper/91ac65431b2dc46919e1673fde67671c29446812">73: When BERT Plays the Lottery, All Tickets Are Winning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.797406</td>
<td>0.995828</td>
<td><a href="https://www.semanticscholar.org/paper/0cbf97173391b0430140117027edcaf1a37968c7">592: TinyBERT: Distilling BERT for Natural Language Understanding</a></td>
</tr>
</table></html>
