<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/a08293b2c9c5bcddb023cc7eb3354d4d86bfae89">216: Distilling Task-Specific Knowledge from BERT into Simple Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.844284</td>
<td>0.781324</td>
<td><a href="https://www.semanticscholar.org/paper/6a2d6237927a076d3da7100a2fcc65710ae443a8">0: Syntactic Knowledge-Infused Transformer and BERT Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.835123</td>
<td>0.953182</td>
<td><a href="https://www.semanticscholar.org/paper/8da4709149a796df0371984d185e1b0f8c26e558">8: Transformers: "The End of History" for NLP?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.830024</td>
<td>0.730084</td>
<td><a href="https://www.semanticscholar.org/paper/9a57d9234eeb5570255910b29c187b9ce43d64e1">30: Contextualized Non-local Neural Networks for Sequence Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.828096</td>
<td>0.658348</td>
<td><a href="https://www.semanticscholar.org/paper/0ebf44fc65ff6f85de330791607325093d362770">34: Mogrifier LSTM</a></td>
</tr>
<tr>
<td>0</td>
<td>0.819955</td>
<td>0.344731</td>
<td><a href="https://www.semanticscholar.org/paper/b7cfccf123f86785476a06c8039889a2eb1e2d73">20: genCNN: A Convolutional Architecture for Word Sequence Prediction</a></td>
</tr>
<tr>
<td>0</td>
<td>0.816602</td>
<td>0.518024</td>
<td><a href="https://www.semanticscholar.org/paper/2e2d56d03636f104d63914fea267231b08250984">2: Scalable Deep Document / Sequence Reasoning with Cognitive Toolkit</a></td>
</tr>
<tr>
<td>0</td>
<td>0.815565</td>
<td>0.766377</td>
<td><a href="https://www.semanticscholar.org/paper/512127e43ba9b7db4933a2e2f121a4b222f6ecbe">1: Attention-based Convolutional Neural Network for Answer Selection using BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.814604</td>
<td>0.457128</td>
<td><a href="https://www.semanticscholar.org/paper/dde07144349a41c4bcffa98ffa671c95485a8383">0: Knowledge Transfer in Neural Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.814420</td>
<td>0.453742</td>
<td><a href="https://www.semanticscholar.org/paper/a583af2696030bcf5f556edc74573fbee902be0b">99: Weakly Supervised Memory Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.685413</td>
<td>0.995744</td>
<td><a href="https://www.semanticscholar.org/paper/ae6fe3b34a5450b6d83cc372cd217597f6096a38">0: Weight Squeezing: Reparameterization for Knowledge Transfer and Model Compression</a></td>
</tr>
<tr>
<td>0</td>
<td>0.679090</td>
<td>0.995035</td>
<td><a href="https://www.semanticscholar.org/paper/fa2b93624725c9e8f090471b884debc3ba8bc879">0: Weight Squeezing: Reparameterization for Model Compression</a></td>
</tr>
<tr>
<td>0</td>
<td>0.779967</td>
<td>0.995029</td>
<td><a href="https://www.semanticscholar.org/paper/93ad19fbc85360043988fa9ea7932b7fdf1fa948">82: Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.702563</td>
<td>0.994834</td>
<td><a href="https://www.semanticscholar.org/paper/f8b97e063d66fbf8a49b33a7b43efdd4fe677ae3">0: Weight Squeezing: Reparameterization for Compression and Fast Inference</a></td>
</tr>
<tr>
<td>0</td>
<td>0.843890</td>
<td>0.994669</td>
<td><a href="https://www.semanticscholar.org/paper/9c5a239b75bade55c830b164e2fadc424e879137">27: XtremeDistil: Multi-stage Distillation for Massive Multilingual Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.828031</td>
<td>0.993570</td>
<td><a href="https://www.semanticscholar.org/paper/7ebed46b7f3ec913e508e6468304fcaea832eda1">102: Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.755841</td>
<td>0.992699</td>
<td><a href="https://www.semanticscholar.org/paper/361a4637d7ce1f51ef45b1b72e9701d5309042b5">0: AdapLeR: Speeding up Inference by Adaptive Length Reduction</a></td>
</tr>
<tr>
<td>0</td>
<td>0.725026</td>
<td>0.992412</td>
<td><a href="https://www.semanticscholar.org/paper/5d34881ff68bd203ff790187e7e5c9e034389cfa">112: FastBERT: a Self-distilling BERT with Adaptive Inference Time</a></td>
</tr>
<tr>
<td>0</td>
<td>0.785775</td>
<td>0.992349</td>
<td><a href="https://www.semanticscholar.org/paper/7402b604f14b8b91c53ed6eed04af92c59636c97">236: Well-Read Students Learn Better: On the Importance of Pre-training Compact Models</a></td>
</tr>
</table></html>
