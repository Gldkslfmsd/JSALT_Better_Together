<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/46b3ba0f3cb8340bc94f26e0fdf6dc4e38f68948">67: Hierarchical Transformers for Long Document Classification</a></td>
</tr>
<tr>
<td>1</td>
<td>0.851852</td>
<td>0.988585</td>
<td><a href="https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992">32318: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.835493</td>
<td>0.950954</td>
<td><a href="https://www.semanticscholar.org/paper/354ec86839539390a148ed41054eb68e0b8caa85">8: BERTSel: Answer Selection with Pre-trained Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.829567</td>
<td>0.951887</td>
<td><a href="https://www.semanticscholar.org/paper/63748e59f4e106cbda6b65939b77589f40e48fcb">624: Text Summarization with Pretrained Encoders</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826547</td>
<td>0.599645</td>
<td><a href="https://www.semanticscholar.org/paper/7f98fff4c5bf4b19321f5476fd76106ce32edcc4">5: Enhanced Bert-Based Ranking Models for Spoken Document Retrieval</a></td>
</tr>
<tr>
<td>0</td>
<td>0.817753</td>
<td>0.905697</td>
<td><a href="https://www.semanticscholar.org/paper/168fc3525f7b97695a97b04e257ee9bd1e832acb">5: Memory Transformer</a></td>
</tr>
<tr>
<td>0</td>
<td>0.813820</td>
<td>0.936703</td>
<td><a href="https://www.semanticscholar.org/paper/79bee63bc21a99dc12dd2725a0c0dcfffa4611ef">0: Pyramid-BERT: Reducing Complexity via Successive Core-set based Token Selection</a></td>
</tr>
<tr>
<td>0</td>
<td>0.813519</td>
<td>0.969848</td>
<td><a href="https://www.semanticscholar.org/paper/6c761cfdb031701072582e434d8f64d436255da6">18: AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.813229</td>
<td>0.835991</td>
<td><a href="https://www.semanticscholar.org/paper/032274e57f7d8b456bd255fe76b909b2c1d7458e">1037: A Deep Reinforced Model for Abstractive Summarization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811654</td>
<td>0.646141</td>
<td><a href="https://www.semanticscholar.org/paper/329484f2e34e7a58ba8aeac000ed57c6702942ee">0: Transformer++</a></td>
</tr>
<tr>
<td>0</td>
<td>0.802861</td>
<td>0.988379</td>
<td><a href="https://www.semanticscholar.org/paper/a022bda79947d1f656a1164003c1b3ae9a843df9">601: How to Fine-Tune BERT for Text Classification?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.733819</td>
<td>0.988196</td>
<td><a href="https://www.semanticscholar.org/paper/3bcb17559ce96eb20fa79af8194f4af0380d194a">366: Pre-trained Models for Natural Language Processing: A Survey</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763973</td>
<td>0.988170</td>
<td><a href="https://www.semanticscholar.org/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035">3381: Improving Language Understanding by Generative Pre-Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.780256</td>
<td>0.988078</td>
<td><a href="https://www.semanticscholar.org/paper/e0c6abdbdecf04ffac65c440da77fb9d66bb474c">4103: XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.782465</td>
<td>0.987709</td>
<td><a href="https://www.semanticscholar.org/paper/1e077413b25c4d34945cc2707e17e46ed4fe784a">2192: Universal Language Model Fine-tuning for Text Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.756010</td>
<td>0.986746</td>
<td><a href="https://www.semanticscholar.org/paper/2ff41a463a374b138bb5a012e5a32bc4beefec20">304: Pre-Training With Whole Word Masking for Chinese BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.580959</td>
<td>0.986531</td>
<td><a href="https://www.semanticscholar.org/paper/4b73e1b62791b25225bacd92d0163f71409e6022">10: Example-Based Named Entity Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.716273</td>
<td>0.986241</td>
<td><a href="https://www.semanticscholar.org/paper/335613303ebc5eac98de757ed02a56377d99e03a">544: What Does BERT Learn about the Structure of Language?</a></td>
</tr>
</table></html>
