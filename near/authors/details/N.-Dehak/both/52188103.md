<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/75b2843539dc8567b1502a19b3788adf6a015eb6">62: Deep Neural Networks for Emotion Recognition Combining Audio and Transcripts</a></td>
</tr>
<tr>
<td>0</td>
<td>0.899552</td>
<td>0.919999</td>
<td><a href="https://www.semanticscholar.org/paper/7b62461b87c6ba17fa3a185f617ea920316e8550">6: Efficient Speech Emotion Recognition Using Multi-Scale CNN and Attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.893504</td>
<td>0.687669</td>
<td><a href="https://www.semanticscholar.org/paper/fd04db534ff7b87e5c8ce091c771c6e39af06179">1: End-to-End Emotion Recognition From Speech With Deep Frame Embeddings And Neutral Speech Handling</a></td>
</tr>
<tr>
<td>1</td>
<td>0.877521</td>
<td>0.978291</td>
<td><a href="https://www.semanticscholar.org/paper/2722e0b982e75b26ecdbef17daac76ed8404138e">14: Attention Driven Fusion for Multi-Modal Emotion Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.867372</td>
<td>0.330480</td>
<td><a href="https://www.semanticscholar.org/paper/a0128f085bcdbb903ae5d4ade460279f0a952711">0: Audio Emotion Recognition with Noise Reduction</a></td>
</tr>
<tr>
<td>0</td>
<td>0.866169</td>
<td>0.733673</td>
<td><a href="https://www.semanticscholar.org/paper/264abd8905ef1f2c27fd6cf1a13c8d83bac04eb8">5: A Multimodal Approach towards Emotion Recognition of Music using Audio and Lyrical Content</a></td>
</tr>
<tr>
<td>0</td>
<td>0.866007</td>
<td>0.814465</td>
<td><a href="https://www.semanticscholar.org/paper/02746edefb5f4dce956880c6b76fdb78f9c0e8e2">104: End-to-End Speech Emotion Recognition Using Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.865903</td>
<td>0.757181</td>
<td><a href="https://www.semanticscholar.org/paper/f1729e1c02c83e7a17c0f09ec3b7125499a1d2a8">11: A Segment Level Approach to Speech Emotion Recognition Using Transfer Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.864461</td>
<td>0.730852</td>
<td><a href="https://www.semanticscholar.org/paper/ae6b920a24638af0c038a819a75d266cd36ea71c">2: Autoencoder With Emotion Embedding for Speech Emotion Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.864267</td>
<td>0.953939</td>
<td><a href="https://www.semanticscholar.org/paper/e49d70d05590093c019aeb83a153e06379607958">6: Deep Encoded Linguistic and Acoustic Cues for Attention Based End to End Speech Emotion Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.852856</td>
<td>0.986482</td>
<td><a href="https://www.semanticscholar.org/paper/101e22de9ef604aa586c806c81926648ac583b99">43: Learning Alignment for Multimodal Emotion Recognition from Speech</a></td>
</tr>
<tr>
<td>0</td>
<td>0.860829</td>
<td>0.985771</td>
<td><a href="https://www.semanticscholar.org/paper/d70ba7cb3872b3df5abadbb230bfaf57b02e7cca">109: Multimodal Speech Emotion Recognition Using Audio and Text</a></td>
</tr>
<tr>
<td>0</td>
<td>0.873642</td>
<td>0.984243</td>
<td><a href="https://www.semanticscholar.org/paper/77f418061bea95cb3ecad1e5287468e1d811538c">53: Speech Emotion Recognition Using Multi-hop Attention Mechanism</a></td>
</tr>
<tr>
<td>0</td>
<td>0.867862</td>
<td>0.980955</td>
<td><a href="https://www.semanticscholar.org/paper/fcb202cabab5f0e423fb5d9748fbf34f67a2e31a">11: Fusion Approaches for Emotion Recognition from Speech Using Acoustic and Text-Based Features</a></td>
</tr>
<tr>
<td>0</td>
<td>0.787907</td>
<td>0.971124</td>
<td><a href="https://www.semanticscholar.org/paper/7d60294d3e19dafa1fb0143136c9efae4b8f59de">3: Group Gated Fusion on Attention-Based Bidirectional Alignment for Multimodal Emotion Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.839531</td>
<td>0.966269</td>
<td><a href="https://www.semanticscholar.org/paper/3d1353331d71fec03fd11d8fdaee2435ca23f247">7: WISE: Word-Level Interaction-Based Multimodal Fusion for Speech Emotion Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.673035</td>
<td>0.966133</td>
<td><a href="https://www.semanticscholar.org/paper/ea3266caff07d07099b649bf75cc222752140a3e">5: A Large Scale Speech Sentiment Corpus</a></td>
</tr>
<tr>
<td>0</td>
<td>0.771110</td>
<td>0.965330</td>
<td><a href="https://www.semanticscholar.org/paper/7a39763121077c5a67343f822e6617fe3013a124">65: Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment</a></td>
</tr>
</table></html>
