<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/6c08613344e993026dafd3e57f64c4d1cfaeeca7">62: Cepstral and long-term features for emotion recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.823225</td>
<td>0.869448</td>
<td><a href="https://www.semanticscholar.org/paper/575c1de9866215e5787a77dfa8c622c562368fa7">4: Speech emotion recognition with ensemble learning methods</a></td>
</tr>
<tr>
<td>1</td>
<td>0.819962</td>
<td>0.975244</td>
<td><a href="https://www.semanticscholar.org/paper/6abd2f7259df76011bc56bb8a20cc756d5838590">64: Brno University of Technology system for Interspeech 2009 emotion challenge</a></td>
</tr>
<tr>
<td>0</td>
<td>0.816506</td>
<td>0.937500</td>
<td><a href="https://www.semanticscholar.org/paper/5734d592028f2f4e82a7ac094b49df1aa92604b2">62: Segment-based approach to the recognition of emotions in speech</a></td>
</tr>
<tr>
<td>0</td>
<td>0.809698</td>
<td>0.922862</td>
<td><a href="https://www.semanticscholar.org/paper/a2f350c6b1a0268779f29db835b208f175e291b6">13: Speech emotion classification using acoustic features</a></td>
</tr>
<tr>
<td>0</td>
<td>0.797329</td>
<td>0.928672</td>
<td><a href="https://www.semanticscholar.org/paper/1d1299464d56f8af359c06b2659fd7a9442c6b8a">4: GMM based speaker variability compensated system for interspeech 2013 compare emotion challenge</a></td>
</tr>
<tr>
<td>0</td>
<td>0.790352</td>
<td>0.910087</td>
<td><a href="https://www.semanticscholar.org/paper/b4641b7b4e3af0703b31c90bc8f9bef0ad84de77">68: Time-Frequency Feature and AMS-GMM Mask for Acoustic Emotion Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.787406</td>
<td>0.937539</td>
<td><a href="https://www.semanticscholar.org/paper/30b7dacbfe40b0a95b99c8b4fb11fcf9d4393982">31: Determining optimal signal features and parameters for HMM-based emotion classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.786545</td>
<td>0.886579</td>
<td><a href="https://www.semanticscholar.org/paper/ef413a993ff516a437f17ceba340ef03937edefc">6: Long-term spectro-temporal information for improved automatic speech emotion classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.785725</td>
<td>0.905039</td>
<td><a href="https://www.semanticscholar.org/paper/0b1a73ebf998ef90105ada2e39efa5b1b413d885">7: Multiclass SVM-based language-independent emotion recognition using selective speech features</a></td>
</tr>
<tr>
<td>0</td>
<td>0.777586</td>
<td>0.978008</td>
<td><a href="https://www.semanticscholar.org/paper/7758898c96b600c8e0826bde8d3457881298717d">61: Anchor Models for Emotion Recognition from Speech</a></td>
</tr>
<tr>
<td>0</td>
<td>0.776739</td>
<td>0.971112</td>
<td><a href="https://www.semanticscholar.org/paper/1fd4b1805ea851703ea36fbc7104b53796bcf05e">94: Emotion Recognition using Mel-Frequency Cepstral Coefficients</a></td>
</tr>
<tr>
<td>0</td>
<td>0.559212</td>
<td>0.970826</td>
<td><a href="https://www.semanticscholar.org/paper/106592864f2ffc22ffb65ca314da786090642592">6: Intelligent Remote Control for TV Program based on Emotion in Arabic Speech</a></td>
</tr>
<tr>
<td>0</td>
<td>0.713390</td>
<td>0.970200</td>
<td><a href="https://www.semanticscholar.org/paper/db779bc16a620a6fd747217272e1161b822c21ec">3: Speech Emotion Recognition Using Affective Saliency</a></td>
</tr>
<tr>
<td>0</td>
<td>0.728946</td>
<td>0.969859</td>
<td><a href="https://www.semanticscholar.org/paper/672b2a96c9a0d11e29c1a8e2f3edbb86cc77237f">19: A Study on the Search of the Most Discriminative Speech Features in the Speaker Dependent Speech Emotion Recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.754877</td>
<td>0.969756</td>
<td><a href="https://www.semanticscholar.org/paper/46840bf4b81e4c3647bb2540bd706a2898308a21">0: Gender with Emotion Recognition Using Machine Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.693996</td>
<td>0.969572</td>
<td><a href="https://www.semanticscholar.org/paper/9ad646e71fed68d7777f71f5c664b01eb8a6297b">278: Getting started with SUSAS: a speech under simulated and actual stress database</a></td>
</tr>
<tr>
<td>0</td>
<td>0.794689</td>
<td>0.969535</td>
<td><a href="https://www.semanticscholar.org/paper/943bb8f0b62ceeec0802ffd1c6abb7efb3e0d6f9">261: Emotion recognition in spontaneous speech using GMMs</a></td>
</tr>
</table></html>
