<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de">1239: Generating Text with Recurrent Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.843753</td>
<td>0.857192</td>
<td><a href="https://www.semanticscholar.org/paper/0d6203718c15f137fda2f295c96269bc2b254644">581: Learning Recurrent Neural Networks with Hessian-Free Optimization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.840636</td>
<td>0.908413</td>
<td><a href="https://www.semanticscholar.org/paper/110116481848d8d2e371652a11874635024426f0">7: Recognizing Long Grammatical Sequences Using Recurrent Networks Augmented With An External Differentiable Stack</a></td>
</tr>
<tr>
<td>0</td>
<td>0.833032</td>
<td>0.813745</td>
<td><a href="https://www.semanticscholar.org/paper/124a518e7de94fda6f70e4474bdd7b058e88fe3b">2: Efficient Sequence Labeling with Actor-Critic Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.817856</td>
<td>0.934504</td>
<td><a href="https://www.semanticscholar.org/paper/6746a18b2820f757334f75bc95428b3ea58d6603">45: Variable Computation in Recurrent Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.803121</td>
<td>0.882677</td>
<td><a href="https://www.semanticscholar.org/paper/08f495b841ff3c39bc9fe5d36efa6e4beef6ecd8">5: Recurrent neural network language model with structured word embeddings for speech recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800095</td>
<td>0.833221</td>
<td><a href="https://www.semanticscholar.org/paper/044bfe1c5111a3ac673b5540d31dc1adf17082ee">3: Text classification based recurrent neural network</a></td>
</tr>
<tr>
<td>0</td>
<td>0.794084</td>
<td>0.848472</td>
<td><a href="https://www.semanticscholar.org/paper/b352751c85ae000ccaeb95fadd51e4bb80ae8018">2: Enhancing the Recurrent Neural Networks with Positional Gates for Sentence Representation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.790873</td>
<td>0.532901</td>
<td><a href="https://www.semanticscholar.org/paper/8e6debf21a980d74570bd93b92c6edecfa6120d4">1: Pixel-Based LSTM Generative Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.781112</td>
<td>0.014840</td>
<td>NA:206757669</td>
</tr>
<tr>
<td>0</td>
<td>0.808289</td>
<td>0.984755</td>
<td><a href="https://www.semanticscholar.org/paper/9665247ea3421929f9b6ad721f139f11edb1dbb8">225: Learning Longer Memory in Recurrent Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.822231</td>
<td>0.984367</td>
<td><a href="https://www.semanticscholar.org/paper/f9a1b3850dfd837793743565a8af95973d395a4e">1464: LSTM Neural Networks for Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.702194</td>
<td>0.977964</td>
<td><a href="https://www.semanticscholar.org/paper/b777a55505ee2ffb4f8f9ada916e4e4a5f13a4ed">64: Depth-Gated Recurrent Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.816188</td>
<td>0.977204</td>
<td><a href="https://www.semanticscholar.org/paper/8cb72cf5490c2a532d52237f688f915a92afe04c">349: From Feedforward to Recurrent LSTM Neural Networks for Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.776235</td>
<td>0.975325</td>
<td><a href="https://www.semanticscholar.org/paper/9819b600a828a57e1cde047bbe710d3446b30da5">4844: Recurrent neural network based language model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.779466</td>
<td>0.975048</td>
<td><a href="https://www.semanticscholar.org/paper/d14c7e5f5cace4c925abc74c88baa474e9f31a28">640: Gated Feedback Recurrent Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.649144</td>
<td>0.974823</td>
<td><a href="https://www.semanticscholar.org/paper/b158a006bebb619e2ea7bf0a22c27d45c5d19004">599: LSTM can Solve Hard Long Time Lag Problems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.679248</td>
<td>0.974555</td>
<td><a href="https://www.semanticscholar.org/paper/a739ae988ba0e3ff232f4507627dfc282ba7b3f4">56: Depth-Gated LSTM</a></td>
</tr>
<tr>
<td>0</td>
<td>0.700655</td>
<td>0.973940</td>
<td><a href="https://www.semanticscholar.org/paper/67fbd11306593635f04def0438d97ac2a21ab10b">0: Myanmar Language Part-of-Speech Tagging Using Deep Learning Models</a></td>
</tr>
</table></html>
