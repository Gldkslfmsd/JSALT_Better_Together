<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e">14531: Reducing the Dimensionality of Data with Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.788324</td>
<td>0.951592</td>
<td><a href="https://www.semanticscholar.org/paper/2f7568e650d4313f6165a07985e385490a0790c1">0: Dimensionality Reduction Applied to Time Response of Linear Systems Using Autoencoders</a></td>
</tr>
<tr>
<td>0</td>
<td>0.768894</td>
<td>0.761723</td>
<td><a href="https://www.semanticscholar.org/paper/fbe4c45e46a9ebd34c3bc9c1024130efde962058">1: Efficient Encoding Using Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.739217</td>
<td>0.519129</td>
<td><a href="https://www.semanticscholar.org/paper/397d70c77e3c8e16cd6d556472207fffdaa28564">93: Learning One-hidden-layer ReLU Networks via Gradient Descent</a></td>
</tr>
<tr>
<td>0</td>
<td>0.738671</td>
<td>0.263936</td>
<td><a href="https://www.semanticscholar.org/paper/8b3ac1b393f584d5df56055451582f4ed66db37a">0: Modeling Sparse Data as Input for Weightless Neural Network</a></td>
</tr>
<tr>
<td>0</td>
<td>0.737480</td>
<td>0.254678</td>
<td><a href="https://www.semanticscholar.org/paper/4a1fade6857ade3e4a014f7be745565fca3a8eec">16: Stochastic Linear Bandits with Hidden Low Rank Structure</a></td>
</tr>
<tr>
<td>0</td>
<td>0.734411</td>
<td>0.437790</td>
<td><a href="https://www.semanticscholar.org/paper/45d7d476363b0d69e9dd93c16bca296cec902420">1: Deep processing of structured data</a></td>
</tr>
<tr>
<td>0</td>
<td>0.734230</td>
<td>0.237784</td>
<td><a href="https://www.semanticscholar.org/paper/613cf0e5be68faf88f907fdf91ed57fa5045032d">0: WeightScale: Interpreting Weight Change in Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.731397</td>
<td>0.803654</td>
<td><a href="https://www.semanticscholar.org/paper/ad65d6b1c1342694cddd1ff3e07ce5183d908293">16: Deep multi-task learning with evolving weights</a></td>
</tr>
<tr>
<td>0</td>
<td>0.726410</td>
<td>0.681565</td>
<td><a href="https://www.semanticscholar.org/paper/d9fbdea61ad6cf991885d2a8448fab7c15aff8dd">7: Stacked Robust Autoencoder for Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.685436</td>
<td>0.986912</td>
<td><a href="https://www.semanticscholar.org/paper/355d44f53428b1ac4fb2ab468d593c720640e5bd">3375: Greedy Layer-Wise Training of Deep Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.649219</td>
<td>0.983220</td>
<td><a href="https://www.semanticscholar.org/paper/8978cf7574ceb35f4c3096be768c7547b28a35d0">13346: A Fast Learning Algorithm for Deep Belief Nets</a></td>
</tr>
<tr>
<td>0</td>
<td>0.673486</td>
<td>0.980921</td>
<td><a href="https://www.semanticscholar.org/paper/e2b7f37cd97a7907b1b8a41138721ed06a0b76cd">5556: Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</a></td>
</tr>
<tr>
<td>0</td>
<td>0.630713</td>
<td>0.979759</td>
<td><a href="https://www.semanticscholar.org/paper/e60ff004dde5c13ec53087872cfcdd12e85beb57">7523: Learning Deep Architectures for AI</a></td>
</tr>
<tr>
<td>0</td>
<td>0.613700</td>
<td>0.976747</td>
<td><a href="https://www.semanticscholar.org/paper/e95d3934e51107da7610acd0b1bcb6551671f9f1">2725: A Practical Guide to Training Restricted Boltzmann Machines</a></td>
</tr>
<tr>
<td>0</td>
<td>-1.000000</td>
<td>0.975123</td>
<td><a href="https://www.semanticscholar.org/paper/f6f80f4d7af095a5dd8742f1a352c4d2dbc9b50d">173: Deep Belief Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.683000</td>
<td>0.975109</td>
<td><a href="https://www.semanticscholar.org/paper/43c8a545f7166659e9e21c88fe234e0323855216">1242: Greedy Layer-Wise Training of Deep Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.532476</td>
<td>0.974046</td>
<td><a href="https://www.semanticscholar.org/paper/c50dca78e97e335d362d6b991ae0e1448914e9a3">285: Reducing the Dimensionality of Data with Neural</a></td>
</tr>
<tr>
<td>0</td>
<td>0.673335</td>
<td>0.969756</td>
<td><a href="https://www.semanticscholar.org/paper/f8c8619ea7d68e604e40b814b40c72888a755e95">482: Unsupervised Feature Learning and Deep Learning: A Review and New Perspectives</a></td>
</tr>
</table></html>
<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e">14531: Reducing the Dimensionality of Data with Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.788324</td>
<td>0.951592</td>
<td><a href="https://www.semanticscholar.org/paper/2f7568e650d4313f6165a07985e385490a0790c1">0: Dimensionality Reduction Applied to Time Response of Linear Systems Using Autoencoders</a></td>
</tr>
<tr>
<td>0</td>
<td>0.768894</td>
<td>0.761723</td>
<td><a href="https://www.semanticscholar.org/paper/fbe4c45e46a9ebd34c3bc9c1024130efde962058">1: Efficient Encoding Using Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.739217</td>
<td>0.519129</td>
<td><a href="https://www.semanticscholar.org/paper/397d70c77e3c8e16cd6d556472207fffdaa28564">93: Learning One-hidden-layer ReLU Networks via Gradient Descent</a></td>
</tr>
<tr>
<td>0</td>
<td>0.738671</td>
<td>0.263936</td>
<td><a href="https://www.semanticscholar.org/paper/8b3ac1b393f584d5df56055451582f4ed66db37a">0: Modeling Sparse Data as Input for Weightless Neural Network</a></td>
</tr>
<tr>
<td>0</td>
<td>0.737480</td>
<td>0.254678</td>
<td><a href="https://www.semanticscholar.org/paper/4a1fade6857ade3e4a014f7be745565fca3a8eec">16: Stochastic Linear Bandits with Hidden Low Rank Structure</a></td>
</tr>
<tr>
<td>0</td>
<td>0.734411</td>
<td>0.437790</td>
<td><a href="https://www.semanticscholar.org/paper/45d7d476363b0d69e9dd93c16bca296cec902420">1: Deep processing of structured data</a></td>
</tr>
<tr>
<td>0</td>
<td>0.734230</td>
<td>0.237784</td>
<td><a href="https://www.semanticscholar.org/paper/613cf0e5be68faf88f907fdf91ed57fa5045032d">0: WeightScale: Interpreting Weight Change in Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.731397</td>
<td>0.803654</td>
<td><a href="https://www.semanticscholar.org/paper/ad65d6b1c1342694cddd1ff3e07ce5183d908293">16: Deep multi-task learning with evolving weights</a></td>
</tr>
<tr>
<td>0</td>
<td>0.726410</td>
<td>0.681565</td>
<td><a href="https://www.semanticscholar.org/paper/d9fbdea61ad6cf991885d2a8448fab7c15aff8dd">7: Stacked Robust Autoencoder for Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.685436</td>
<td>0.986912</td>
<td><a href="https://www.semanticscholar.org/paper/355d44f53428b1ac4fb2ab468d593c720640e5bd">3375: Greedy Layer-Wise Training of Deep Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.649219</td>
<td>0.983220</td>
<td><a href="https://www.semanticscholar.org/paper/8978cf7574ceb35f4c3096be768c7547b28a35d0">13346: A Fast Learning Algorithm for Deep Belief Nets</a></td>
</tr>
<tr>
<td>0</td>
<td>0.673486</td>
<td>0.980921</td>
<td><a href="https://www.semanticscholar.org/paper/e2b7f37cd97a7907b1b8a41138721ed06a0b76cd">5556: Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</a></td>
</tr>
<tr>
<td>0</td>
<td>0.630713</td>
<td>0.979759</td>
<td><a href="https://www.semanticscholar.org/paper/e60ff004dde5c13ec53087872cfcdd12e85beb57">7523: Learning Deep Architectures for AI</a></td>
</tr>
<tr>
<td>0</td>
<td>0.613700</td>
<td>0.976747</td>
<td><a href="https://www.semanticscholar.org/paper/e95d3934e51107da7610acd0b1bcb6551671f9f1">2725: A Practical Guide to Training Restricted Boltzmann Machines</a></td>
</tr>
<tr>
<td>0</td>
<td>-1.000000</td>
<td>0.975123</td>
<td><a href="https://www.semanticscholar.org/paper/f6f80f4d7af095a5dd8742f1a352c4d2dbc9b50d">173: Deep Belief Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.683000</td>
<td>0.975109</td>
<td><a href="https://www.semanticscholar.org/paper/43c8a545f7166659e9e21c88fe234e0323855216">1242: Greedy Layer-Wise Training of Deep Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.532476</td>
<td>0.974046</td>
<td><a href="https://www.semanticscholar.org/paper/c50dca78e97e335d362d6b991ae0e1448914e9a3">285: Reducing the Dimensionality of Data with Neural</a></td>
</tr>
<tr>
<td>0</td>
<td>0.673335</td>
<td>0.969756</td>
<td><a href="https://www.semanticscholar.org/paper/f8c8619ea7d68e604e40b814b40c72888a755e95">482: Unsupervised Feature Learning and Deep Learning: A Review and New Perspectives</a></td>
</tr>
</table></html>
