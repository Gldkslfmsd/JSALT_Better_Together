<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/4a6fb70a76968fbb136f370551f720e8d745698c">151: Sparse Overcomplete Word Vector Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.856057</td>
<td>0.968643</td>
<td><a href="https://www.semanticscholar.org/paper/e09beb33880425e5439fe076da4be7a6916df460">75: Non-distributional Word Vector Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.805064</td>
<td>0.928828</td>
<td><a href="https://www.semanticscholar.org/paper/f37e1b62a767a307c046404ca96bc140b3e68cb5">22193: GloVe: Global Vectors for Word Representation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.794164</td>
<td>0.966307</td>
<td><a href="https://www.semanticscholar.org/paper/330da625c15427c6e42ccfa3b747fb29e5835bf0">21530: Efficient Estimation of Word Representations in Vector Space</a></td>
</tr>
<tr>
<td>0</td>
<td>0.782610</td>
<td>0.724875</td>
<td><a href="https://www.semanticscholar.org/paper/844121143daacba24fa54bc8c69a834ee6c3d670">1: Non-linear tagging models with localist and distributed word representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.770998</td>
<td>0.913816</td>
<td><a href="https://www.semanticscholar.org/paper/b4f943584acf7694cc369a42a75ba804f8493ed3">104: NASARI: a Novel Approach to a Semantically-Aware Representation of Items</a></td>
</tr>
<tr>
<td>0</td>
<td>0.764554</td>
<td>0.870395</td>
<td><a href="https://www.semanticscholar.org/paper/b27546759bcd7e689d2e6de7b4085de794c74743">0: A Model for High-coverage Lexical Semantic Annotation Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763540</td>
<td>0.920509</td>
<td><a href="https://www.semanticscholar.org/paper/ca0332d5c05cbdedaf8c29e3bdad14f371c77118">2: Using Embedding Masks for Word Categorization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763362</td>
<td>0.852163</td>
<td><a href="https://www.semanticscholar.org/paper/60a5a7396416238bddc2e3e3efd8ac1489e88131">2: Beyond Word2Vec: Embedding Words and Phrases in Same Vector Space</a></td>
</tr>
<tr>
<td>0</td>
<td>0.762486</td>
<td>0.929130</td>
<td><a href="https://www.semanticscholar.org/paper/ebb1cec82701f12af244b3b38bf39f677cd348dc">8: Leveraging distributed representations and lexico-syntactic fixedness for token-level prediction of the idiomaticity of English verb-noun combinations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.727184</td>
<td>0.993993</td>
<td><a href="https://www.semanticscholar.org/paper/fe360f17983b2a8b2fc6a045e653c92d562be9c7">64: Sparse Word Embeddings Using â„“1 Regularized Online Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.782624</td>
<td>0.989159</td>
<td><a href="https://www.semanticscholar.org/paper/45ea9f9ae5368774d921e56e28fade358d171b2f">155: Linear Algebraic Structure of Word Senses, with Applications to Polysemy</a></td>
</tr>
<tr>
<td>0</td>
<td>0.774815</td>
<td>0.988338</td>
<td><a href="https://www.semanticscholar.org/paper/41d366fa8c7e454e7b478c9f1b2d6d40b54dbc1c">59: A Compositional and Interpretable Semantic Space</a></td>
</tr>
<tr>
<td>0</td>
<td>0.722276</td>
<td>0.987920</td>
<td><a href="https://www.semanticscholar.org/paper/6d479f9110f4ee1a18a0e5bf347ba4c1e299893d">68: SPINE: SParse Interpretable Neural Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.677041</td>
<td>0.985477</td>
<td><a href="https://www.semanticscholar.org/paper/ba9769758c577ad3474785e070156365c2ede09e">93: Factors Influencing the Surprising Instability of Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.649618</td>
<td>0.985019</td>
<td><a href="https://www.semanticscholar.org/paper/b7b83e3605828fa5bb4b6b321473699cefb61333">10: Learning Geometric Word Meta-Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.680353</td>
<td>0.984514</td>
<td><a href="https://www.semanticscholar.org/paper/08059481168f24e5788c617bfae50821f55a2182">35: Learning Word Meta-Embeddings by Autoencoding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.738148</td>
<td>0.984334</td>
<td><a href="https://www.semanticscholar.org/paper/7dbc24dfc2dd4f9640958790f08b377be121e5a9">217: A Latent Variable Model Approach to PMI-based Word Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.713888</td>
<td>0.983643</td>
<td><a href="https://www.semanticscholar.org/paper/74d0d0ba893a3dad04737c47d680b2497c8b9757">10: P-SIF: Document Embeddings Using Partition Averaging</a></td>
</tr>
</table></html>
