<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/f6fbb6809374ca57205bd2cf1421d4f4fa04f975">429: Linguistic Knowledge and Transferability of Contextual Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.867804</td>
<td>0.991875</td>
<td><a href="https://www.semanticscholar.org/paper/ac11062f1f368d97f4c826c317bf50dcc13fdb59">257: Dissecting Contextual Word Embeddings: Architecture and Representation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.864102</td>
<td>0.956030</td>
<td><a href="https://www.semanticscholar.org/paper/c6dc2f21e943c5a1dd35fb3d6ff18525c9c86ca0">17: Infusing Finetuning with Semantic Dependencies</a></td>
</tr>
<tr>
<td>0</td>
<td>0.854796</td>
<td>0.737558</td>
<td><a href="https://www.semanticscholar.org/paper/3dbb476a4bccfa8d401f4d1b08e38ee972920447">2: Reverse Transfer Learning: Can Word Embeddings Trained for Different NLP Tasks Improve Neural Language Models?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.836218</td>
<td>0.818682</td>
<td><a href="https://www.semanticscholar.org/paper/9c8776ae04d71fd13ee11dc8c16686e63cb3174d">0: Integration of BERT and WordNet for improving Natural Language Understanding Name :</a></td>
</tr>
<tr>
<td>0</td>
<td>0.833098</td>
<td>0.577575</td>
<td><a href="https://www.semanticscholar.org/paper/3ffbbfa569d5125b85b0ccb7625e77e556dfb493">13: Learning Context-Specific Word/Character Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.826701</td>
<td>0.957983</td>
<td><a href="https://www.semanticscholar.org/paper/421fc2556836a6b441de806d7b393a35b6eaea58">849: Contextual String Embeddings for Sequence Labeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.825026</td>
<td>0.936389</td>
<td><a href="https://www.semanticscholar.org/paper/32d281a1e7a0a2d4e2b3f34e0f71780c987e1374">131: DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.823848</td>
<td>0.854805</td>
<td><a href="https://www.semanticscholar.org/paper/4b0e6b4b451ddc7e6b1bbb480e206c18f498ee82">0: Leveraging pre-trained language models for conversational information seeking from text</a></td>
</tr>
<tr>
<td>0</td>
<td>0.822104</td>
<td>0.850165</td>
<td><a href="https://www.semanticscholar.org/paper/223e6bfe8f9f4f83c971da94115b22e2407e6a33">30: Can LSTM Learn to Capture Agreement? The Case of Basque</a></td>
</tr>
<tr>
<td>0</td>
<td>0.816814</td>
<td>0.999073</td>
<td><a href="https://www.semanticscholar.org/paper/5744f56d3253bd7c4341d36de40a93fceaa266b3">172: Semantics-aware BERT for Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.830750</td>
<td>0.998791</td>
<td><a href="https://www.semanticscholar.org/paper/e2587eddd57bc4ba286d91b27c185083f16f40ee">484: What do you learn from context? Probing for sentence structure in contextualized word representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.745719</td>
<td>0.997948</td>
<td><a href="https://www.semanticscholar.org/paper/455a8838cde44f288d456d01c76ede95b56dc675">581: A Structural Probe for Finding Syntax in Word Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.798809</td>
<td>0.997657</td>
<td><a href="https://www.semanticscholar.org/paper/c12e6c65e1de5d3993c5b65d0e234ae1f60c85ae">95: TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection</a></td>
</tr>
<tr>
<td>0</td>
<td>0.751219</td>
<td>0.997357</td>
<td><a href="https://www.semanticscholar.org/paper/473921de1b52f98f34f37afd507e57366ff7d1ca">50: CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters</a></td>
</tr>
<tr>
<td>0</td>
<td>0.753749</td>
<td>0.997281</td>
<td><a href="https://www.semanticscholar.org/paper/199ff73d2f728e997f860b62a2322823d3e3d9e8">241: Designing and Interpreting Probes with Control Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.820882</td>
<td>0.997119</td>
<td><a href="https://www.semanticscholar.org/paper/1c3112ef8a346b9817382ed34a8c146c53d5bcf5">497: XNLI: Evaluating Cross-lingual Sentence Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.802894</td>
<td>0.996932</td>
<td><a href="https://www.semanticscholar.org/paper/4fa37d012ad0014552a6a5a03624b29f95558bf7">339: CamemBERT: a Tasty French Language Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.719668</td>
<td>0.996709</td>
<td><a href="https://www.semanticscholar.org/paper/a622332550eaf535cf0f0f6c3a3f3ba197c39cac">109: PhoBERT: Pre-trained language models for Vietnamese</a></td>
</tr>
</table></html>
