<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/668db48c6a79826456341680ee1175dfc4cced71">2404: Get To The Point: Summarization with Pointer-Generator Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.885075</td>
<td>0.956643</td>
<td><a href="https://www.semanticscholar.org/paper/e88ecf39729913cdb42f18b1f7ddfe6a50fc9c30">0: Blending approaches to Abstractive Summarization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.881256</td>
<td>0.868644</td>
<td><a href="https://www.semanticscholar.org/paper/a198e0b89c188f07c52e43c3511734723345cd6e">4: Extractive-abstractive summarization with pointer and coverage mechanism</a></td>
</tr>
<tr>
<td>0</td>
<td>0.881238</td>
<td>0.887002</td>
<td><a href="https://www.semanticscholar.org/paper/8fcee66f13a0c3d2a26dafba94acf90940834f6a">1: TL ; DR : Improving Abstractive Summarization Using LSTMs</a></td>
</tr>
<tr>
<td>0</td>
<td>0.859471</td>
<td>0.839970</td>
<td><a href="https://www.semanticscholar.org/paper/33ec2a50587b2ea97c716ed7a6ff55d126231851">0: Neural Abstractive Summarization on the Gigaword Dataset</a></td>
</tr>
<tr>
<td>0</td>
<td>0.847982</td>
<td>0.871534</td>
<td><a href="https://www.semanticscholar.org/paper/cabcb46ff353a86b84a9bbd60c7a2e1a46aec408">1: Improving generation quality of pointer networks via guided attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.841950</td>
<td>0.917394</td>
<td><a href="https://www.semanticscholar.org/paper/19c10e1f6f005c716c190e4ba3de6478aec3a97b">4: Diverse Decoding for Abstractive Document Summarization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.835264</td>
<td>0.954322</td>
<td><a href="https://www.semanticscholar.org/paper/cbd55ffa66eed64a55c29c07a38bed385dbcdacf">2: Main Point Generator: Summarizing with a Focus</a></td>
</tr>
<tr>
<td>0</td>
<td>0.831035</td>
<td>-0.046379</td>
<td><a href="https://www.semanticscholar.org/paper/132e4cedbc1dca67113af543bf58bbf9cb50742f">0: Summarization With Self-Aware Context Selecting Mechanism.</a></td>
</tr>
<tr>
<td>0</td>
<td>0.830941</td>
<td>0.952993</td>
<td><a href="https://www.semanticscholar.org/paper/154e3dd761b9a766a421dcc20aa4ad236381dc1c">17: Reading Like HER: Human Reading Inspired Extractive Summarization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.804036</td>
<td>0.997570</td>
<td><a href="https://www.semanticscholar.org/paper/d381709212dccf397284eee54a1e3010a4ef777f">173: A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss</a></td>
</tr>
<tr>
<td>0</td>
<td>0.761230</td>
<td>0.997115</td>
<td><a href="https://www.semanticscholar.org/paper/a8b21f72bdc251689f636d3d7ff52a6b85ab7ce9">215: Neural Question Generation from Text: A Preliminary Study</a></td>
</tr>
<tr>
<td>0</td>
<td>0.750645</td>
<td>0.996976</td>
<td><a href="https://www.semanticscholar.org/paper/850e07b5811049ad02f80d4bd3c99ea1a619e18e">104: Answer-focused and Position-aware Neural Question Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.838294</td>
<td>0.995636</td>
<td><a href="https://www.semanticscholar.org/paper/41b3180745068934bd9f7f2fbc2efc00c64d534b">400: Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting</a></td>
</tr>
<tr>
<td>0</td>
<td>0.822408</td>
<td>0.995362</td>
<td><a href="https://www.semanticscholar.org/paper/032274e57f7d8b456bd255fe76b909b2c1d7458e">1037: A Deep Reinforced Model for Abstractive Summarization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811718</td>
<td>0.995252</td>
<td><a href="https://www.semanticscholar.org/paper/3ed3310558a40388d24c77a43d7b3f13eb6e3d3c">162: Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793312</td>
<td>0.995217</td>
<td><a href="https://www.semanticscholar.org/paper/9b4a861151fabae1dfd61c917d031c86d26be704">190: Controllable Abstractive Summarization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.767238</td>
<td>0.994986</td>
<td><a href="https://www.semanticscholar.org/paper/1c5d2289e7db0a99f56badb081405f6f3bff6e38">30: Improving Abstractive Document Summarization with Salient Information Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.770054</td>
<td>0.994807</td>
<td><a href="https://www.semanticscholar.org/paper/9d3472849dc2cadf194ae29adbf46bdda861d8b7">398: Learning to Ask: Neural Question Generation for Reading Comprehension</a></td>
</tr>
</table></html>
