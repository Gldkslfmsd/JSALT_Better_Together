<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/756810258e3419af76aff38c895c20343b0602d0">1134: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a></td>
</tr>
<tr>
<td>0</td>
<td>0.991315</td>
<td>0.000519</td>
<td>NA:208229926</td>
</tr>
<tr>
<td>0</td>
<td>0.981663</td>
<td>0.000519</td>
<td>NA:211114050</td>
</tr>
<tr>
<td>0</td>
<td>0.840412</td>
<td>0.897951</td>
<td><a href="https://www.semanticscholar.org/paper/d9b7620f9b9653ada1a7ce36b0d6617f5979fff2">11: CAPT: Contrastive Pre-Training for Learning Denoised Sequence Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.829502</td>
<td>0.959340</td>
<td><a href="https://www.semanticscholar.org/paper/077108a733f9b505437d404bf44d85a5858a434f">2: Learning to Sample Replacements for ELECTRA Pre-Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.805354</td>
<td>0.595445</td>
<td><a href="https://www.semanticscholar.org/paper/a4f533f2b7d77b667e1f05b210924ec7c90cc5d1">0: How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.780958</td>
<td>0.901079</td>
<td><a href="https://www.semanticscholar.org/paper/54139d3e04d1d78f26cee9494423c1718160a434">8: Pre-Trained Language Models for Interactive Decision-Making</a></td>
</tr>
<tr>
<td>0</td>
<td>0.779350</td>
<td>0.000519</td>
<td><a href="https://www.semanticscholar.org/paper/70a44c65710086eab9de00071fda466bb724b6b4">0: Towards Understanding Label Regularization for Fine-tuning Pre-trained Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.773338</td>
<td>0.000519</td>
<td>NA:231925558</td>
</tr>
<tr>
<td>0</td>
<td>0.770138</td>
<td>0.025902</td>
<td><a href="https://www.semanticscholar.org/paper/b92a67109e990f850d02a62e9aa4812dc61eb78b">0: Neural Models Comparison in Natural Language Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.731160</td>
<td>0.998218</td>
<td><a href="https://www.semanticscholar.org/paper/e816f788767eec6a8ef0ea9eddd0e902435d4271">720: Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.761186</td>
<td>0.998098</td>
<td><a href="https://www.semanticscholar.org/paper/7a064df1aeada7e69e5173f7d4c8606f4470365b">2612: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.645731</td>
<td>0.998009</td>
<td><a href="https://www.semanticscholar.org/paper/d78aed1dac6656affa4a04cbf225ced11a83d103">285: Revealing the Dark Secrets of BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.689462</td>
<td>0.997992</td>
<td><a href="https://www.semanticscholar.org/paper/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2">2153: Transformers: State-of-the-Art Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.723697</td>
<td>0.997821</td>
<td><a href="https://www.semanticscholar.org/paper/93b8da28d006415866bf48f9a6e06b5242129195">2544: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.709892</td>
<td>0.997467</td>
<td><a href="https://www.semanticscholar.org/paper/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de">6919: RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></td>
</tr>
<tr>
<td>0</td>
<td>0.747442</td>
<td>0.997290</td>
<td><a href="https://www.semanticscholar.org/paper/bbde0f942a2541437e3edd55941ddafba6c5adb0">190: Transformers : State-ofthe-art Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.785416</td>
<td>0.997242</td>
<td><a href="https://www.semanticscholar.org/paper/b47381e04739ea3f392ba6c8faaf64105493c196">258: Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.824679</td>
<td>0.997047</td>
<td><a href="https://www.semanticscholar.org/paper/0e002114cd379efaca0ec5cda6d262b5fe0be104">83: MPNet: Masked and Permuted Pre-training for Language Understanding</a></td>
</tr>
</table></html>
