<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/fa025e5d117929361bcf798437957762eb5bb6d4">324: Zero-Shot Relation Extraction via Reading Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.835052</td>
<td>0.864714</td>
<td><a href="https://www.semanticscholar.org/paper/f867ae6525f39987bbc7398510d00bfb1bf3ac86">1: Zero-Shot Slot Filling via Latent Question Representation and Reading Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.814932</td>
<td>0.907238</td>
<td><a href="https://www.semanticscholar.org/paper/f03ff33bbc473c4e3efc62cced53ff16b172d9d8">89: Context-Aware Representations for Knowledge Base Relation Extraction</a></td>
</tr>
<tr>
<td>0</td>
<td>0.814763</td>
<td>0.979575</td>
<td><a href="https://www.semanticscholar.org/paper/baabaf96830b8c2388ba9a128d3677e936bec9d7">3: Learning with Instance Bundles for Reading Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800560</td>
<td>0.946942</td>
<td><a href="https://www.semanticscholar.org/paper/3cec488a0910b69f50811cebe8c655dca22078d5">1: Evidence Extraction for Machine Reading Comprehension with Deep Probabilistic Logic</a></td>
</tr>
<tr>
<td>0</td>
<td>0.799827</td>
<td>0.961338</td>
<td><a href="https://www.semanticscholar.org/paper/947fbe8dbe87ea8ec7024e483b6a27d445855940">0: Question answering model based on machine reading comprehension with knowledge enhancement and answer verification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.796991</td>
<td>0.921780</td>
<td><a href="https://www.semanticscholar.org/paper/6b54744bc8059ef53beaff5c903a6564a64f12c2">1: Incidental Supervision from Question-Answering Signals</a></td>
</tr>
<tr>
<td>0</td>
<td>0.796421</td>
<td>0.995153</td>
<td><a href="https://www.semanticscholar.org/paper/25c3b294b9ed2786c4476a25e8b36ebf49fd5b4b">27: ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.795825</td>
<td>0.984964</td>
<td><a href="https://www.semanticscholar.org/paper/11baa9cc02d6158edd9cb1f299579dad7828e162">20: An Improved Baseline for Sentence-level Relation Extraction</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792019</td>
<td>0.945317</td>
<td><a href="https://www.semanticscholar.org/paper/7c66309416e2dba5bfb8f04a67e2dadc821ad964">6: Multi-Perspective Context Aggregation for Semi-supervised Cloze-style Reading Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.777572</td>
<td>0.996631</td>
<td><a href="https://www.semanticscholar.org/paper/1283ca87e6215b7393eba1653a4a2e4bf28d2868">27: Learning Dense Representations of Phrases at Scale</a></td>
</tr>
<tr>
<td>0</td>
<td>0.716420</td>
<td>0.996433</td>
<td><a href="https://www.semanticscholar.org/paper/cf8c493079702ec420ab4fc9c0fabb56b2a16c84">271: SciTaiL: A Textual Entailment Dataset from Science Question Answering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.717669</td>
<td>0.996115</td>
<td><a href="https://www.semanticscholar.org/paper/06a1bf4a7333bbc78dbd7470666b33bd9e26882b">70: Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.730146</td>
<td>0.995753</td>
<td><a href="https://www.semanticscholar.org/paper/b29db655a18e7417e1188ba392a06b6314f0cb87">85: Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index</a></td>
</tr>
<tr>
<td>0</td>
<td>0.794001</td>
<td>0.994962</td>
<td><a href="https://www.semanticscholar.org/paper/f010affab57b5fcf1cd6be23df79d8ec98c7289c">867: TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</a></td>
</tr>
<tr>
<td>0</td>
<td>0.675355</td>
<td>0.994959</td>
<td><a href="https://www.semanticscholar.org/paper/c263507db2c15a8b2e3c955bda7b3c29a1ebd106">5: HINT3: Raising the bar for Intent Detection in the Wild</a></td>
</tr>
<tr>
<td>0</td>
<td>0.699856</td>
<td>0.994349</td>
<td><a href="https://www.semanticscholar.org/paper/1f4ab7875649852babc851090fc8e0ce73d0e323">5: How to Pre-Train Your Model? Comparison of Different Pre-Training Models for Biomedical Question Answering</a></td>
</tr>
<tr>
<td>0</td>
<td>0.635835</td>
<td>0.994262</td>
<td><a href="https://www.semanticscholar.org/paper/f9a5a7132949c6424e45f6b88abba83ecff26802">0: A Sequential and Intensive Weighted Language Modeling Scheme for Multi-Task Learning-Based Natural Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.773564</td>
<td>0.994219</td>
<td><a href="https://www.semanticscholar.org/paper/6642ad0b2fd2bf834388b804250eb9337ceb3f88">43: Improving Question Answering with External Knowledge</a></td>
</tr>
</table></html>
