<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de">6919: RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></td>
</tr>
<tr>
<td>0</td>
<td>0.983255</td>
<td>0.961707</td>
<td><a href="https://www.semanticscholar.org/paper/1360c8a066defbd061a62056030015a3c48648b4">9: ROBERTA: A ROBUSTLY OPTIMIZED BERT PRE-</a></td>
</tr>
<tr>
<td>0</td>
<td>0.806139</td>
<td>0.927941</td>
<td><a href="https://www.semanticscholar.org/paper/81815d9a847e406f8d49fb5051e2ae1055e13208">6: To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.787686</td>
<td>0.950066</td>
<td><a href="https://www.semanticscholar.org/paper/aa8f3e081ad2869c9469e2726364bdae0d9bdc7f">1: Fusing finetuned models for better pretraining</a></td>
</tr>
<tr>
<td>0</td>
<td>0.780817</td>
<td>0.978785</td>
<td><a href="https://www.semanticscholar.org/paper/5d34881ff68bd203ff790187e7e5c9e034389cfa">112: FastBERT: a Self-distilling BERT with Adaptive Inference Time</a></td>
</tr>
<tr>
<td>0</td>
<td>0.771611</td>
<td>0.980193</td>
<td><a href="https://www.semanticscholar.org/paper/e71fff3eb700250dc4f6e2c17626e4743bba300d">0: Can depth-adaptive BERT perform better on binary classification tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.766500</td>
<td>0.006198</td>
<td><a href="https://www.semanticscholar.org/paper/5fef57fac0f4dd82ccf2cb979f6eebeb3ccdb132">0: Multitask Learning Using BERT with Task-Embedded Attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.766342</td>
<td>0.015826</td>
<td><a href="https://www.semanticscholar.org/paper/e38617fec83db13655f643d29b7211dc1d865265">3: Hyperparameter Optimisation with Early Termination of Poor Performers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.765553</td>
<td>0.987475</td>
<td><a href="https://www.semanticscholar.org/paper/d3cacb4806886eb2fe59c90d4b6f822c24ff1822">80: Visualizing and Understanding the Effectiveness of BERT</a></td>
</tr>
<tr>
<td>0</td>
<td>0.764288</td>
<td>0.921414</td>
<td><a href="https://www.semanticscholar.org/paper/291016368158f28829c06c0a037e0ca1a6548cca">5: HiddenCut: Simple Data Augmentation for Natural Language Understanding with Better Generalizability</a></td>
</tr>
<tr>
<td>0</td>
<td>0.692108</td>
<td>0.998727</td>
<td><a href="https://www.semanticscholar.org/paper/1fa9ed2bea208511ae698a967875e943049f16b6">2630: HuggingFace's Transformers: State-of-the-art Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.682372</td>
<td>0.998697</td>
<td><a href="https://www.semanticscholar.org/paper/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2">2153: Transformers: State-of-the-Art Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.828508</td>
<td>0.998243</td>
<td><a href="https://www.semanticscholar.org/paper/7a064df1aeada7e69e5173f7d4c8606f4470365b">2612: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.677194</td>
<td>0.998009</td>
<td><a href="https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0">534: A Primer in BERTology: What We Know About How BERT Works</a></td>
</tr>
<tr>
<td>0</td>
<td>0.728274</td>
<td>0.997712</td>
<td><a href="https://www.semanticscholar.org/paper/e0c6abdbdecf04ffac65c440da77fb9d66bb474c">4103: XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.709892</td>
<td>0.997467</td>
<td><a href="https://www.semanticscholar.org/paper/756810258e3419af76aff38c895c20343b0602d0">1134: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a></td>
</tr>
<tr>
<td>0</td>
<td>0.675654</td>
<td>0.997246</td>
<td><a href="https://www.semanticscholar.org/paper/6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6">1649: Unsupervised Cross-lingual Representation Learning at Scale</a></td>
</tr>
<tr>
<td>0</td>
<td>0.675805</td>
<td>0.997142</td>
<td><a href="https://www.semanticscholar.org/paper/97906df07855b029b7aae7c2a1c6c5e8df1d531c">653: BERT Rediscovers the Classical NLP Pipeline</a></td>
</tr>
<tr>
<td>0</td>
<td>0.638940</td>
<td>0.996224</td>
<td><a href="https://www.semanticscholar.org/paper/93b8da28d006415866bf48f9a6e06b5242129195">2544: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</a></td>
</tr>
</table></html>
