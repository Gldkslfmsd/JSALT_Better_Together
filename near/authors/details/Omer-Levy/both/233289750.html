<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/7694aae9766d5f1fe74d900cd82aee898cb6e8e9">11: How to Train BERT with an Academic Budget</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811537</td>
<td>0.973490</td>
<td><a href="https://www.semanticscholar.org/paper/090be9851f8ceea2acaebce4763c780287c85693">6: Extremely Small BERT Models from Mixed-Vocabulary Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.752357</td>
<td>0.988851</td>
<td><a href="https://www.semanticscholar.org/paper/7b99c51d562e33309a46601c846abbe72a65c6a4">22: What to Pre-Train on? Efficient Intermediate Task Selection</a></td>
</tr>
<tr>
<td>0</td>
<td>0.744680</td>
<td>0.101876</td>
<td><a href="https://www.semanticscholar.org/paper/15c72d39aada7fddf6f7e0d19f093a1d3a81ffe9">1: Incremental learning with deep neural networks using a test-time oracle</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741435</td>
<td>0.967283</td>
<td><a href="https://www.semanticscholar.org/paper/b69ed195aacff0b6410eb3b8431ea968e42620af">17: Attentive Student Meets Multi-Task Teacher: Improved Knowledge Distillation for Pretrained Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.728200</td>
<td>0.982440</td>
<td><a href="https://www.semanticscholar.org/paper/9a0ce4d9b12337c1c4b65e56cab5b87dcc5f5aa2">1: XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.727681</td>
<td>0.125142</td>
<td><a href="https://www.semanticscholar.org/paper/4d873674436f035cf2ef7d353da46fd19531d52a">0: Challenges in leveraging GANs for few-shot data augmentation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.727620</td>
<td>0.951789</td>
<td><a href="https://www.semanticscholar.org/paper/a878ad41f8780ffda65bb3cdfea14ec51b224f37">2: Applying Ensembling Methods to BERT to Boost Model Performance</a></td>
</tr>
<tr>
<td>0</td>
<td>0.724964</td>
<td>0.830157</td>
<td><a href="https://www.semanticscholar.org/paper/ada909f9365c3930abdc7e8816c0655e88e7da2c">3: Efficient Contextual Representation Learning Without Softmax Layer</a></td>
</tr>
<tr>
<td>0</td>
<td>0.723804</td>
<td>0.495685</td>
<td><a href="https://www.semanticscholar.org/paper/8dea407aab8dd9bbd664ad1f471f7c9b5fd17980">18: Scalable Multi Corpora Neural Language Models for ASR</a></td>
</tr>
<tr>
<td>0</td>
<td>0.370180</td>
<td>0.998330</td>
<td><a href="https://www.semanticscholar.org/paper/a4d9f2e0320e2a125349ca2cac411368c4edb73b">3: GBe at FinCausal 2020, Task 2: Span-based Causality Extraction for Financial Documents</a></td>
</tr>
<tr>
<td>0</td>
<td>0.565934</td>
<td>0.998147</td>
<td><a href="https://www.semanticscholar.org/paper/eb2fc03b8865b8e1b4cb933d917ea269ebe14584">0: Learning from Mistakes: Using Mis-predictions as Harm Alerts in Language Pre-Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.737603</td>
<td>0.997342</td>
<td><a href="https://www.semanticscholar.org/paper/309ddaec87c97fb2e1958a7c82bc5444040c6ffe">3: Undivided Attention: Are Intermediate Layers Necessary for BERT?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.771608</td>
<td>0.996976</td>
<td><a href="https://www.semanticscholar.org/paper/df434c1289f3c7243b585cb9982afac3c5bf0439">0: MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.681315</td>
<td>0.996714</td>
<td><a href="https://www.semanticscholar.org/paper/ca25f7d6954561b342577643053b9ce8d618ade2">0: Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.597884</td>
<td>0.996143</td>
<td><a href="https://www.semanticscholar.org/paper/2d439ec2c301d058bd4a8b4743328e3d9939625e">3: Should You Mask 15% in Masked Language Modeling?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.748926</td>
<td>0.996099</td>
<td><a href="https://www.semanticscholar.org/paper/b6ec1e8f18185b4b3d46201359a440404575460c">1: METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals</a></td>
</tr>
<tr>
<td>0</td>
<td>0.724576</td>
<td>0.995601</td>
<td><a href="https://www.semanticscholar.org/paper/a0ab4106dabd6bc067c7b3e4db06807e2c0f6036">3: NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework</a></td>
</tr>
<tr>
<td>0</td>
<td>0.631066</td>
<td>0.995400</td>
<td><a href="https://www.semanticscholar.org/paper/c553280c1fc1d0bc7b94683bb75910e309b0d579">11: Larger-Scale Transformers for Multilingual Masked Language Modeling</a></td>
</tr>
</table></html>
