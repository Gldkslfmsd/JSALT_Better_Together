<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/e816f788767eec6a8ef0ea9eddd0e902435d4271">720: Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.821396</td>
<td>0.938317</td>
<td><a href="https://www.semanticscholar.org/paper/62da76a8dbff4de50495be2f4746f25c4cd7ac0c">0: CLIN-X: pre-trained language models and a study on cross-task transfer for concept extraction in the clinical domain</a></td>
</tr>
<tr>
<td>0</td>
<td>0.810143</td>
<td>0.977690</td>
<td><a href="https://www.semanticscholar.org/paper/06a1bf4a7333bbc78dbd7470666b33bd9e26882b">70: Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.801683</td>
<td>0.854324</td>
<td><a href="https://www.semanticscholar.org/paper/992f949de6d9c9355e6f1b224fdff4ac1967b2bf">11: Feature Adaptation of Pre-Trained Language Models across Languages and Domains with Robust Self-Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.796681</td>
<td>0.959127</td>
<td><a href="https://www.semanticscholar.org/paper/b11f392ff1b9b2c4fe77f7560e249dbd9e57cdd8">0: Robust Question Answering with Task Adaptive Pretraining and Data Augmentation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.795960</td>
<td>0.265424</td>
<td><a href="https://www.semanticscholar.org/paper/f04f38ce7fe04f61ebe538348eaf881b8ce19160">6: Modeling Non-Linguistic Contextual Signals in LSTM Language Models Via Domain Adaptation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.795616</td>
<td>0.826717</td>
<td><a href="https://www.semanticscholar.org/paper/87abd59cbfc342e4feade633d03b0e688abcbdaa">2: Feature Adaptation of Pre-Trained Language Models across Languages and Domains for Text Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.791140</td>
<td>0.928263</td>
<td><a href="https://www.semanticscholar.org/paper/16976d834ba10e7a6fb497c463447278b3d2029b">0: Multi-Task Learning and Domain-Specific Models to Improve Robustness of QA System</a></td>
</tr>
<tr>
<td>0</td>
<td>0.789548</td>
<td>0.973421</td>
<td><a href="https://www.semanticscholar.org/paper/157a2b417f94b0d21c3192ee877f9a3f0c6ca2c9">12: Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining</a></td>
</tr>
<tr>
<td>0</td>
<td>0.787999</td>
<td>0.970151</td>
<td><a href="https://www.semanticscholar.org/paper/19e2f3a1e0c3b8340c14cb83e9ca6878a2598852">2: IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.760669</td>
<td>0.998666</td>
<td><a href="https://www.semanticscholar.org/paper/b47381e04739ea3f392ba6c8faaf64105493c196">258: Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.731160</td>
<td>0.998218</td>
<td><a href="https://www.semanticscholar.org/paper/756810258e3419af76aff38c895c20343b0602d0">1134: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a></td>
</tr>
<tr>
<td>0</td>
<td>0.760732</td>
<td>0.997785</td>
<td><a href="https://www.semanticscholar.org/paper/80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef">325: ERNIE 2.0: A Continual Pre-training Framework for Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.764669</td>
<td>0.997726</td>
<td><a href="https://www.semanticscholar.org/paper/6afe0fb12ceacadbbfed7202d430770a3f344731">138: Revisiting Pre-Trained Models for Chinese Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.776019</td>
<td>0.997554</td>
<td><a href="https://www.semanticscholar.org/paper/d0086b86103a620a86bc918746df0aa642e2a8a3">668: Language Models as Knowledge Bases?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.705760</td>
<td>0.997181</td>
<td><a href="https://www.semanticscholar.org/paper/4f03e69963b9649950ba29ae864a0de8c14f1f86">158: K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</a></td>
</tr>
<tr>
<td>0</td>
<td>0.690573</td>
<td>0.997144</td>
<td><a href="https://www.semanticscholar.org/paper/3b2538f84812f434c740115c185be3e5e216c526">185: Cross-Lingual Ability of Multilingual BERT: An Empirical Study</a></td>
</tr>
<tr>
<td>0</td>
<td>0.693712</td>
<td>0.997135</td>
<td><a href="https://www.semanticscholar.org/paper/d9f6ada77448664b71128bb19df15765336974a6">797: SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.664615</td>
<td>0.997062</td>
<td><a href="https://www.semanticscholar.org/paper/81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85">248: How Can We Know What Language Models Know?</a></td>
</tr>
</table></html>
