<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/443516aeb2819d4d362ffe7d5418a54e5427a016">301: ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.860317</td>
<td>0.860610</td>
<td><a href="https://www.semanticscholar.org/paper/868207797e69df5055f5c3fd4aa78a33e5a7ca45">92: Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics</a></td>
</tr>
<tr>
<td>0</td>
<td>0.851368</td>
<td>0.984653</td>
<td><a href="https://www.semanticscholar.org/paper/a292cfbb556fff965eb46fc142f596d8dfca98df">1: Meta-Evaluation of a Diagnostic Quality Metric for Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.847737</td>
<td>0.992593</td>
<td><a href="https://www.semanticscholar.org/paper/9b69dbee140bae5da2fc46959b359b4cbd9b0ea4">9: Mining the Correlation between Human and Automatic Evaluation at Sentence Level</a></td>
</tr>
<tr>
<td>0</td>
<td>0.842526</td>
<td>0.993408</td>
<td><a href="https://www.semanticscholar.org/paper/91c9a3bf96ff0f8e981eeb45a7f27bb1178dde2c">15: Significance tests of automatic machine translation evaluation metrics</a></td>
</tr>
<tr>
<td>0</td>
<td>0.827607</td>
<td>0.979986</td>
<td><a href="https://www.semanticscholar.org/paper/2cbcf9319e879511a2031f1c057a0935b069d972">5: Automatic Metrics for Machine Translation Evaluation and Minority Languages</a></td>
</tr>
<tr>
<td>0</td>
<td>0.823294</td>
<td>0.986956</td>
<td><a href="https://www.semanticscholar.org/paper/2bd12b08763607455bc6ec2f6e52aa7d3b820129">2: Automatic Evaluation Metric for Machine Translation that is Independent of Sentence Length</a></td>
</tr>
<tr>
<td>0</td>
<td>0.819355</td>
<td>0.980718</td>
<td><a href="https://www.semanticscholar.org/paper/0f2fcc603bef154b56efd98c206583f6a5812a64">3: An Automatic Metric for Machine Translation Evaluation Based on Maximum Similarity</a></td>
</tr>
<tr>
<td>0</td>
<td>0.818704</td>
<td>0.983035</td>
<td><a href="https://www.semanticscholar.org/paper/f9d781e2fe44cad806fb765f721a0fb2721523eb">1: Meta-evaluation of Machine Translation Using Parallel Legal Texts</a></td>
</tr>
<tr>
<td>0</td>
<td>0.817122</td>
<td>0.982873</td>
<td><a href="https://www.semanticscholar.org/paper/40901bd2a0f4ae9b2e8f05ada66289051e339dcc">0: Measures of Machine Translation Quality</a></td>
</tr>
<tr>
<td>0</td>
<td>0.730428</td>
<td>0.999061</td>
<td><a href="https://www.semanticscholar.org/paper/0a1f4cc5e1d7ccdce98c65545bbcccc23a6c16e7">716: Re-evaluating the Role of Bleu in Machine Translation Research</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763552</td>
<td>0.998170</td>
<td><a href="https://www.semanticscholar.org/paper/c36578358e375f31a8a4b33b92a5777729a9a8ba">21: RED: A Reference Dependency Based MT Evaluation Metric</a></td>
</tr>
<tr>
<td>0</td>
<td>0.788569</td>
<td>0.998095</td>
<td><a href="https://www.semanticscholar.org/paper/51951073580f6995e55be873db9a7f6a9736ca86">2189: A Study of Translation Edit Rate with Targeted Human Annotation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.821631</td>
<td>0.997975</td>
<td><a href="https://www.semanticscholar.org/paper/0f8992ee6418d367d8e50ecbb59b08ea15e8431f">388: Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.818497</td>
<td>0.997718</td>
<td><a href="https://www.semanticscholar.org/paper/417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f">1569: Automatic evaluation of machine translation quality using n-gram co-occurrence statistics</a></td>
</tr>
<tr>
<td>0</td>
<td>0.767735</td>
<td>0.997339</td>
<td><a href="https://www.semanticscholar.org/paper/a5899f1ec92af7d01f35225161430116a6eabbea">1635: A STUDY OF TRANSLATION ERROR RATE WITH TARGETED HUMAN ANNOTATION</a></td>
</tr>
<tr>
<td>0</td>
<td>0.837463</td>
<td>0.997195</td>
<td><a href="https://www.semanticscholar.org/paper/7b598fcbbf6f92c630f33d25f1c1b97b52a6faa8">13: How Does Automatic Machine Translation Evaluation Correlate with Human Scoring as the Number of Reference Translations Increases?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.751019</td>
<td>0.997112</td>
<td><a href="https://www.semanticscholar.org/paper/6b3f1ea74964a27ad3785eb8a3462231fbcefce2">99: Towards Automatic Error Analysis of Machine Translation Output</a></td>
</tr>
<tr>
<td>0</td>
<td>0.612475</td>
<td>0.997075</td>
<td><a href="https://www.semanticscholar.org/paper/d3ae09b218763f3f7e2d3d62fc248504fc368215">9: BEER: BEtter Evaluation as Ranking</a></td>
</tr>
</table></html>
