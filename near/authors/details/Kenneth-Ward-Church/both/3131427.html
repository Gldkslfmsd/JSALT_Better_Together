<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/9e09439f009dd0d4618949e4e01964df2e1f1d2d">65: - 1-What â€™ s Wrong with Adding One ?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.731691</td>
<td>0.128599</td>
<td><a href="https://www.semanticscholar.org/paper/319a78ae4e3b4169c45e34085ac165fc8c25a0be">0: Gos corpus n-grams 1.0</a></td>
</tr>
<tr>
<td>0</td>
<td>0.718123</td>
<td>0.128599</td>
<td>NA:61570845</td>
</tr>
<tr>
<td>0</td>
<td>0.707350</td>
<td>0.788046</td>
<td><a href="https://www.semanticscholar.org/paper/ed97e813df6610c5a373a48aa439fddb54c84f6e">220: On a Distribution Law for Word Frequencies</a></td>
</tr>
<tr>
<td>0</td>
<td>0.695085</td>
<td>0.795215</td>
<td><a href="https://www.semanticscholar.org/paper/e7ef88a7421c4925f44c281c9f32098702c8c0e1">0: Using Sparse Training to Estimate Context-Sensitive Translation Probabilities</a></td>
</tr>
<tr>
<td>0</td>
<td>0.688899</td>
<td>0.015765</td>
<td><a href="https://www.semanticscholar.org/paper/104ca771588d0b6230cea1df2366142433b08057">1: How Many Words Are There</a></td>
</tr>
<tr>
<td>0</td>
<td>0.676004</td>
<td>0.775230</td>
<td><a href="https://www.semanticscholar.org/paper/610211d5d7ef7f26e08af6f17885615e2d42c898">15: A Survey of Techniques for Unsupervised Word Sense Induction</a></td>
</tr>
<tr>
<td>0</td>
<td>0.675236</td>
<td>0.421638</td>
<td><a href="https://www.semanticscholar.org/paper/7dc3946464982110189c632052529a3523500dc6">2: In Search Of The Semantic Value(S) Of An Occurrence: An Example And A Framework</a></td>
</tr>
<tr>
<td>0</td>
<td>0.671150</td>
<td>0.128599</td>
<td><a href="https://www.semanticscholar.org/paper/8cabf988eafa98c80ebbff2cbc248c7bbf766d94">0: A Word Can Hide Another One. The Case of the Function Word de</a></td>
</tr>
<tr>
<td>0</td>
<td>0.670858</td>
<td>0.642642</td>
<td><a href="https://www.semanticscholar.org/paper/303410f2c64f0424e59cd233783abcd578e268ca">12: Analysis and System Combination of Phrase- and N-Gram-Based Statistical Machine Translation Systems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.508252</td>
<td>0.988452</td>
<td><a href="https://www.semanticscholar.org/paper/a3fe33c2ff42137dc345c61771618f460e9ee614">1: Statistical Behavior Analysis of Smoothing Methods for Language Models of Mandarin Data Sets</a></td>
</tr>
<tr>
<td>0</td>
<td>-1.000000</td>
<td>0.986151</td>
<td><a href="https://www.semanticscholar.org/paper/c245e054aacc430c089bd1200908ec516a7456a5">1: Analyzing the Statistical Behavior of Smoothing Method</a></td>
</tr>
<tr>
<td>0</td>
<td>0.740532</td>
<td>0.984918</td>
<td><a href="https://www.semanticscholar.org/paper/ef9190e7669ea5523c3ef61180b35385b0ea345f">292: A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams</a></td>
</tr>
<tr>
<td>0</td>
<td>0.556753</td>
<td>0.984834</td>
<td><a href="https://www.semanticscholar.org/paper/49b2862ab73be40bf69ac3f457039f18d12df0ae">74: Adaptive Language Modeling Using the Maximum Entropy Principle</a></td>
</tr>
<tr>
<td>0</td>
<td>0.605498</td>
<td>0.982529</td>
<td><a href="https://www.semanticscholar.org/paper/1f5d21625f8264f455591b3c7cbdac18b983b3c0">865: The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression</a></td>
</tr>
<tr>
<td>0</td>
<td>0.593864</td>
<td>0.981680</td>
<td><a href="https://www.semanticscholar.org/paper/0b26fa1b848ed808a0511db34bce2426888f0b68">420: Adaptive Statistical Language Modeling; A Maximum Entropy Approach</a></td>
</tr>
<tr>
<td>0</td>
<td>0.560171</td>
<td>0.981343</td>
<td><a href="https://www.semanticscholar.org/paper/0451b89c28c93ca3f30f76bb7d590f2a0620850e">12: NYU Language Modeling Experiments for the 1995 CSR Evaluation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.723976</td>
<td>0.980879</td>
<td><a href="https://www.semanticscholar.org/paper/461d44a5636b5908f53084bb6a4364dfb9d342bf">94: On the Estimation of 'Small' Probabilities by Leaving-One-Out</a></td>
</tr>
<tr>
<td>0</td>
<td>0.553808</td>
<td>0.980707</td>
<td><a href="https://www.semanticscholar.org/paper/9ebac969a7f6a440765be4623f078cd54e84ca7f">0: ANALYSISOF SMOOTHINGMETHODS FOR LANGUAGE MODELS ONSMALLCHINESECO RPORA</a></td>
</tr>
</table></html>
