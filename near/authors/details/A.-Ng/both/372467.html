<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/3127190433230b3dc1abd0680bb58dced4bcd90e">2997: Large Scale Distributed Deep Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.875765</td>
<td>0.848008</td>
<td><a href="https://www.semanticscholar.org/paper/2d6bd6218a1f81f4101574149c8aa5e2b8748983">3: On Distributed Deep Network for Processing Large-Scale Sets of Complex Data</a></td>
</tr>
<tr>
<td>0</td>
<td>0.821369</td>
<td>0.055596</td>
<td><a href="https://www.semanticscholar.org/paper/48dfe11a38b0c13542ac3d94953ec7a7c44b89ff">0: Predicting Throughput of Distributed Stochastic Gradient Descent</a></td>
</tr>
<tr>
<td>0</td>
<td>0.812434</td>
<td>0.820717</td>
<td><a href="https://www.semanticscholar.org/paper/c26ddc4be489c265961f6418dff385fc5ba16489">0: Challenges for Machine Learning on Distributed Platforms</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811315</td>
<td>0.826111</td>
<td><a href="https://www.semanticscholar.org/paper/34b678e3ff93fe089fe1d48e248878f18ca2b5f1">1: Practical Lessons of Distributed Deep Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.805907</td>
<td>0.696105</td>
<td><a href="https://www.semanticscholar.org/paper/36630a3b605da02e2aac36ea56d2a9ba88710c2a">0: Poster Abstract: Model Average-based Distributed Training for Sparse Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.801057</td>
<td>0.055596</td>
<td>NA:246821779</td>
</tr>
<tr>
<td>0</td>
<td>0.797974</td>
<td>0.385551</td>
<td><a href="https://www.semanticscholar.org/paper/481813caadf69883205537cd47d7b6ad6572ea04">5: Deep Generative Models that Solve PDEs: Distributed Computing for Training Large Data-Free Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.792035</td>
<td>0.305774</td>
<td><a href="https://www.semanticscholar.org/paper/657c6950b5aaec208e718b2f6831dd324bc0b1c2">17: P3: Distributed Deep Graph Learning at Scale</a></td>
</tr>
<tr>
<td>0</td>
<td>0.791005</td>
<td>0.905434</td>
<td><a href="https://www.semanticscholar.org/paper/a70b1b3ffb9170886c43282008e59680d4d16360">1: DBS: Dynamic Batch Size For Distributed Deep Neural Network Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.808505</td>
<td>0.980884</td>
<td><a href="https://www.semanticscholar.org/paper/e69c8b5df8a4178b1c8c7f154a761147a6f030be">655: Project Adam: Building an Efficient and Scalable Deep Learning Training System</a></td>
</tr>
<tr>
<td>0</td>
<td>0.754423</td>
<td>0.980084</td>
<td><a href="https://www.semanticscholar.org/paper/52eaf7415eceb8e569173479790e4d1e860b5fc2">35: Dogwild! â€“ Distributed Hogwild for CPU & GPU</a></td>
</tr>
<tr>
<td>0</td>
<td>0.775376</td>
<td>0.972483</td>
<td><a href="https://www.semanticscholar.org/paper/3705da054c46880b1d0493795993614f5142819b">76: Ako: Decentralised Deep Learning with Partial Gradient Exchange</a></td>
</tr>
<tr>
<td>0</td>
<td>0.802528</td>
<td>0.972294</td>
<td><a href="https://www.semanticscholar.org/paper/46983b869dd6149bae5f5593a8927bc7f36966a2">9: A Data and Model-Parallel, Distributed and Scalable Framework for Training of Deep Networks in Apache Spark</a></td>
</tr>
<tr>
<td>0</td>
<td>0.699198</td>
<td>0.970660</td>
<td><a href="https://www.semanticscholar.org/paper/9766fee22e0d5e9247dd28556ba48ee371d33f5b">17: Faster Asynchronous SGD</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811119</td>
<td>0.968707</td>
<td><a href="https://www.semanticscholar.org/paper/32192d744d86e7cde73f0c9aa773214f88619a9e">152: SparkNet: Training Deep Networks in Spark</a></td>
</tr>
<tr>
<td>0</td>
<td>0.761422</td>
<td>0.968528</td>
<td><a href="https://www.semanticscholar.org/paper/6b39bea0ae1720dcbc1ed19ffa697114c4d356c4">26: Scalable Deep Learning on Distributed Infrastructures</a></td>
</tr>
<tr>
<td>0</td>
<td>0.756124</td>
<td>0.968097</td>
<td><a href="https://www.semanticscholar.org/paper/2fe56066a61ab9db229c7d1e6ade14deacbbe9d6">51: Scalable Deep Learning on Distributed Infrastructures: Challenges, Techniques and Tools</a></td>
</tr>
<tr>
<td>0</td>
<td>0.715797</td>
<td>0.967563</td>
<td><a href="https://www.semanticscholar.org/paper/7e122f57fc25509c8b0e1e08003192bafe0d43d8">55: Omnivore: An Optimizer for Multi-device Deep Learning on CPUs and GPUs</a></td>
</tr>
</table></html>
