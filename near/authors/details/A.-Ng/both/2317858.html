<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/0ca7d208ff8d81377e0eaa9723820aeae7a7322d">781: Grounded Compositional Semantics for Finding and Describing Images with Sentences</a></td>
</tr>
<tr>
<td>0</td>
<td>0.819644</td>
<td>0.646665</td>
<td><a href="https://www.semanticscholar.org/paper/1025bdeb849b9e30490003c5e2657a661701e9c5">3: Automated Image Captioning Using ConvNets and Recurrent Neural Network</a></td>
</tr>
<tr>
<td>0</td>
<td>0.812086</td>
<td>0.847067</td>
<td><a href="https://www.semanticscholar.org/paper/34ccd89adc90215dc92af57b5809c215db8c7e91">2: Associating Images with Sentences Using Recurrent Canonical Correlation Analysis</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793755</td>
<td>-0.054319</td>
<td><a href="https://www.semanticscholar.org/paper/5f1df0f0555ef92d641ff9affd2b07e89862e466">0: Towards learning domain-general representations for language from multi-modal data</a></td>
</tr>
<tr>
<td>0</td>
<td>0.789736</td>
<td>0.870604</td>
<td><a href="https://www.semanticscholar.org/paper/c8f8ea39c64cf08792cd49c6ad04e85e3b90c88f">5: CRUR: Coupled-Recurrent Unit for Unification, Conceptualization and Context Capture for Language Representation - A Generalization of Bi Directional LSTM</a></td>
</tr>
<tr>
<td>0</td>
<td>0.789025</td>
<td>0.920343</td>
<td><a href="https://www.semanticscholar.org/paper/a72b8bbd039989db39769da836cdb287737deb92">431: Mind's eye: A recurrent visual representation for image caption generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.781769</td>
<td>0.662445</td>
<td><a href="https://www.semanticscholar.org/paper/34c10e8ecb5843a63215eb6b2314a85623361a0a">0: Encoding Lexico-Semantic Knowledge using Ensembles of Feature Maps from Deep Convolutional Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.780608</td>
<td>0.799437</td>
<td><a href="https://www.semanticscholar.org/paper/95c379fa77e05cd2adc9f65e0dbb8e8065e30c43">1: Neural Symbolic Representation Learning for Image Captioning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.780314</td>
<td>0.457130</td>
<td><a href="https://www.semanticscholar.org/paper/56d1003fd02346e93354ab55cd204485c268512a">40: Compositional Explanations of Neurons</a></td>
</tr>
<tr>
<td>0</td>
<td>0.772232</td>
<td>0.848921</td>
<td><a href="https://www.semanticscholar.org/paper/936227f7483938097cc1cdd3032016df54dbd5b6">55: Learning to generalize to new compositions in image understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.757406</td>
<td>0.972463</td>
<td><a href="https://www.semanticscholar.org/paper/cc18cb42289fd570a06896b5543b085ebabee57b">33: Training and Evaluating Multimodal Word Embeddings with Large-scale Web Annotated Images</a></td>
</tr>
<tr>
<td>0</td>
<td>0.768151</td>
<td>0.972211</td>
<td><a href="https://www.semanticscholar.org/paper/fad611e35b3731740b4d8b754241e77add5a70b9">571: Multimodal Neural Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.806029</td>
<td>0.971090</td>
<td><a href="https://www.semanticscholar.org/paper/074eb7a6940d8629c6f26594a1f485d42aba8821">8: Simple Image Description Generator via a Linear Phrase-based Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.710887</td>
<td>0.966904</td>
<td><a href="https://www.semanticscholar.org/paper/4e614e344ecbb36770d45fc14d3b5152b653aa97">0: Exploration on Grounded Word Embedding: Matching Words and Images with Image-Enhanced Skip-Gram Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.695924</td>
<td>0.966422</td>
<td><a href="https://www.semanticscholar.org/paper/83eaeab8ba1cf8680580e2910d47518fea83639f">5: Better Text Understanding Through Image-To-Text Transfer</a></td>
</tr>
<tr>
<td>0</td>
<td>0.597525</td>
<td>0.960236</td>
<td><a href="https://www.semanticscholar.org/paper/bc81a001f53f3cf79d64aa56a62885693ff29ddc">0: Semantic generation mechanism of news images based on Concept-Net</a></td>
</tr>
<tr>
<td>0</td>
<td>0.691446</td>
<td>0.956621</td>
<td><a href="https://www.semanticscholar.org/paper/0a59a5ed672dda9f45187bdb7727922e21f24fd9">1: An image retrieval method based on semantic matching with multiple positional representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.757715</td>
<td>0.955504</td>
<td><a href="https://www.semanticscholar.org/paper/3298161a0021e1e56d18840bfb5a3df52cf799a5">0: Image-Sentence Multimodal Embedding with Instructive Objectives</a></td>
</tr>
<tr>
<td>0</td>
<td>0.545164</td>
<td>0.955083</td>
<td><a href="https://www.semanticscholar.org/paper/f6066f9ab66c10b038f2afffb621b9db8042a4ed">0: Exploring the Impact of Training Data Bias on Automatic Generation of Video Captions</a></td>
</tr>
</table></html>
<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/0ca7d208ff8d81377e0eaa9723820aeae7a7322d">781: Grounded Compositional Semantics for Finding and Describing Images with Sentences</a></td>
</tr>
<tr>
<td>0</td>
<td>0.819644</td>
<td>0.646665</td>
<td><a href="https://www.semanticscholar.org/paper/1025bdeb849b9e30490003c5e2657a661701e9c5">3: Automated Image Captioning Using ConvNets and Recurrent Neural Network</a></td>
</tr>
<tr>
<td>0</td>
<td>0.812086</td>
<td>0.847067</td>
<td><a href="https://www.semanticscholar.org/paper/34ccd89adc90215dc92af57b5809c215db8c7e91">2: Associating Images with Sentences Using Recurrent Canonical Correlation Analysis</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793755</td>
<td>-0.054319</td>
<td><a href="https://www.semanticscholar.org/paper/5f1df0f0555ef92d641ff9affd2b07e89862e466">0: Towards learning domain-general representations for language from multi-modal data</a></td>
</tr>
<tr>
<td>0</td>
<td>0.789736</td>
<td>0.870604</td>
<td><a href="https://www.semanticscholar.org/paper/c8f8ea39c64cf08792cd49c6ad04e85e3b90c88f">5: CRUR: Coupled-Recurrent Unit for Unification, Conceptualization and Context Capture for Language Representation - A Generalization of Bi Directional LSTM</a></td>
</tr>
<tr>
<td>0</td>
<td>0.789025</td>
<td>0.920343</td>
<td><a href="https://www.semanticscholar.org/paper/a72b8bbd039989db39769da836cdb287737deb92">431: Mind's eye: A recurrent visual representation for image caption generation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.781769</td>
<td>0.662445</td>
<td><a href="https://www.semanticscholar.org/paper/34c10e8ecb5843a63215eb6b2314a85623361a0a">0: Encoding Lexico-Semantic Knowledge using Ensembles of Feature Maps from Deep Convolutional Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.780608</td>
<td>0.799437</td>
<td><a href="https://www.semanticscholar.org/paper/95c379fa77e05cd2adc9f65e0dbb8e8065e30c43">1: Neural Symbolic Representation Learning for Image Captioning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.780314</td>
<td>0.457130</td>
<td><a href="https://www.semanticscholar.org/paper/56d1003fd02346e93354ab55cd204485c268512a">40: Compositional Explanations of Neurons</a></td>
</tr>
<tr>
<td>0</td>
<td>0.772232</td>
<td>0.848921</td>
<td><a href="https://www.semanticscholar.org/paper/936227f7483938097cc1cdd3032016df54dbd5b6">55: Learning to generalize to new compositions in image understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.757406</td>
<td>0.972463</td>
<td><a href="https://www.semanticscholar.org/paper/cc18cb42289fd570a06896b5543b085ebabee57b">33: Training and Evaluating Multimodal Word Embeddings with Large-scale Web Annotated Images</a></td>
</tr>
<tr>
<td>0</td>
<td>0.768151</td>
<td>0.972211</td>
<td><a href="https://www.semanticscholar.org/paper/fad611e35b3731740b4d8b754241e77add5a70b9">571: Multimodal Neural Language Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.806029</td>
<td>0.971090</td>
<td><a href="https://www.semanticscholar.org/paper/074eb7a6940d8629c6f26594a1f485d42aba8821">8: Simple Image Description Generator via a Linear Phrase-based Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.710887</td>
<td>0.966904</td>
<td><a href="https://www.semanticscholar.org/paper/4e614e344ecbb36770d45fc14d3b5152b653aa97">0: Exploration on Grounded Word Embedding: Matching Words and Images with Image-Enhanced Skip-Gram Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.695924</td>
<td>0.966422</td>
<td><a href="https://www.semanticscholar.org/paper/83eaeab8ba1cf8680580e2910d47518fea83639f">5: Better Text Understanding Through Image-To-Text Transfer</a></td>
</tr>
<tr>
<td>0</td>
<td>0.597525</td>
<td>0.960236</td>
<td><a href="https://www.semanticscholar.org/paper/bc81a001f53f3cf79d64aa56a62885693ff29ddc">0: Semantic generation mechanism of news images based on Concept-Net</a></td>
</tr>
<tr>
<td>0</td>
<td>0.691446</td>
<td>0.956621</td>
<td><a href="https://www.semanticscholar.org/paper/0a59a5ed672dda9f45187bdb7727922e21f24fd9">1: An image retrieval method based on semantic matching with multiple positional representations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.757715</td>
<td>0.955504</td>
<td><a href="https://www.semanticscholar.org/paper/3298161a0021e1e56d18840bfb5a3df52cf799a5">0: Image-Sentence Multimodal Embedding with Instructive Objectives</a></td>
</tr>
<tr>
<td>0</td>
<td>0.545164</td>
<td>0.955083</td>
<td><a href="https://www.semanticscholar.org/paper/f6066f9ab66c10b038f2afffb621b9db8042a4ed">0: Exploring the Impact of Training Data Bias on Automatic Generation of Video Captions</a></td>
</tr>
</table></html>
