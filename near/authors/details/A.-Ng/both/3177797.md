<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/f198043a866e9187925a8d8db9a55e3bfdd47f2c">30792: Latent Dirichlet Allocation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.858724</td>
<td>0.986619</td>
<td><a href="https://www.semanticscholar.org/paper/2c224bcaa9fa736dd1700f4e62430fad4b58b437">21: Matching Results of Latent Dirichlet Allocation for Text</a></td>
</tr>
<tr>
<td>0</td>
<td>0.850930</td>
<td>0.929307</td>
<td><a href="https://www.semanticscholar.org/paper/64e9b6093924f86130845cf47778f5393749f9e8">51: Kernel Topic Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.849049</td>
<td>0.955520</td>
<td><a href="https://www.semanticscholar.org/paper/f2f66d2e570ea8e14b48dd7eebb706854407883a">15: Topic Models Conditioned on Relations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.845950</td>
<td>0.963557</td>
<td><a href="https://www.semanticscholar.org/paper/20fe32e4ac65f59e6f1442522f58c26d2849f500">307: Sparse Additive Generative Models of Text</a></td>
</tr>
<tr>
<td>0</td>
<td>0.832885</td>
<td>0.928894</td>
<td><a href="https://www.semanticscholar.org/paper/b82a13558947840c19ff5b87bd62ba07894c948d">30: Latent dirichlet language model for speech recognition</a></td>
</tr>
<tr>
<td>0</td>
<td>0.824491</td>
<td>0.966589</td>
<td><a href="https://www.semanticscholar.org/paper/3410766cda0527510051e83ff4864f3a2ff65a41">0: Empirical prior latent Dirichlet allocation model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.824046</td>
<td>0.957324</td>
<td><a href="https://www.semanticscholar.org/paper/e01199a7629a9817ec5ef7e9cdb637128fccd0b3">1: Bayesian latent topic clustering model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.823011</td>
<td>0.929354</td>
<td><a href="https://www.semanticscholar.org/paper/8fd980bccb20cbcc96e66534418ad54ede58a59a">0: Stochastic Variational Optimization of a Hierarchical Dirichlet Process Latent Beta-Liouville Topic Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.820319</td>
<td>0.823525</td>
<td><a href="https://www.semanticscholar.org/paper/9738c1362af11eb993a6c14063b3cdeb19175917">8: Partially collapsed Gibbs sampling for latent Dirichlet allocation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.640493</td>
<td>0.994834</td>
<td><a href="https://www.semanticscholar.org/paper/064702f560c9fe4eae921d6f07c208d3df7c7c4f">89: A Theoretical and Practical Implementation Tutorial on Topic Modeling and Gibbs Sampling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.728451</td>
<td>0.994558</td>
<td><a href="https://www.semanticscholar.org/paper/b22de434b462558a127f327f29e2b0c673c0d7ab">388: Latent Dirichlet allocation (LDA) and topic modeling: models, applications, a survey</a></td>
</tr>
<tr>
<td>0</td>
<td>0.490252</td>
<td>0.994046</td>
<td><a href="https://www.semanticscholar.org/paper/7314be5cd836c8f06bd1ecab565b00b65259eac6">3789: Probabilistic topic models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.694311</td>
<td>0.993671</td>
<td><a href="https://www.semanticscholar.org/paper/59d7d8415dacd300eb4d98b0da3cb32d27503b36">211: Visualizing Topic Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.761624</td>
<td>0.993616</td>
<td><a href="https://www.semanticscholar.org/paper/edd0aec78e53c08c90305e7a1234c2644d8f104a">1738: Reading Tea Leaves: How Humans Interpret Topic Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.701232</td>
<td>0.992990</td>
<td><a href="https://www.semanticscholar.org/paper/43861ec6fe4efb630c6da9697c5f6248e42ccd37">179: Improving Topic Coherence with Regularized Topic Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.721640</td>
<td>0.992825</td>
<td><a href="https://www.semanticscholar.org/paper/c3a5ee3eb7e35cf960cab3c17ea8ea5abfcab9a4">216: Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA</a></td>
</tr>
<tr>
<td>0</td>
<td>0.727059</td>
<td>0.992720</td>
<td><a href="https://www.semanticscholar.org/paper/e9a822d6fe66b0cfd0b4c5a10411172b80346bf1">1332: Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora</a></td>
</tr>
<tr>
<td>0</td>
<td>0.790448</td>
<td>0.992539</td>
<td><a href="https://www.semanticscholar.org/paper/fb32ed2d631dd7803a03b9c44eb6da10a7b5b00f">18: A comparison of the performance of latent Dirichlet allocation and the Dirichlet multinomial mixture model on short text</a></td>
</tr>
</table></html>
