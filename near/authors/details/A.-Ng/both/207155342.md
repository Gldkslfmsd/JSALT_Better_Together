<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/f65020fc3b1692d7989e099d6b6e698be5a50a93">2527: Apprenticeship learning via inverse reinforcement learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.862080</td>
<td>0.981435</td>
<td><a href="https://www.semanticscholar.org/paper/94858023ba2e784ee1b893984730197f8b3a1060">0: Stochastic convex optimization for provably efficient apprenticeship learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.860135</td>
<td>0.985865</td>
<td><a href="https://www.semanticscholar.org/paper/3b76c51beceb5057b1285bd7d709817cda17adc0">248: Exploration and apprenticeship learning in reinforcement learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.842069</td>
<td>0.985010</td>
<td><a href="https://www.semanticscholar.org/paper/a34698d5e89ce6d73e6d69a7d09cec36a80a68de">3: Inverse Reinforcement Learning Based on Behaviors of a Learning Agent</a></td>
</tr>
<tr>
<td>0</td>
<td>0.834375</td>
<td>0.966043</td>
<td><a href="https://www.semanticscholar.org/paper/ca40b0e554386c35e15b724f031d2640ed3fc7b1">6: Inverse Reinforcement Learning via Neural Network in Driver Behavior Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.832291</td>
<td>0.965615</td>
<td><a href="https://www.semanticscholar.org/paper/deea69aadc7119bb7e0663648ef8aedbd2ffff50">1: Policy invariant explicit shaping: an efficient alternative to reward shaping</a></td>
</tr>
<tr>
<td>0</td>
<td>0.830194</td>
<td>0.973952</td>
<td><a href="https://www.semanticscholar.org/paper/f0665b8ff64fa4a02ad5f21321d88546ebbc6f94">9: Generalizing Across Multi-Objective Reward Functions in Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.829736</td>
<td>0.987248</td>
<td><a href="https://www.semanticscholar.org/paper/45f573f302dc7e77cbc5d1a74ccbac3564bbebc8">16: PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.821595</td>
<td>0.991857</td>
<td><a href="https://www.semanticscholar.org/paper/1b75dc0186ad5a8ddf0cafb0b3494d1c0a7436b9">4: Inverse Constrained Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.816493</td>
<td>0.992132</td>
<td><a href="https://www.semanticscholar.org/paper/17f23a5fdc202bd208bbc87de2a4824e6c125d5e">10: Active Task-Inference-Guided Deep Inverse Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.570750</td>
<td>0.999082</td>
<td><a href="https://www.semanticscholar.org/paper/b05b67aca720d0bc39bc9afad02a19f522c7a1bc">2380: Pharmacokinetics of a novel formulation of ivermectin after administration to goats</a></td>
</tr>
<tr>
<td>0</td>
<td>0.818231</td>
<td>0.998013</td>
<td><a href="https://www.semanticscholar.org/paper/66ac3d7d8e75a64766fc59747d580bfa6d9e4031">100: Feature Construction for Inverse Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.767236</td>
<td>0.997794</td>
<td><a href="https://www.semanticscholar.org/paper/1e045f3447f69d9a7cac18ef23062ea8dd661285">289: Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></td>
</tr>
<tr>
<td>0</td>
<td>0.789175</td>
<td>0.996963</td>
<td><a href="https://www.semanticscholar.org/paper/8c802ca0f26177d2dda7664778ab7bb3337edfb7">63: MAP Inference for Bayesian Inverse Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.846211</td>
<td>0.996888</td>
<td><a href="https://www.semanticscholar.org/paper/6553b04761e1030b95755a83337627535a372c18">257: A Game-Theoretic Approach to Apprenticeship Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.770841</td>
<td>0.996522</td>
<td><a href="https://www.semanticscholar.org/paper/9d4d8509f6da094a7c31e063f307e0e8592db27f">124: A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress</a></td>
</tr>
<tr>
<td>0</td>
<td>0.779230</td>
<td>0.996105</td>
<td><a href="https://www.semanticscholar.org/paper/11b6bdfe36c48b11367b27187da11d95892f0361">1959: Maximum Entropy Inverse Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.795620</td>
<td>0.995389</td>
<td><a href="https://www.semanticscholar.org/paper/d0f19d65ca8edfd1a044bc7e03b90a40f021e703">44: A survey of inverse reinforcement learning techniques</a></td>
</tr>
<tr>
<td>0</td>
<td>0.797664</td>
<td>0.995185</td>
<td><a href="https://www.semanticscholar.org/paper/0eb1a4c84bf6c2c96decfe53c1e9899c2fb0b7ce">24: Learning from Suboptimal Demonstration via Self-Supervised Reward Regression</a></td>
</tr>
</table></html>
