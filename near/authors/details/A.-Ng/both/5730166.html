<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/94066dc12fe31e96af7557838159bde598cb4f10">1573: Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping</a></td>
</tr>
<tr>
<td>0</td>
<td>0.822357</td>
<td>0.974035</td>
<td><a href="https://www.semanticscholar.org/paper/1eb663285d1b72060e20feeee1963a8b633e658a">4: Design and Convergence Analysis of a Heuristic Reward Function for Reinforcement Learning Algorithms</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811364</td>
<td>0.986467</td>
<td><a href="https://www.semanticscholar.org/paper/2ac5ba136f4ebea841ca0c482136ab153e028cdf">3: Average Reward Adjusted Discounted Reinforcement Learning: Near-Blackwell-Optimal Policies for Real-World Applications</a></td>
</tr>
<tr>
<td>0</td>
<td>0.807939</td>
<td>0.977981</td>
<td><a href="https://www.semanticscholar.org/paper/c3d0e3c5ca9fa56cf2ff7303a2f67bf44694e6d4">72: LTL and Beyond: Formal Languages for Reward Function Specification in Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.793060</td>
<td>0.986688</td>
<td><a href="https://www.semanticscholar.org/paper/321b217fcbd27b3c5d7283f5ce092840707e2301">29: Learning Shaping Rewards in Model-based Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.786260</td>
<td>0.894163</td>
<td><a href="https://www.semanticscholar.org/paper/2de978766da1086596788f4a3212e69a7767426e">3: State-Augmentation Transformations for Risk-Sensitive Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.779801</td>
<td>0.450936</td>
<td><a href="https://www.semanticscholar.org/paper/c83f493d9baf48307c4b1262d694c1520f3a1902">0: Reward Bases: Instantaneous reward revaluation with temporal difference learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.771695</td>
<td>0.921254</td>
<td><a href="https://www.semanticscholar.org/paper/094cd58f07e7d82084cc8d88eb447c7f0234dc35">5: Regularized Policies are Reward Robust</a></td>
</tr>
<tr>
<td>0</td>
<td>0.762757</td>
<td>0.982690</td>
<td><a href="https://www.semanticscholar.org/paper/92edf4d699a139b757e6cfb5407872af5472a758">1: Explore and Control with Adversarial Surprise</a></td>
</tr>
<tr>
<td>0</td>
<td>0.762062</td>
<td>0.960825</td>
<td><a href="https://www.semanticscholar.org/paper/6cc143037044cd3ef3a3f194ba0a1b2446854e26">30: Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective</a></td>
</tr>
<tr>
<td>0</td>
<td>0.675507</td>
<td>0.998009</td>
<td><a href="https://www.semanticscholar.org/paper/e245ef09fe9e4f56d0fa7f75257298588f4d0392">40: Agent-Agnostic Human-in-the-Loop Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.732739</td>
<td>0.997807</td>
<td><a href="https://www.semanticscholar.org/paper/8e767ae28510558344f90b0be08344d2b3769cd6">62: Reward Design via Online Gradient Ascent</a></td>
</tr>
<tr>
<td>0</td>
<td>0.785871</td>
<td>0.997698</td>
<td><a href="https://www.semanticscholar.org/paper/00ec8123dd2ba03afab7c1fa02f774062f769181">99: Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.703012</td>
<td>0.997343</td>
<td><a href="https://www.semanticscholar.org/paper/dc0adbb27267f1ae88dc5c9982ec7acb5124b95f">65: Exploration from Demonstration for Interactive Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.752543</td>
<td>0.997282</td>
<td><a href="https://www.semanticscholar.org/paper/f926d3bf410875857effe1b5000a4fbc25397b74">144: Reinforcement Learning from Demonstration through Shaping</a></td>
</tr>
<tr>
<td>0</td>
<td>0.641133</td>
<td>0.997240</td>
<td><a href="https://www.semanticscholar.org/paper/630bf4c0b7e5abd276ec38468f490b7d6222f3a2">132: Interactive Learning from Policy-Dependent Human Feedback</a></td>
</tr>
<tr>
<td>0</td>
<td>0.763086</td>
<td>0.997219</td>
<td><a href="https://www.semanticscholar.org/paper/e54152403da98a0403afef8477d42383d606e1f9">337: Off-Policy Temporal Difference Learning with Function Approximation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.746375</td>
<td>0.997126</td>
<td><a href="https://www.semanticscholar.org/paper/dfb5a933d2bfa67b9980eb35d39b0e947a434f8e">54: Expressing Arbitrary Reward Functions as Potential-Based Advice</a></td>
</tr>
<tr>
<td>0</td>
<td>0.699447</td>
<td>0.997022</td>
<td><a href="https://www.semanticscholar.org/paper/e2e8d2ae77ffaf9b526d28bd7ed4fb555e50ee24">73: Multi-step Reinforcement Learning: A Unifying Algorithm</a></td>
</tr>
</table></html>
