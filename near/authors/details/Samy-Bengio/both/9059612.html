<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/e2a85a6766b982ff7c8980e57ca6342d22493827">1852: Adversarial Machine Learning at Scale</a></td>
</tr>
<tr>
<td>1</td>
<td>0.915485</td>
<td>0.999675</td>
<td><a href="https://www.semanticscholar.org/paper/136dee73f203df2f4831994bf4f0c0a4ad2e764e">1650: Ensemble Adversarial Training: Attacks and Defenses</a></td>
</tr>
<tr>
<td>0</td>
<td>0.895631</td>
<td>0.981486</td>
<td><a href="https://www.semanticscholar.org/paper/d5577abcc1fbf57d66017e3b5b2211a82022842c">429: Generating Adversarial Examples with Adversarial Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.889124</td>
<td>0.994634</td>
<td><a href="https://www.semanticscholar.org/paper/a8cf0781b1acf3f7ea660209d5210f08d7173a85">8: Efficient Two-Step Adversarial Defense for Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.888342</td>
<td>0.995507</td>
<td><a href="https://www.semanticscholar.org/paper/b59963aa00fa2928d1dd32f9385f0790f819a8fe">7: Using Single-Step Adversarial Training to Defend Iterative Adversarial Examples</a></td>
</tr>
<tr>
<td>0</td>
<td>0.879206</td>
<td>0.996340</td>
<td><a href="https://www.semanticscholar.org/paper/428c2e5992d6ed3186c087cba0fdba2ab6a468b2">85: Improving the Generalization of Adversarial Training with Domain Adaptation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.876797</td>
<td>0.993665</td>
<td><a href="https://www.semanticscholar.org/paper/efdb28e0107a3c6142ff5b01e4fc5a974b5ea762">16: Detecting Adversarial Examples Using Data Manifolds</a></td>
</tr>
<tr>
<td>0</td>
<td>0.876447</td>
<td>0.993288</td>
<td><a href="https://www.semanticscholar.org/paper/54a970d2e08925704a06287fa64bdcab7b4cc4ed">2: Simultaneous Adversarial Training - Learn from Othersâ€™ Mistakes</a></td>
</tr>
<tr>
<td>0</td>
<td>0.872596</td>
<td>0.981860</td>
<td><a href="https://www.semanticscholar.org/paper/788199cf1c757eb1d574b60d89e7b428032a5b76">0: Defense-guided Transferable Adversarial Attacks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.872146</td>
<td>0.778511</td>
<td><a href="https://www.semanticscholar.org/paper/146f6a424ccadfbccd48ab5488f52935d4a677b9">3: Achieving Model Robustness through Discrete Adversarial Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.818643</td>
<td>0.999671</td>
<td><a href="https://www.semanticscholar.org/paper/df40ce107a71b770c9d0354b78fdd8989da80d2f">4538: Towards Evaluating the Robustness of Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.849613</td>
<td>0.999620</td>
<td><a href="https://www.semanticscholar.org/paper/b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b">3254: Adversarial examples in the physical world</a></td>
</tr>
<tr>
<td>0</td>
<td>0.671422</td>
<td>0.999275</td>
<td><a href="https://www.semanticscholar.org/paper/52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35">2913: DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.635860</td>
<td>0.999225</td>
<td><a href="https://www.semanticscholar.org/paper/651adaa058f821a890f2c5d1053d69eb481a8352">1996: Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples</a></td>
</tr>
<tr>
<td>0</td>
<td>0.779282</td>
<td>0.999101</td>
<td><a href="https://www.semanticscholar.org/paper/9fec45e1ff97ffb0e0cf9f039e39b46043430301">1054: Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.814399</td>
<td>0.999051</td>
<td><a href="https://www.semanticscholar.org/paper/405b6ff2ea2ec9a7c7d6b18ac951dc778892ffcf">594: Detecting Adversarial Samples from Artifacts</a></td>
</tr>
<tr>
<td>0</td>
<td>0.851791</td>
<td>0.998972</td>
<td><a href="https://www.semanticscholar.org/paper/531e3a7b7768f199fdd401b266504db245ca039a">685: On Detecting Adversarial Perturbations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.883417</td>
<td>0.998868</td>
<td><a href="https://www.semanticscholar.org/paper/78aa018ee7d52360e15d103390ea1cdb3a0beb41">1131: Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples</a></td>
</tr>
</table></html>
