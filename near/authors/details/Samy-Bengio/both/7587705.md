<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/aea0f946e8dcddb65cc2e907456c42453f246a50">421: Large scale image annotation: learning to rank with joint word-image embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.976657</td>
<td>0.847551</td>
<td><a href="https://www.semanticscholar.org/paper/ae5195c44ef7bff090bb5a17a9fe5f86a8c3b316">1: Web Scale Image Annotation: Learning to Rank with Joint Word-Image Embeddings</a></td>
</tr>
<tr>
<td>1</td>
<td>0.868336</td>
<td>0.962585</td>
<td><a href="https://www.semanticscholar.org/paper/51480ee8f067453c2878f0148ffcfa3a856a02dc">724: WSABIE: Scaling Up to Large Vocabulary Image Annotation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.833034</td>
<td>0.678319</td>
<td><a href="https://www.semanticscholar.org/paper/99a054369f3be3a7eef8dfca77793ba4d44e491b">2: Structured Prediction with Output Embeddings for Semantic Image Annotation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.807822</td>
<td>0.886598</td>
<td><a href="https://www.semanticscholar.org/paper/2f7c1d91821b2ce21eb736c81a00f7eddb29cfed">8: VSE-ens: Visual-Semantic Embeddings with Efficient Negative Sampling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.803062</td>
<td>0.712260</td>
<td><a href="https://www.semanticscholar.org/paper/f0f823511188d8c10b67512d23eb9cb7f3dd2f9a">11: Learning by expansion: Exploiting social media for image classification with few training examples</a></td>
</tr>
<tr>
<td>0</td>
<td>0.796039</td>
<td>0.657612</td>
<td><a href="https://www.semanticscholar.org/paper/c2e250b4b49a9aa04b68dfd40dc69b022b1f8b3d">784: The Multidimensional Wisdom of Crowds</a></td>
</tr>
<tr>
<td>0</td>
<td>0.795772</td>
<td>0.271303</td>
<td><a href="https://www.semanticscholar.org/paper/72be4e0750cf5591d527d7792aa861353526e311">3: Self-Supervised VQA: Answering Visual Questions using Images and Captions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.795561</td>
<td>0.597812</td>
<td><a href="https://www.semanticscholar.org/paper/05c605eeafcaf2aa65c7386c8609e05a5c6023c0">3: Large-Scale Zero-Shot Image Classification from Rich and Diverse Textual Descriptions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.795492</td>
<td>0.652869</td>
<td><a href="https://www.semanticscholar.org/paper/d793bcd4e2f4e8e006278bb2e82a124fe25b34b8">147: Keywords to visual categories: Multiple-instance learning forweakly supervised object categorization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.730283</td>
<td>0.958356</td>
<td><a href="https://www.semanticscholar.org/paper/772ba4e0bd4af02b393078ec84b4cb62ea60a918">40: Learning of Multimodal Representations With Random Walks on the Click Graph</a></td>
</tr>
<tr>
<td>0</td>
<td>0.613306</td>
<td>0.950269</td>
<td><a href="https://www.semanticscholar.org/paper/d0b021e886534e3602c4b826568a49167cd46d64">0: Information Enhancement for Travelogues via a Hybrid Clustering Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.745699</td>
<td>0.946893</td>
<td><a href="https://www.semanticscholar.org/paper/d511bc8d630248e9befb6deb6fe6e5e719f1e414">63: Start from Scratch: Towards Automatically Identifying, Modeling, and Naming Visual Attributes</a></td>
</tr>
<tr>
<td>0</td>
<td>0.681943</td>
<td>0.936233</td>
<td><a href="https://www.semanticscholar.org/paper/759bf1bd37909e6452e0d53aa2fa5d22806ea365">21: Textual Similarity with a Bag-of-Embedded-Words Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.700597</td>
<td>0.934158</td>
<td><a href="https://www.semanticscholar.org/paper/2f77e464414dae330df16a51971ec296ad76467e">75: Cross-media semantic representation via bi-directional learning to rank</a></td>
</tr>
<tr>
<td>0</td>
<td>0.699047</td>
<td>0.934000</td>
<td><a href="https://www.semanticscholar.org/paper/8c0bd8f06395d822d8665d016ed7b70dddc438c6">5: Image and tag retrieval by leveraging image-group links with multi-domain graph embedding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.567284</td>
<td>0.932104</td>
<td><a href="https://www.semanticscholar.org/paper/da43f625bcbaf07378938242f2d3c071b54ae506">1: Clothing Matching Based on Multi-modal Data</a></td>
</tr>
<tr>
<td>0</td>
<td>0.731232</td>
<td>0.928793</td>
<td><a href="https://www.semanticscholar.org/paper/a8d3dc5c68032c60ebbe3b547ac948d7cf8dd1d8">18: Multi-Label Zero-Shot Learning via Concept Embedding</a></td>
</tr>
</table></html>
