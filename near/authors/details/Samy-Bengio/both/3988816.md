<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/642c1b4a9da95ea4239708afc5929a5007a1870d">372: Tensor2Tensor for Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.781037</td>
<td>0.108907</td>
<td><a href="https://www.semanticscholar.org/paper/ea25d96c94cbac170ac53db9eb828ebc098bc5bb">2: Prediction of training time for deep neural networks in TensorFlow</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741807</td>
<td>-0.025439</td>
<td>NA:215849399</td>
</tr>
<tr>
<td>0</td>
<td>0.700936</td>
<td>0.224538</td>
<td><a href="https://www.semanticscholar.org/paper/69f19b6563ff54dcf9b20cd62b4d5537e7fb43c6">9: Why Does Unsupervised Pre-training Help Deep Learning?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.697344</td>
<td>0.090551</td>
<td><a href="https://www.semanticscholar.org/paper/0ec62f6d80d2ce3784149a48faae423517b9c760">6: Evaluation of Pretraining Methods for Deep Reinforcement Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.682142</td>
<td>0.013059</td>
<td><a href="https://www.semanticscholar.org/paper/beb7f696ecab1d7df35ccc15ef5f6aa13c743ea0">6: MUSCO: Multi-Stage Compression of neural networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.676967</td>
<td>-0.018289</td>
<td><a href="https://www.semanticscholar.org/paper/f97d1f6939ef2b63a8a712d442eb8586493207b8">4: One time is not enough: iterative tensor decomposition for neural network compression</a></td>
</tr>
<tr>
<td>0</td>
<td>0.675079</td>
<td>0.052798</td>
<td><a href="https://www.semanticscholar.org/paper/b8bbda9bc5e0861a64a54057af7f6a88b49498c7">19: Automated Multi-Stage Compression of Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.672979</td>
<td>-0.025439</td>
<td>NA:215932669</td>
</tr>
<tr>
<td>0</td>
<td>0.667019</td>
<td>-0.025439</td>
<td>NA:213215531</td>
</tr>
<tr>
<td>0</td>
<td>0.600452</td>
<td>0.991292</td>
<td><a href="https://www.semanticscholar.org/paper/528b5f5356bc7ad91edc4dc074b0273e1e55fb03">59: Modeling Recurrence for Transformer</a></td>
</tr>
<tr>
<td>0</td>
<td>0.550622</td>
<td>0.990907</td>
<td><a href="https://www.semanticscholar.org/paper/6e45251b16cd423f3c025f004959c6d2b26efab0">96: Accelerating Neural Transformer via an Average Attention Network</a></td>
</tr>
<tr>
<td>0</td>
<td>0.497485</td>
<td>0.990467</td>
<td><a href="https://www.semanticscholar.org/paper/fdbdd4e0461d23905104460a02a176907d945f44">97: Multi-Head Attention with Disagreement Regularization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.517939</td>
<td>0.989888</td>
<td><a href="https://www.semanticscholar.org/paper/bb669de2fce407df2f5cb2f8c51dedee3f467e04">323: The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.572058</td>
<td>0.989773</td>
<td><a href="https://www.semanticscholar.org/paper/15e81c8d1c21f9e928c72721ac46d458f3341454">310: Non-Autoregressive Neural Machine Translation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.565467</td>
<td>0.989641</td>
<td><a href="https://www.semanticscholar.org/paper/9c5c89199114858eafbe50b46d77d38ffd03b28a">283: Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement</a></td>
</tr>
<tr>
<td>0</td>
<td>0.510870</td>
<td>0.989475</td>
<td><a href="https://www.semanticscholar.org/paper/bdc046e65bc80cf13929ca0c3934d6faee830723">65: Convolutional Self-Attention Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.554166</td>
<td>0.989170</td>
<td><a href="https://www.semanticscholar.org/paper/2ca5071b8fa8cb0d23ae2a8044988f302d6642e9">3: Analyzing Word Translation of Transformer Layers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.375990</td>
<td>0.989161</td>
<td><a href="https://www.semanticscholar.org/paper/1af138dc72fa855cc3bc9c0b83750b461c26e29d">112: Modeling Localness for Self-Attention Networks</a></td>
</tr>
</table></html>
