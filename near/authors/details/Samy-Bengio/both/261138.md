<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/4aa4069693bee00d1b0759ca3df35e59284e9845">1932: DeViSE: A Deep Visual-Semantic Embedding Model</a></td>
</tr>
<tr>
<td>0</td>
<td>0.871956</td>
<td>0.860844</td>
<td><a href="https://www.semanticscholar.org/paper/730d0a7ddc35a081c5fa7beeef9aaea96c5deecd">0: Semantic Web and Zero-Shot Learning of Large Scale Visual Classes</a></td>
</tr>
<tr>
<td>0</td>
<td>0.853636</td>
<td>0.889578</td>
<td><a href="https://www.semanticscholar.org/paper/7198f506f09675803163bf3485edfadb89f577da">10: Open-World Visual Recognition Using Knowledge Graphs</a></td>
</tr>
<tr>
<td>0</td>
<td>0.830158</td>
<td>0.859672</td>
<td><a href="https://www.semanticscholar.org/paper/8f6f20c48a9e99fac227cb58570f6285cab8b730">28: A Large-Scale Attribute Dataset for Zero-Shot Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.828537</td>
<td>0.668218</td>
<td><a href="https://www.semanticscholar.org/paper/eecd9a070ed333077a066bfdcf776c51c2c74406">1: Deep image representations using caption generators</a></td>
</tr>
<tr>
<td>0</td>
<td>0.827460</td>
<td>-0.026296</td>
<td>NA:206596207</td>
</tr>
<tr>
<td>0</td>
<td>0.827459</td>
<td>0.924968</td>
<td><a href="https://www.semanticscholar.org/paper/36bfe217afafd8bccce3e8d2bd23c4e4018a98bc">95: Zero-Shot Recognition Using Dual Visual-Semantic Mapping Paths</a></td>
</tr>
<tr>
<td>0</td>
<td>0.825795</td>
<td>0.163693</td>
<td><a href="https://www.semanticscholar.org/paper/09e0ff1dc82b0d57b9e2dab6fae0c53164d896a5">0: Synthetic training data for deep neural networks on visual correspondence tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.825632</td>
<td>0.660125</td>
<td><a href="https://www.semanticscholar.org/paper/456b955f87bb58f1961bd9f222481d49d8c27cff">0: Learning Representations with Strong Supervision for Image Search</a></td>
</tr>
<tr>
<td>0</td>
<td>0.818349</td>
<td>0.809867</td>
<td><a href="https://www.semanticscholar.org/paper/657ec6e3d6332e41a2daa2352ecf2bda0bdb8038">0: On Guiding Visual Attention with Language Specification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.853380</td>
<td>0.966668</td>
<td><a href="https://www.semanticscholar.org/paper/6540cb7971d1a9d72562d465172e010fbb729bc3">337: Predicting Deep Zero-Shot Convolutional Neural Networks Using Textual Descriptions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.757790</td>
<td>0.964362</td>
<td><a href="https://www.semanticscholar.org/paper/7541e6e97c5d94184922f7db5b7fe185063941ef">33: A Shared Multi-Attention Framework for Multi-Label Zero-Shot Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.842120</td>
<td>0.964068</td>
<td><a href="https://www.semanticscholar.org/paper/877972274fc47f9993acadaaec4e3956e124212a">58: Zero-shot Image Tagging by Hierarchical Semantic Embedding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.853119</td>
<td>0.958789</td>
<td><a href="https://www.semanticscholar.org/paper/755e9f43ce398ae8737366720c5f82685b0c253e">1161: Zero-Shot Learning Through Cross-Modal Transfer</a></td>
</tr>
<tr>
<td>0</td>
<td>0.839800</td>
<td>0.957168</td>
<td><a href="https://www.semanticscholar.org/paper/be2f5d8a7e6b415f1e22cee7dfd9be56b1afd8be">769: Zero-Shot Learning by Convex Combination of Semantic Embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.823075</td>
<td>0.956607</td>
<td><a href="https://www.semanticscholar.org/paper/68def1260e8040e6ff139e27ec101fc4911b8e04">4: Enhancing Visual Embeddings through Weakly Supervised Captioning for Zero-Shot Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.764685</td>
<td>0.956347</td>
<td><a href="https://www.semanticscholar.org/paper/ffbff2edb9994ceac5d7b08d0049424974d20eae">4: CNN Region featuresInput image Guided loss SGA SGA Class semantic features Semantic embedding label Attention features Attention features Embedded loss Softm</a></td>
</tr>
<tr>
<td>0</td>
<td>0.695834</td>
<td>0.953050</td>
<td><a href="https://www.semanticscholar.org/paper/fdadf23925884b3cfd9f3f72e48136b8fcf4276b">0: Recognizing Bengali Word Images - A Zero-Shot Learning Perspective</a></td>
</tr>
<tr>
<td>0</td>
<td>0.791667</td>
<td>0.952965</td>
<td><a href="https://www.semanticscholar.org/paper/0ea46e70da5e0882ac6e08afd8a9f6285abfb12a">4: HUSE: Hierarchical Universal Semantic Embeddings</a></td>
</tr>
</table></html>
