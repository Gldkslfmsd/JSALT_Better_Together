<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/8f18c9da3d1763723c6ef8c3734d74db005d0cff">222: Fantastic Generalization Measures and Where to Find Them</a></td>
</tr>
<tr>
<td>0</td>
<td>0.975427</td>
<td>0.966619</td>
<td><a href="https://www.semanticscholar.org/paper/fd2f488c73e1ac7a1a3ac87b6cffce6e1099bee0">1: FANTASTIC GENERALIZATION MEASURES</a></td>
</tr>
<tr>
<td>0</td>
<td>0.806148</td>
<td>0.977088</td>
<td><a href="https://www.semanticscholar.org/paper/e837dfa120e8ce3cd587bde7b0787ef43fa7832d">286: Sensitivity and Generalization in Neural Networks: an Empirical Study</a></td>
</tr>
<tr>
<td>0</td>
<td>0.758565</td>
<td>0.947647</td>
<td><a href="https://www.semanticscholar.org/paper/30f47601cd2e00d5892927344873bb5173df23a6">4: A Functional Characterization of Randomly Initialized Gradient Descent in Deep ReLU Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.757947</td>
<td>0.020696</td>
<td><a href="https://www.semanticscholar.org/paper/c5b3c22d2a6882adb5b10d4a9cbefc11f114d039">9: SepNE: Bringing Separability to Network Embedding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.757275</td>
<td>0.861335</td>
<td><a href="https://www.semanticscholar.org/paper/4873c78f0cd5a1fad96300e49e196af75800a24e">115: Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.750590</td>
<td>0.772283</td>
<td><a href="https://www.semanticscholar.org/paper/c7784864058e828d29223fb0825de8d82f4cf7e7">0: ENTROPY PENALTY: TOWARDS GENERALIZATION BE-</a></td>
</tr>
<tr>
<td>0</td>
<td>0.747210</td>
<td>0.232903</td>
<td><a href="https://www.semanticscholar.org/paper/e71cd818b64ad5db39010969603a107b794fdcff">5: On Deep Set Learning and the Choice of Aggregations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.743256</td>
<td>0.685455</td>
<td><a href="https://www.semanticscholar.org/paper/309eff82da3d88bc8d93db217242e4897dc00772">0: Understanding and Discussing Weaknesses in Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.742556</td>
<td>0.329173</td>
<td><a href="https://www.semanticscholar.org/paper/2db01b9466e4251a2140fc2ef9d216417b48cbe4">5: Does Data Augmentation Improve Generalization in NLP</a></td>
</tr>
<tr>
<td>0</td>
<td>0.739703</td>
<td>0.994433</td>
<td><a href="https://www.semanticscholar.org/paper/4e431827bf1ed1d8c435c01e75b12c79ba968721">106: SGD on Neural Networks Learns Functions of Increasing Complexity</a></td>
</tr>
<tr>
<td>0</td>
<td>0.744256</td>
<td>0.989989</td>
<td><a href="https://www.semanticscholar.org/paper/38693fd784453f8819781aec59eb74e3abb1571f">26: Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.708431</td>
<td>0.986850</td>
<td><a href="https://www.semanticscholar.org/paper/29090beb90c184a9aaf7aa610bfed5ee1631d2f2">171: Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.749088</td>
<td>0.986230</td>
<td><a href="https://www.semanticscholar.org/paper/a6585a04a10febf57b978d9c4c0329929f7b8139">12: Towards Task and Architecture-Independent Generalization Gap Predictors</a></td>
</tr>
<tr>
<td>0</td>
<td>0.678689</td>
<td>0.985367</td>
<td><a href="https://www.semanticscholar.org/paper/3ce2f7a1f857725ca3f477967aeef96ccf4eb0c4">18: Catastrophic Fisher Explosion: Early Phase Fisher Matrix Impacts Generalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.704117</td>
<td>0.984740</td>
<td><a href="https://www.semanticscholar.org/paper/0204871837acb118871e8d1bb59407da73142333">68: Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience</a></td>
</tr>
<tr>
<td>0</td>
<td>0.802925</td>
<td>0.984112</td>
<td><a href="https://www.semanticscholar.org/paper/c11f7f8444e1f47d8824ba226b3ee8564e083366">23: Truth or Backpropaganda? An Empirical Investigation of Deep Learning Theory</a></td>
</tr>
<tr>
<td>0</td>
<td>0.630736</td>
<td>0.984081</td>
<td><a href="https://www.semanticscholar.org/paper/f081ed289b31e1f2f471ffacd6fccae9e3dba481">60: Stiffness: A New Perspective on Generalization in Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.729770</td>
<td>0.983784</td>
<td><a href="https://www.semanticscholar.org/paper/d53fb3feeeab07a0d70bf466dd473ec6052ecc07">739: Exploring Generalization in Deep Learning</a></td>
</tr>
</table></html>
