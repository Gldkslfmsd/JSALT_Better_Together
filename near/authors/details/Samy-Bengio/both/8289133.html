<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e">615: Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge</a></td>
</tr>
<tr>
<td>0</td>
<td>0.935127</td>
<td>0.986099</td>
<td><a href="https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0">4457: Show and tell: A neural image caption generator</a></td>
</tr>
<tr>
<td>0</td>
<td>0.849470</td>
<td>0.931725</td>
<td><a href="https://www.semanticscholar.org/paper/c2ce1912727a3e8c88b4af65c9ca088b3c8eb1a0">0: Vision and Language Learning: From Image Captioning and Visual Question Answering towards Embodied Agents</a></td>
</tr>
<tr>
<td>0</td>
<td>0.847211</td>
<td>0.704656</td>
<td><a href="https://www.semanticscholar.org/paper/0c44883e2a58f3067ae134405fd6d60dbc8ee348">0: Learning Cross-modal Representations with Multi-relations for Image Captioning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.845060</td>
<td>0.951019</td>
<td><a href="https://www.semanticscholar.org/paper/b5bfe824fc49fe78b538ac15f21c4cd6a9d44347">4: Image Caption with Synchronous Cross-Attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.824037</td>
<td>-0.029621</td>
<td><a href="https://www.semanticscholar.org/paper/50f94ffc8b294e0e54f35448af038db42a4e0e39">0: Image Caption Generation using Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.819798</td>
<td>0.932761</td>
<td><a href="https://www.semanticscholar.org/paper/0c014c19b68a781ccd6e26fcc7c47ba9b1cf020f">13: Boosting Video Description Generation by Explicitly Translating from Frame-Level Captions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.815927</td>
<td>-0.029621</td>
<td><a href="https://www.semanticscholar.org/paper/b33d4029bd232f9a05a36094c22c13fb76bc93b4">0: An Image Captioner for the Visually Challenged</a></td>
</tr>
<tr>
<td>0</td>
<td>0.813934</td>
<td>0.969420</td>
<td><a href="https://www.semanticscholar.org/paper/2df61fcd01b6a70a94dff2b25d6ed8dc4c16e422">3: The role of image representations in vision to language tasks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.812957</td>
<td>0.942803</td>
<td><a href="https://www.semanticscholar.org/paper/daad11aee75bcf597602e654c33a12de61343dda">4: Image-to-Tree: A Tree-Structured Decoder for Image Captioning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.825835</td>
<td>0.995006</td>
<td><a href="https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88">2464: Deep Visual-Semantic Alignments for Generating Image Descriptions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.754704</td>
<td>0.991157</td>
<td><a href="https://www.semanticscholar.org/paper/7e27d44e3fac723ccb703e0a83b22711bd42efe8">292: A Comprehensive Survey of Deep Learning for Image Captioning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.834063</td>
<td>0.991021</td>
<td><a href="https://www.semanticscholar.org/paper/15f102c3c9f4d4fe6ba105e221df48c6e8902b3b">1109: From captions to visual concepts and back</a></td>
</tr>
<tr>
<td>0</td>
<td>0.836215</td>
<td>0.990967</td>
<td><a href="https://www.semanticscholar.org/paper/bf55591e09b58ea9ce8d66110d6d3000ee804bdd">1203: Image Captioning with Semantic Attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.797381</td>
<td>0.989479</td>
<td><a href="https://www.semanticscholar.org/paper/d85704f4814e9fa5ff0b68b1e5cad9e6527d0bbf">85: An Empirical Study of Language CNN for Image Captioning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.748427</td>
<td>0.989370</td>
<td><a href="https://www.semanticscholar.org/paper/7d1a7dae43b630d61d19d6cf139824380f2cf42f">95: Image Caption with Global-Local Attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.820180</td>
<td>0.988658</td>
<td><a href="https://www.semanticscholar.org/paper/e516d22697bad6d0f7956b0e8bfa93d6eb0b2f17">230: Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data</a></td>
</tr>
<tr>
<td>0</td>
<td>0.825468</td>
<td>0.988294</td>
<td><a href="https://www.semanticscholar.org/paper/ae9850ce1ba187dc5f9e5ab0da381d8a551c1fc0">170: Attention Correctness in Neural Image Captioning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.784344</td>
<td>0.988186</td>
<td><a href="https://www.semanticscholar.org/paper/54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745">1001: Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)</a></td>
</tr>
</table></html>
