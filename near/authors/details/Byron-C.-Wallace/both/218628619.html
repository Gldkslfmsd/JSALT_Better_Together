<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/0696ad8beb0d765973aa5cdbc6e118889d3583b0">54: Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.803914</td>
<td>0.496057</td>
<td><a href="https://www.semanticscholar.org/paper/32f36170b44cbd3e644c39a4ea37aa40843f2b35">1: Towards a Joint Approach to Produce Decisions and Explanations Using CNNs</a></td>
</tr>
<tr>
<td>0</td>
<td>0.803005</td>
<td>0.494115</td>
<td><a href="https://www.semanticscholar.org/paper/cd64ad18e1dbba780e8c8b02d9ed7bc7990a9add">0: Towards Better Visual Explanations for Deep Image Classifiers</a></td>
</tr>
<tr>
<td>0</td>
<td>0.800219</td>
<td>0.904441</td>
<td><a href="https://www.semanticscholar.org/paper/2d88b671af49e477e3a0e85014fb853b6d3bd363">16: On the Validity of Self-Attention as Explanation in Transformer Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.798316</td>
<td>0.934764</td>
<td><a href="https://www.semanticscholar.org/paper/1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f">603: Attention is not Explanation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.789252</td>
<td>0.959742</td>
<td><a href="https://www.semanticscholar.org/paper/5b8db46bb3770684ced9922a49c496d467e7eec2">6: Explaining Neural Network Predictions on Sentence Pairs via Learning Word-Group Masks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.789007</td>
<td>0.565234</td>
<td><a href="https://www.semanticscholar.org/paper/712d211cc66e1ac00076bf331c9bd9e3ab59e2ad">31: Generating Contrastive Explanations with Monotonic Attribute Functions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.788421</td>
<td>0.550291</td>
<td><a href="https://www.semanticscholar.org/paper/ca29d8b1db049080d4ae8ec66dac437cbc99cbed">15: Towards Global Explanations of Convolutional Neural Networks With Concept Attribution</a></td>
</tr>
<tr>
<td>0</td>
<td>0.784978</td>
<td>0.943794</td>
<td><a href="https://www.semanticscholar.org/paper/ce177672b00ddf46e4906157a7e997ca9338b8b9">407: Attention is not not Explanation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.784322</td>
<td>0.591642</td>
<td><a href="https://www.semanticscholar.org/paper/28c09b905d0b33f54370f1f4035fac91b3533c1e">10: FLEX: Faithful Linguistic Explanations for Neural Net Based Model Decisions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.736858</td>
<td>0.992367</td>
<td><a href="https://www.semanticscholar.org/paper/c7d8b6180b52f9be891fec1223b7f561770b0206">1: Thermostat: A Large Collection of NLP Model Explanations and Analysis Tools</a></td>
</tr>
<tr>
<td>0</td>
<td>0.699615</td>
<td>0.992343</td>
<td><a href="https://www.semanticscholar.org/paper/898b14509593d235414df054527b7702e35c3099">11: Post-hoc Interpretability for Neural NLP: A Survey</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741034</td>
<td>0.991824</td>
<td><a href="https://www.semanticscholar.org/paper/19377dc1b26157db9e63d1938c4f21240fb0f5ca">5: Evaluating Explanations for Reading Comprehension with Realistic Counterfactuals</a></td>
</tr>
<tr>
<td>0</td>
<td>0.746238</td>
<td>0.991809</td>
<td><a href="https://www.semanticscholar.org/paper/044ebe003405eae1715f729c80546210be05363d">27: Interpretation of NLP Models through Input Marginalization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.801415</td>
<td>0.991613</td>
<td><a href="https://www.semanticscholar.org/paper/ddd27dba038d0ed14c48cd027812df58a902ece2">69: AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models</a></td>
</tr>
<tr>
<td>0</td>
<td>0.646627</td>
<td>0.990691</td>
<td><a href="https://www.semanticscholar.org/paper/2232808cf3161ca4c434126e35f47ee33c0c8219">40: Evaluating Explanations: How Much Do Explanations from the Teacher Aid Students?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.766867</td>
<td>0.990056</td>
<td><a href="https://www.semanticscholar.org/paper/1b5f117513da0e22b1c1282e831303938650919d">5: On Sample Based Explanation Methods for NLP: Faithfulness, Efficiency and Semantic Evaluation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.695639</td>
<td>0.990013</td>
<td><a href="https://www.semanticscholar.org/paper/2a456fd1d47a396feef9a1a2cf140a71bbc78ad4">4: Connecting Attributions and QA Model Behavior on Realistic Counterfactuals</a></td>
</tr>
<tr>
<td>0</td>
<td>0.684878</td>
<td>0.988465</td>
<td><a href="https://www.semanticscholar.org/paper/5c599dc162bfd33abf390ba00474453b54ddf60f">13: Contrastive Explanations for Model Interpretability</a></td>
</tr>
</table></html>
