<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/03e04983f7ce6a9c2b42948840b3312aea33f9f3">492: On the Expressive Power of Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.841421</td>
<td>0.903127</td>
<td><a href="https://www.semanticscholar.org/paper/0ae7047bf0973308b3553774e5624ff054057842">4: Investigating the Compositional Structure Of Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.838255</td>
<td>0.893924</td>
<td><a href="https://www.semanticscholar.org/paper/9e87f8cfbd3db53ef4b0cb94b8c84f0b9c659c41">4: O N THE EXPRESSIVE POWER OF DEEP NEURAL NETWORKS</a></td>
</tr>
<tr>
<td>0</td>
<td>0.806006</td>
<td>0.655839</td>
<td><a href="https://www.semanticscholar.org/paper/9f544e0ed6848638036214bd88af40eafe8bbc0c">0: Functionally weighted neural networks: frugal models with high accuracy</a></td>
</tr>
<tr>
<td>0</td>
<td>0.802243</td>
<td>0.856648</td>
<td><a href="https://www.semanticscholar.org/paper/2484ebfa2053999b4481ab2cd979eccb47cb0321">52: Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology</a></td>
</tr>
<tr>
<td>0</td>
<td>0.799413</td>
<td>0.624071</td>
<td><a href="https://www.semanticscholar.org/paper/2adb616a77fe28b49be2a2d66cccf2d7400e4a04">322: Data-Driven Sparse Structure Selection for Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.798988</td>
<td>0.767604</td>
<td><a href="https://www.semanticscholar.org/paper/478936ccd824405f35fcb7540c51b8c66540ed9d">5: MaxGain: Regularisation of Neural Networks by Constraining Activation Magnitudes</a></td>
</tr>
<tr>
<td>0</td>
<td>0.788809</td>
<td>0.920862</td>
<td><a href="https://www.semanticscholar.org/paper/d94dcffeff6f68c3c15c49e556cbf2f9edb2f870">0: Using Summary Layers to Probe Neural Network Behaviour</a></td>
</tr>
<tr>
<td>0</td>
<td>0.787558</td>
<td>0.820746</td>
<td><a href="https://www.semanticscholar.org/paper/83b3f1ed9906e73d64e5574a394b7fb3d7cd54a9">0: Reinforcement learning of conditional computation policies for neural networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.785598</td>
<td>0.907536</td>
<td><a href="https://www.semanticscholar.org/paper/0067b52b8e2211c7a2a4b397a1eb4491b8c259b1">0: On the Effective Number of Linear Regions in Shallow Univariate ReLU Networks: Convergence Guarantees and Implicit Bias</a></td>
</tr>
<tr>
<td>0</td>
<td>0.704476</td>
<td>0.992089</td>
<td><a href="https://www.semanticscholar.org/paper/6e997fec1412abb4b630d0e6d4df95813a01e093">406: Exponential expressivity in deep neural networks through transient chaos</a></td>
</tr>
<tr>
<td>0</td>
<td>0.804511</td>
<td>0.984651</td>
<td><a href="https://www.semanticscholar.org/paper/b034b5769ab94acf9fb8ae48c7edb560a300bb63">911: On the Number of Linear Regions of Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.734960</td>
<td>0.983172</td>
<td><a href="https://www.semanticscholar.org/paper/9375729d21a344a5ccccd5f53556ddf90b957cd9">387: Understanding Deep Neural Networks with Rectified Linear Units</a></td>
</tr>
<tr>
<td>0</td>
<td>0.698625</td>
<td>0.981763</td>
<td><a href="https://www.semanticscholar.org/paper/11db042ed2264f3ea1b8f20151adf725ec3461e8">16: Stuck in a What? Adventures in Weight Space</a></td>
</tr>
<tr>
<td>0</td>
<td>0.710145</td>
<td>0.980496</td>
<td><a href="https://www.semanticscholar.org/paper/4d4d09ae8f6a11547441f7fee36405758102a801">354: Qualitatively characterizing neural network optimization problems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.685590</td>
<td>0.979856</td>
<td><a href="https://www.semanticscholar.org/paper/ad8a12a19e74d9788f8fe92f5c0dfea7b6a52aba">946: The Loss Surfaces of Multilayer Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.715427</td>
<td>0.978190</td>
<td><a href="https://www.semanticscholar.org/paper/641c7c0f5d1fa355bf95fd49cee19409bbadfc45">65: Theory II: Landscape of the Empirical Risk in Deep Learning</a></td>
</tr>
<tr>
<td>0</td>
<td>0.747996</td>
<td>0.977365</td>
<td><a href="https://www.semanticscholar.org/paper/2ecc5d410a6c163cba41c555ceb16cbdf037639b">1: Using activation histograms to bound the number of affine regions in ReLU feed-forward neural networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.654883</td>
<td>0.975910</td>
<td><a href="https://www.semanticscholar.org/paper/4206c84525a7904df3613b843491c0ae6a5507eb">436: Benefits of Depth in Neural Networks</a></td>
</tr>
</table></html>
