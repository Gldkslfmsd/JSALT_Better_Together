<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/d2e4147eecae6f914e9e1e9aece8fdd2eaed809f">1251: Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.869189</td>
<td>0.983955</td>
<td><a href="https://www.semanticscholar.org/paper/1661b64a5ae2a0a5da882ec89b1dd7a0306060d3">80: Balanced Quantization: An Effective and Efficient Approach to Quantized Neural Networks</a></td>
</tr>
<tr>
<td>1</td>
<td>0.864369</td>
<td>0.995039</td>
<td><a href="https://www.semanticscholar.org/paper/6eecc808d4c74e7d0d7ef6b8a4112c985ced104d">1696: Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1</a></td>
</tr>
<tr>
<td>0</td>
<td>0.844152</td>
<td>0.867812</td>
<td><a href="https://www.semanticscholar.org/paper/a062e4a3d05849f45c63fa386ef14f0c86b98f5d">1: Sub-bit Neural Networks: Learning to Compress and Accelerate Binary Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.836989</td>
<td>0.951416</td>
<td><a href="https://www.semanticscholar.org/paper/86284d298539db8dc499a99dcd2b633b035dc713">103: Quantization Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.834551</td>
<td>0.942902</td>
<td><a href="https://www.semanticscholar.org/paper/c94965b8d36ae6c71a6d29a100530e05cc79f19b">11: ADaPTION: Toolbox and Benchmark for Training Convolutional Neural Networks with Reduced Numerical Precision Weights and Activation</a></td>
</tr>
<tr>
<td>0</td>
<td>0.823250</td>
<td>0.945981</td>
<td><a href="https://www.semanticscholar.org/paper/24338d38e35bf346257cf591474ed20205b28db7">0: Direct Quantization for Training Highly Accurate Low Bit-width Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.821561</td>
<td>0.981070</td>
<td><a href="https://www.semanticscholar.org/paper/c4b46673466290f68333c9753a7388db421ce370">24: Learning Low Precision Deep Neural Networks through Regularization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.816619</td>
<td>0.777886</td>
<td><a href="https://www.semanticscholar.org/paper/ade10fb5025da934edc963fe0dfa59e67229a506">0: Log-quantization on GRU networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.815895</td>
<td>0.912321</td>
<td><a href="https://www.semanticscholar.org/paper/d7995d8b306ea4da3166cd123984213af97cb04c">0: LG-LSQ: Learned Gradient Linear Symmetric Quantization</a></td>
</tr>
<tr>
<td>0</td>
<td>0.828438</td>
<td>0.995030</td>
<td><a href="https://www.semanticscholar.org/paper/28135fd3e80dda50a673cd556f10b9b972005d27">1236: Binarized Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.799786</td>
<td>0.994249</td>
<td><a href="https://www.semanticscholar.org/paper/a5733ff08daff727af834345b9cfff1d0aa109ec">2076: BinaryConnect: Training Deep Neural Networks with binary weights during propagations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.811780</td>
<td>0.992500</td>
<td><a href="https://www.semanticscholar.org/paper/6e5f2c7d1092d733ae85c269bf12e47755b90368">115: A Survey on Methods and Theories of Quantized Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.762654</td>
<td>0.992494</td>
<td><a href="https://www.semanticscholar.org/paper/710bcef2c7c2e6a1ba455c136cb0aaa5580fb8e5">239: Model Compression and Acceleration for Deep Neural Networks: The Principles, Progress, and Challenges</a></td>
</tr>
<tr>
<td>0</td>
<td>0.749590</td>
<td>0.992398</td>
<td><a href="https://www.semanticscholar.org/paper/0868e159a8bc83d0b69271462ab2c3195a8fcdc9">16: Full Deep Neural Network Training On A Pruned Weight Budget</a></td>
</tr>
<tr>
<td>0</td>
<td>0.724863</td>
<td>0.992090</td>
<td><a href="https://www.semanticscholar.org/paper/aee7d6df11320512618b48199f1dbe580bbfd3a3">11: DropBack: Continuous Pruning During Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.771489</td>
<td>0.991013</td>
<td><a href="https://www.semanticscholar.org/paper/bc68c55e53269cbafb4da7d91e8943530ab63245">5: SinReQ: Generalized Sinusoidal Regularization for Low-Bitwidth Deep Quantized Training</a></td>
</tr>
<tr>
<td>0</td>
<td>0.815550</td>
<td>0.990009</td>
<td><a href="https://www.semanticscholar.org/paper/c3cb27f9ef7176658f37b607e75cc2c37f5e0ea8">76: Accurate and Efficient 2-bit Quantized Neural Networks</a></td>
</tr>
</table></html>
