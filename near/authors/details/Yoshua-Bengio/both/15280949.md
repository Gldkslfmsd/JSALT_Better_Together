<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/204a4a70428f3938d2c538a4d74c7ae0416306d8">1473: A Structured Self-attentive Sentence Embedding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.818317</td>
<td>0.965641</td>
<td><a href="https://www.semanticscholar.org/paper/5f38a07add820c82a2b13a17d891f08cdcaef500">55: Enhancing Sentence Embedding with Generalized Pooling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.806761</td>
<td>0.532432</td>
<td><a href="https://www.semanticscholar.org/paper/0e04a7488984db1842c7486a0b04c3221766fed5">1: A Self-Weighting Module to Improve Sentiment Analysis</a></td>
</tr>
<tr>
<td>0</td>
<td>0.797183</td>
<td>0.946243</td>
<td><a href="https://www.semanticscholar.org/paper/3f08a914f90737ce4fcae70265c54a9dfdc649ac">32: Importance of Self-Attention for Sentiment Analysis</a></td>
</tr>
<tr>
<td>0</td>
<td>0.796433</td>
<td>0.756540</td>
<td><a href="https://www.semanticscholar.org/paper/cbf7482e468f2fe45371b4a05e260fa24dadbf9b">0: Meta-Embedding Sentence Representation for Textual Similarity</a></td>
</tr>
<tr>
<td>0</td>
<td>0.791485</td>
<td>0.929869</td>
<td><a href="https://www.semanticscholar.org/paper/a45b4247219ec3ca8297b7617f4b299639c3937a">0: Generalize Sentence Representation with Self-Inference</a></td>
</tr>
<tr>
<td>0</td>
<td>0.772990</td>
<td>0.808181</td>
<td><a href="https://www.semanticscholar.org/paper/8d4a717da36b90b089875f2dd5d91fde938b0478">7: In Search for Linear Relations in Sentence Embedding Spaces</a></td>
</tr>
<tr>
<td>0</td>
<td>0.772644</td>
<td>0.714172</td>
<td><a href="https://www.semanticscholar.org/paper/7511c899c14db49afd00d652edc2c9d00e61d2fb">1: Analyzing the compositional properties of word embeddings</a></td>
</tr>
<tr>
<td>0</td>
<td>0.770663</td>
<td>0.915720</td>
<td><a href="https://www.semanticscholar.org/paper/15c4ccd0f90334253ab1d239c595707538970206">20: Replicate, Walk, and Stop on Syntax: An Effective Neural Network Model for Aspect-Level Sentiment Classification</a></td>
</tr>
<tr>
<td>0</td>
<td>0.769440</td>
<td>0.767571</td>
<td><a href="https://www.semanticscholar.org/paper/6be2a7322e1c5580444461e9eb7c1e4698dd07f7">6: Single Training Dimension Selection for Word Embedding with PCA</a></td>
</tr>
<tr>
<td>0</td>
<td>0.684449</td>
<td>0.991227</td>
<td><a href="https://www.semanticscholar.org/paper/adc276e6eae7051a027a4c269fb21dae43cadfed">499: DiSAN: Directional Self-Attention Network for RNN/CNN-free Language Understanding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.693610</td>
<td>0.987076</td>
<td><a href="https://www.semanticscholar.org/paper/6ed376a26045ff0048ec2b216785d396960d6ed1">237: Deep Semantic Role Labeling with Self-Attention</a></td>
</tr>
<tr>
<td>0</td>
<td>0.700041</td>
<td>0.983810</td>
<td><a href="https://www.semanticscholar.org/paper/ab456c1ed181c5c48a34adb61395d4806a0ba949">93: Attention in Natural Language Processing</a></td>
</tr>
<tr>
<td>0</td>
<td>0.605895</td>
<td>0.983391</td>
<td><a href="https://www.semanticscholar.org/paper/0ef460c47377c3b9482d8177cbcafad1730a91a5">123: Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.661124</td>
<td>0.981455</td>
<td><a href="https://www.semanticscholar.org/paper/13fe71da009484f240c46f14d9330e932f8de210">750: Long Short-Term Memory-Networks for Machine Reading</a></td>
</tr>
<tr>
<td>0</td>
<td>0.658547</td>
<td>0.977240</td>
<td><a href="https://www.semanticscholar.org/paper/3a7895b17db0cda7bbf86bcda52c46a3e03b6ded">217: DialogueRNN: An Attentive RNN for Emotion Detection in Conversations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.741203</td>
<td>0.976374</td>
<td><a href="https://www.semanticscholar.org/paper/b6b6dd01e4f844249963cc9b6b7a1cac4025484f">1: A Unified Model for Reverse Dictionary and Definition Modelling</a></td>
</tr>
<tr>
<td>0</td>
<td>0.602981</td>
<td>0.975726</td>
<td><a href="https://www.semanticscholar.org/paper/7ade3b90e440b946d6ade5d36249341c2f6c28b4">49: Information Aggregation via Dynamic Routing for Sequence Encoding</a></td>
</tr>
<tr>
<td>0</td>
<td>0.705015</td>
<td>0.974609</td>
<td><a href="https://www.semanticscholar.org/paper/f137b1dd1b5b90cd89b20b07db52a8834c8aaee1">7: Sentiment Injected Iteratively Co-Interactive Network for Spoken Language Understanding</a></td>
</tr>
</table></html>
