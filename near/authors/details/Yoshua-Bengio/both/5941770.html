<html><table><tr>
<th>Both</th>
<th>Specter</th>
<th>Proposed</th>
<th>Paper</th>
</tr>
<tr>
<td>1</td>
<td>1.000000</td>
<td>1.000000</td>
<td><a href="https://www.semanticscholar.org/paper/b034b5769ab94acf9fb8ae48c7edb560a300bb63">911: On the Number of Linear Regions of Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.883534</td>
<td>0.930030</td>
<td><a href="https://www.semanticscholar.org/paper/fe700c227b5b5873df11708fc21ef063716002a9">112: Complexity of Linear Regions in Deep Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.856243</td>
<td>0.956345</td>
<td><a href="https://www.semanticscholar.org/paper/cb8e423d46c35e8af2e57d1609b6e4ed0be2463f">139: Bounding and Counting Linear Regions of Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.803137</td>
<td>0.949943</td>
<td><a href="https://www.semanticscholar.org/paper/47d540eaa09d5f060b620146a319b5556805e08e">30: Why Deep Neural Networks?</a></td>
</tr>
<tr>
<td>0</td>
<td>0.789831</td>
<td>0.883667</td>
<td><a href="https://www.semanticscholar.org/paper/9f9fc406c76255fec51a6196ce167c0ff1d1efc0">528: Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent</a></td>
</tr>
<tr>
<td>0</td>
<td>0.784781</td>
<td>0.910805</td>
<td><a href="https://www.semanticscholar.org/paper/bcaacc1777a95236a421f703ec10d69006e1bf1d">0: F UNCTIONAL VS . PARAMETRIC EQUIVALENCE OF R E LU NETWORKS</a></td>
</tr>
<tr>
<td>0</td>
<td>0.780714</td>
<td>0.926768</td>
<td><a href="https://www.semanticscholar.org/paper/5e2e307ae3abad12a97fc858cdbee3a97c01de3f">6: An Analysis of the Expressiveness of Deep Neural Network Architectures Based on Their Lipschitz Constants</a></td>
</tr>
<tr>
<td>1</td>
<td>0.778673</td>
<td>0.973191</td>
<td><a href="https://www.semanticscholar.org/paper/ad8a12a19e74d9788f8fe92f5c0dfea7b6a52aba">946: The Loss Surfaces of Multilayer Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.777572</td>
<td>0.789272</td>
<td><a href="https://www.semanticscholar.org/paper/8f571b6173c195757d8759ca2cdb2c86c5f17740">0: Over-parametrized neural networks as under-determined linear systems</a></td>
</tr>
<tr>
<td>0</td>
<td>0.776166</td>
<td>0.697071</td>
<td><a href="https://www.semanticscholar.org/paper/62dfcbde02aa9a448820201573232f8ae56b58d8">1: Function Norms for Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.861437</td>
<td>0.989701</td>
<td><a href="https://www.semanticscholar.org/paper/d06a2ce22b60b5d806b22bbbc754b4d83d7c82d1">159: On the number of response regions of deep feed forward networks with piece-wise linear activations</a></td>
</tr>
<tr>
<td>0</td>
<td>0.777215</td>
<td>0.985079</td>
<td><a href="https://www.semanticscholar.org/paper/9375729d21a344a5ccccd5f53556ddf90b957cd9">387: Understanding Deep Neural Networks with Rectified Linear Units</a></td>
</tr>
<tr>
<td>0</td>
<td>0.804511</td>
<td>0.984651</td>
<td><a href="https://www.semanticscholar.org/paper/03e04983f7ce6a9c2b42948840b3312aea33f9f3">492: On the Expressive Power of Deep Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.704520</td>
<td>0.982812</td>
<td><a href="https://www.semanticscholar.org/paper/4206c84525a7904df3613b843491c0ae6a5507eb">436: Benefits of Depth in Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.720239</td>
<td>0.979902</td>
<td><a href="https://www.semanticscholar.org/paper/dbc78a005763e8f0a2132a8c40bbba0e099dce59">41: Neural Networks Should Be Wide Enough to Learn Disconnected Decision Regions</a></td>
</tr>
<tr>
<td>0</td>
<td>0.784645</td>
<td>0.978836</td>
<td><a href="https://www.semanticscholar.org/paper/a9da71715d54e33959751c88bb69a5875a23e324">553: The Power of Depth for Feedforward Neural Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.721154</td>
<td>0.975383</td>
<td><a href="https://www.semanticscholar.org/paper/f8e99110b48c353aabdb11cf0fd399e1e2a09980">162: Representation Benefits of Deep Feedforward Networks</a></td>
</tr>
<tr>
<td>0</td>
<td>0.715211</td>
<td>0.971438</td>
<td><a href="https://www.semanticscholar.org/paper/8cd2771048d1cc92c7c788cedc38420134fa23a7">9: On decision regions of narrow deep neural networks</a></td>
</tr>
</table></html>
